{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acb13de7",
   "metadata": {},
   "source": [
    "\n",
    "# ViEWS 3 ensembles\n",
    "## Fatalities project, cm level\n",
    "This notebook evaluates constituent models and explores some very simple ensemble algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72e401e",
   "metadata": {},
   "source": [
    "## Importing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a04c5fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refreshing\n"
     ]
    }
   ],
   "source": [
    "# Basics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cbook as cbook\n",
    "# sklearn\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# Views 3\n",
    "from viewser.operations import fetch\n",
    "from viewser import Queryset, Column\n",
    "import views_runs\n",
    "from views_partitioning import data_partitioner, legacy\n",
    "from stepshift import views\n",
    "import views_dataviz\n",
    "from views_runs import storage, ModelMetadata\n",
    "from views_runs.storage import store, retrieve, list, fetch_metadata\n",
    "from views_forecasts.extensions import *\n",
    "\n",
    "# Mapper\n",
    "import geopandas as gpd\n",
    "\n",
    "from views_dataviz.map import mapper, utils\n",
    "from views_dataviz import color\n",
    "from views_dataviz.map.presets import ViewsMap\n",
    "\n",
    "import sqlalchemy as sa\n",
    "from ingester3.config import source_db_path\n",
    "\n",
    "import sqlalchemy as sa\n",
    "from ingester3.config import source_db_path\n",
    "\n",
    "# Other packages\n",
    "import pickle as pkl\n",
    "\n",
    "#Parallelization\n",
    "from joblib import Parallel, delayed, cpu_count\n",
    "from functools import partial\n",
    "from genetic2 import *\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e69c40da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common parameters:\n",
    "\n",
    "run_id = 'Fat_devel_v7'\n",
    "FutureStart = 505\n",
    "RunGeneticAlgo = False\n",
    "\n",
    "steps = [*range(1, 36+1, 1)] # Which steps to train and predict for\n",
    "\n",
    "#steps = [1,2,3,4,5,6,7,8,9,10,11,12,15,18,21,24] # Which steps to train and predict for\n",
    "#fi_steps = [1,3,6,12,36] # Which steps to present feature importances for\n",
    "#steps = [1,12,24,36]\n",
    "fi_steps = [1,3,6,12,36]\n",
    "#steps = [1,6,36]\n",
    "#fi_steps = [1,6,36]\n",
    "\n",
    "# Specifying partitions\n",
    "\n",
    "calib_partitioner_dict = {\"train\":(121,396),\"predict\":(397,444)}\n",
    "test_partitioner_dict = {\"train\":(121,444),\"predict\":(445,492)}\n",
    "future_partitioner_dict = {\"train\":(121,492),\"predict\":(493,504)}\n",
    "calib_partitioner =  views_runs.DataPartitioner({\"calib\":calib_partitioner_dict})\n",
    "test_partitioner =  views_runs.DataPartitioner({\"test\":test_partitioner_dict})\n",
    "future_partitioner =  views_runs.DataPartitioner({\"future\":future_partitioner_dict})\n",
    "\n",
    "Mydropbox = '/Users/havardhegre/Dropbox (ViEWS)/ViEWS'\n",
    "overleafpath = '/Users/havardhegre/Dropbox (ViEWS)/Apps/Overleaf/ViEWS predicting fatalities/Tables/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c08b0209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 3 decimal places in output display\n",
    "pd.set_option(\"display.precision\", 3)\n",
    "\n",
    "# Don't wrap repr(DataFrame) across additional lines\n",
    "pd.set_option(\"display.expand_frame_repr\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abddfefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built on script from Geoff Hurdock: https://geoffruddock.com/building-a-hurdle-regression-estimator-in-scikit-learn/\n",
    "\n",
    "from typing import Optional, Union\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.utils.estimator_checks import check_estimator\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "\n",
    "\n",
    "class HurdleRegression(BaseEstimator):\n",
    "    \"\"\" Regression model which handles excessive zeros by fitting a two-part model and combining predictions:\n",
    "            1) binary classifier\n",
    "            2) continuous regression\n",
    "    Implementeted as a valid sklearn estimator, so it can be used in pipelines and GridSearch objects.\n",
    "    Args:\n",
    "        clf_name: currently supports either 'logistic' or 'LGBMClassifier'\n",
    "        reg_name: currently supports either 'linear' or 'LGBMRegressor'\n",
    "        clf_params: dict of parameters to pass to classifier sub-model when initialized\n",
    "        reg_params: dict of parameters to pass to regression sub-model when initialized\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 clf_name: str = 'logistic',\n",
    "                 reg_name: str = 'linear',\n",
    "                 clf_params: Optional[dict] = None,\n",
    "                 reg_params: Optional[dict] = None):\n",
    "\n",
    "        self.clf_name = clf_name\n",
    "        self.reg_name = reg_name\n",
    "        self.clf_params = clf_params\n",
    "        self.reg_params = reg_params\n",
    "        self.clf_fi = []\n",
    "        self.reg_fi = []\n",
    "\n",
    "    @staticmethod\n",
    "    def _resolve_estimator(func_name: str):\n",
    "        \"\"\" Lookup table for supported estimators.\n",
    "        This is necessary because sklearn estimator default arguments\n",
    "        must pass equality test, and instantiated sub-estimators are not equal. \"\"\"\n",
    "\n",
    "        funcs = {'linear': LinearRegression(),\n",
    "                 'logistic': LogisticRegression(solver='liblinear'),\n",
    "                 'LGBMRegressor': LGBMRegressor(n_estimators=100),\n",
    "                 'LGBMClassifier': LGBMClassifier(n_estimators=100),\n",
    "                 'RFRegressor': XGBRFRegressor(n_estimators=300,n_jobs=nj),\n",
    "                 'RFClassifier': XGBRFClassifier(n_estimators=300,n_jobs=nj),\n",
    "                 'GBMRegressor': GradientBoostingRegressor(n_estimators=200),\n",
    "                 'GBMClassifier': GradientBoostingClassifier(n_estimators=200),\n",
    "                 'XGBRegressor': XGBRegressor(n_estimators=200,tree_method='hist',n_jobs=nj),\n",
    "                 'XGBClassifier': XGBClassifier(n_estimators=200,tree_method='hist',n_jobs=nj,use_label_encoder=False),\n",
    "                 'HGBRegressor': HistGradientBoostingRegressor(max_iter=200),\n",
    "                 'HGBClassifier': HistGradientBoostingClassifier(max_iter=200),\n",
    "                }\n",
    "\n",
    "        return funcs[func_name]\n",
    "\n",
    "    def fit(self,\n",
    "            X: Union[np.ndarray, pd.DataFrame],\n",
    "            y: Union[np.ndarray, pd.Series]):\n",
    "        X, y = check_X_y(X, y, dtype=None,\n",
    "                         accept_sparse=False,\n",
    "                         accept_large_sparse=False,\n",
    "                         force_all_finite='allow-nan')\n",
    "\n",
    "        if X.shape[1] < 2:\n",
    "            raise ValueError('Cannot fit model when n_features = 1')\n",
    "\n",
    "        self.clf_ = self._resolve_estimator(self.clf_name)\n",
    "        if self.clf_params:\n",
    "            self.clf_.set_params(**self.clf_params)\n",
    "        self.clf_.fit(X, y > 0)\n",
    "        self.clf_fi = self.clf_.feature_importances_\n",
    "\n",
    "        self.reg_ = self._resolve_estimator(self.reg_name)\n",
    "        if self.reg_params:\n",
    "            self.reg_.set_params(**self.reg_params)\n",
    "        self.reg_.fit(X[y > 0], y[y > 0])\n",
    "        self.reg_fi = self.reg_.feature_importances_\n",
    "\n",
    "        self.is_fitted_ = True\n",
    "        return self\n",
    "\n",
    "\n",
    "#    def predict(self, X: Union[np.ndarray, pd.DataFrame]):\n",
    "    def predict_bck(self, X: Union[np.ndarray, pd.DataFrame]):\n",
    "        \"\"\" Predict combined response using binary classification outcome \"\"\"\n",
    "        X = check_array(X, accept_sparse=False, accept_large_sparse=False)\n",
    "        check_is_fitted(self, 'is_fitted_')\n",
    "        return self.clf_.predict(X) * self.reg_.predict(X)\n",
    "\n",
    "    def predict(self, X: Union[np.ndarray, pd.DataFrame]):\n",
    "#    def predict_expected_value(self, X: Union[np.ndarray, pd.DataFrame]):\n",
    "        \"\"\" Predict combined response using probabilistic classification outcome \"\"\"\n",
    "        X = check_array(X, accept_sparse=False, accept_large_sparse=False)\n",
    "        check_is_fitted(self, 'is_fitted_')\n",
    "        return self.clf_.predict_proba(X)[:, 1] * self.reg_.predict(X)\n",
    "\n",
    "\n",
    "def manual_test():\n",
    "    \"\"\" Validate estimator using sklearn's provided utility and ensure it can fit and predict on fake dataset. \"\"\"\n",
    "    check_estimator(HurdleRegression)\n",
    "    from sklearn.datasets import make_regression\n",
    "    X, y = make_regression()\n",
    "    reg = HurdleRegression()\n",
    "    reg.fit(X, y)\n",
    "    reg.predict(X)\n",
    "\n",
    "\n",
    "#if __name__ == '__main__':\n",
    "#    manual_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c1219b",
   "metadata": {},
   "source": [
    "## Retrieve models and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29f5db0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pickle as pkl\n",
    "#path = '/Users/havardhegre/temp/'\n",
    "RetrieveAll = True\n",
    "\n",
    "if RetrieveAll:\n",
    "    localpath = '/Users/havardhegre/temp/'\n",
    "    picklename = localpath + 'ModelList_cm_' + run_id + '.p'\n",
    "else:\n",
    "    path = '/Users/havardhegre/Dropbox (ViEWS)/ViEWS/Projects/PredictingFatalities/Predictions/'\n",
    "    picklename = path + 'ModelList_db_cm_' + run_id + '.p'\n",
    "EnsembleList = pkl.load( open (picklename, \"rb\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca3e5bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 fat_baseline_rf\n",
      "1 fat_conflicthistory_srf\n",
      "2 fat_conflicthistory_rf\n",
      "3 fat_conflicthistory_gbm\n",
      "4 fat_conflicthistory_hurdle_rf\n",
      "5 fat_conflicthistory_hurdle_xgb\n",
      "6 fat_conflicthistory_hurdle_lgb\n",
      "7 fat_conflicthistory_histgbm\n",
      "8 fat_conflicthistory_xgb\n",
      "9 fat_conflicthistory_lgb\n",
      "10 fat_conflicthistory_long_rf\n",
      "11 fat_conflicthistory_long_xgb\n",
      "12 fat_vdem_rf\n",
      "13 fat_vdem_xgb\n",
      "14 fat_vdem_hurdle_xgb\n",
      "15 fat_wdi_rf\n",
      "16 fat_wdi_xgb\n",
      "17 fat_wdi_hurdle_xgb\n",
      "18 fat_topics_rf\n",
      "19 fat_topics_histgbm\n",
      "20 fat_topics_xgb\n",
      "21 fat_topics_hurdle_xgb\n",
      "22 fat_prs_rf\n",
      "23 fat_prs_xgb\n",
      "24 fat_prs_hurdle_xgb\n",
      "25 fat_broad_rf\n",
      "26 fat_broad_xgb\n",
      "27 fat_broad_hurdle_xgb\n",
      "28 fat_greatest_hits_rf\n",
      "29 fat_greatest_hits_xgb\n",
      "30 fat_greatest_hits_hurdle_xgb\n",
      "31 fat_greatest_hits_lgb\n",
      "32 fat_hh20_srf\n",
      "33 fat_hh20_rf\n",
      "34 fat_hh20_gbm\n",
      "35 fat_hh20_hurdle_rf\n",
      "36 fat_hh20_hurdle_xgb\n",
      "37 fat_hh20_hurdle_lgb\n",
      "38 fat_hh20_histgbm\n",
      "39 fat_hh20_xgb\n",
      "40 fat_hh20_lgb\n",
      "41 fat_all_pca3_xgb\n",
      "42 fat_all_pca3_lgb\n",
      "43 fat_topics_pca3_xgb\n",
      "44 fat_topics_pca3_lgb\n",
      "45 fat_vdem_pca3_lgb\n",
      "46 fat_wdi_pca3_lgb\n",
      "47 fat_hh20_Markov_glm\n",
      "48 fat_hh20_Markov_rf\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "for model in EnsembleList:\n",
    "    print(i,model['modelname'])\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45bbcfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibrate to conform to mean and standard deviation\n",
    "\n",
    "# Calibration function\n",
    "def mean_sd_calibrated(y_true_calpart,y_pred_calpart,y_pred_test,shift, threshold=0):\n",
    "    ''' \n",
    "    Calibrates predictions. Expects the input columns from calibration partition to be without infinity values\n",
    "    '''\n",
    "    expand = y_true_calpart.loc[y_true_calpart>=threshold].std() / y_pred_calpart.loc[y_pred_calpart>=threshold].std()\n",
    "    shiftsize = 0\n",
    "    expanded = y_pred_test.copy()\n",
    "    expanded.loc[expanded>=threshold] = expanded * expand\n",
    "    if shift==True:\n",
    "        shiftsize = y_true_calpart.loc[y_true_calpart>=threshold].mean() - y_pred_calpart.loc[y_pred_calpart>=threshold].mean()\n",
    "        shifted = expanded\n",
    "        shifted.loc[shifted>=threshold] = shifted + shiftsize\n",
    "        calibrated_pred = shifted \n",
    "    if shift==False:\n",
    "        calibrated_pred = expanded       \n",
    "#    print('Calibration --', 'threshold:',threshold,'Shift:',shiftsize,'Expand:',expand)\n",
    "    return (calibrated_pred,expand,shiftsize)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62640103",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda list | grep views-forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a082d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ViewsMetadata().mine().with_name('baseline').fetch()\n",
    "run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed1c75ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fat_baseline_rf\n",
      "pr_33_cm_fat_baseline_rf_calib.parquet\n",
      "pr_33_cm_fat_baseline_rf_test.parquet\n",
      "fat_conflicthistory_srf\n",
      "pr_33_cm_fat_conflicthistory_srf_calib.parquet\n",
      "pr_33_cm_fat_conflicthistory_srf_test.parquet\n",
      "fat_conflicthistory_rf\n",
      "pr_33_cm_fat_conflicthistory_rf_calib.parquet\n",
      "pr_33_cm_fat_conflicthistory_rf_test.parquet\n",
      "fat_conflicthistory_gbm\n",
      "pr_33_cm_fat_conflicthistory_gbm_calib.parquet\n",
      "pr_33_cm_fat_conflicthistory_gbm_test.parquet\n",
      "fat_conflicthistory_hurdle_rf\n",
      "pr_33_cm_fat_conflicthistory_hurdle_rf_calib.parquet\n",
      "pr_33_cm_fat_conflicthistory_hurdle_rf_test.parquet\n",
      "fat_conflicthistory_hurdle_xgb\n",
      "pr_33_cm_fat_conflicthistory_hurdle_xgb_calib.parquet\n",
      "pr_33_cm_fat_conflicthistory_hurdle_xgb_test.parquet\n",
      "fat_conflicthistory_hurdle_lgb\n",
      "pr_33_cm_fat_conflicthistory_hurdle_lgb_calib.parquet\n",
      "pr_33_cm_fat_conflicthistory_hurdle_lgb_test.parquet\n",
      "fat_conflicthistory_histgbm\n",
      "pr_33_cm_fat_conflicthistory_histgbm_calib.parquet\n",
      "pr_33_cm_fat_conflicthistory_histgbm_test.parquet\n",
      "fat_conflicthistory_xgb\n",
      "pr_33_cm_fat_conflicthistory_xgb_calib.parquet\n",
      "pr_33_cm_fat_conflicthistory_xgb_test.parquet\n",
      "fat_conflicthistory_lgb\n",
      "pr_33_cm_fat_conflicthistory_lgb_calib.parquet\n",
      "pr_33_cm_fat_conflicthistory_lgb_test.parquet\n",
      "fat_conflicthistory_long_rf\n",
      "pr_33_cm_fat_conflicthistory_long_rf_calib.parquet\n",
      "pr_33_cm_fat_conflicthistory_long_rf_test.parquet\n",
      "fat_conflicthistory_long_xgb\n",
      "pr_33_cm_fat_conflicthistory_long_xgb_calib.parquet\n",
      "pr_33_cm_fat_conflicthistory_long_xgb_test.parquet\n",
      "fat_vdem_rf\n",
      "pr_33_cm_fat_vdem_rf_calib.parquet\n",
      "pr_33_cm_fat_vdem_rf_test.parquet\n",
      "fat_vdem_xgb\n",
      "pr_33_cm_fat_vdem_xgb_calib.parquet\n",
      "pr_33_cm_fat_vdem_xgb_test.parquet\n",
      "fat_vdem_hurdle_xgb\n",
      "pr_33_cm_fat_vdem_hurdle_xgb_calib.parquet\n",
      "pr_33_cm_fat_vdem_hurdle_xgb_test.parquet\n",
      "fat_wdi_rf\n",
      "pr_33_cm_fat_wdi_rf_calib.parquet\n",
      "pr_33_cm_fat_wdi_rf_test.parquet\n",
      "fat_wdi_xgb\n",
      "pr_33_cm_fat_wdi_xgb_calib.parquet\n",
      "pr_33_cm_fat_wdi_xgb_test.parquet\n",
      "fat_wdi_hurdle_xgb\n",
      "pr_33_cm_fat_wdi_hurdle_xgb_calib.parquet\n",
      "pr_33_cm_fat_wdi_hurdle_xgb_test.parquet\n",
      "fat_topics_rf\n",
      "pr_33_cm_fat_topics_rf_calib.parquet\n",
      "pr_33_cm_fat_topics_rf_test.parquet\n",
      "fat_topics_histgbm\n",
      "pr_33_cm_fat_topics_histgbm_calib.parquet\n",
      "pr_33_cm_fat_topics_histgbm_test.parquet\n",
      "fat_topics_xgb\n",
      "pr_33_cm_fat_topics_xgb_calib.parquet\n",
      "pr_33_cm_fat_topics_xgb_test.parquet\n",
      "fat_topics_hurdle_xgb\n",
      "pr_33_cm_fat_topics_hurdle_xgb_calib.parquet\n",
      "pr_33_cm_fat_topics_hurdle_xgb_test.parquet\n",
      "fat_prs_rf\n",
      "pr_33_cm_fat_prs_rf_calib.parquet\n",
      "pr_33_cm_fat_prs_rf_test.parquet\n",
      "fat_prs_xgb\n",
      "pr_33_cm_fat_prs_xgb_calib.parquet\n",
      "pr_33_cm_fat_prs_xgb_test.parquet\n",
      "fat_prs_hurdle_xgb\n",
      "pr_33_cm_fat_prs_hurdle_xgb_calib.parquet\n",
      "pr_33_cm_fat_prs_hurdle_xgb_test.parquet\n",
      "fat_broad_rf\n",
      "pr_33_cm_fat_broad_rf_calib.parquet\n",
      "pr_33_cm_fat_broad_rf_test.parquet\n",
      "fat_broad_xgb\n",
      "pr_33_cm_fat_broad_xgb_calib.parquet\n",
      "pr_33_cm_fat_broad_xgb_test.parquet\n",
      "fat_broad_hurdle_xgb\n",
      "pr_33_cm_fat_broad_hurdle_xgb_calib.parquet\n",
      "pr_33_cm_fat_broad_hurdle_xgb_test.parquet\n",
      "fat_greatest_hits_rf\n",
      "pr_33_cm_fat_greatest_hits_rf_calib.parquet\n",
      "pr_33_cm_fat_greatest_hits_rf_test.parquet\n",
      "fat_greatest_hits_xgb\n",
      "pr_33_cm_fat_greatest_hits_xgb_calib.parquet\n",
      "pr_33_cm_fat_greatest_hits_xgb_test.parquet\n",
      "fat_greatest_hits_hurdle_xgb\n",
      "pr_33_cm_fat_greatest_hits_hurdle_xgb_calib.parquet\n",
      "pr_33_cm_fat_greatest_hits_hurdle_xgb_test.parquet\n",
      "fat_greatest_hits_lgb\n",
      "pr_33_cm_fat_greatest_hits_lgb_calib.parquet\n",
      "pr_33_cm_fat_greatest_hits_lgb_test.parquet\n",
      "fat_hh20_srf\n",
      "pr_33_cm_fat_hh20_srf_calib.parquet\n",
      "pr_33_cm_fat_hh20_srf_test.parquet\n",
      "fat_hh20_rf\n",
      "pr_33_cm_fat_hh20_rf_calib.parquet\n",
      "pr_33_cm_fat_hh20_rf_test.parquet\n",
      "fat_hh20_gbm\n",
      "pr_33_cm_fat_hh20_gbm_calib.parquet\n",
      "pr_33_cm_fat_hh20_gbm_test.parquet\n",
      "fat_hh20_hurdle_rf\n",
      "pr_33_cm_fat_hh20_hurdle_rf_calib.parquet\n",
      "pr_33_cm_fat_hh20_hurdle_rf_test.parquet\n",
      "fat_hh20_hurdle_xgb\n",
      "pr_33_cm_fat_hh20_hurdle_xgb_calib.parquet\n",
      "pr_33_cm_fat_hh20_hurdle_xgb_test.parquet\n",
      "fat_hh20_hurdle_lgb\n",
      "pr_33_cm_fat_hh20_hurdle_lgb_calib.parquet\n",
      "pr_33_cm_fat_hh20_hurdle_lgb_test.parquet\n",
      "fat_hh20_histgbm\n",
      "pr_33_cm_fat_hh20_histgbm_calib.parquet\n",
      "pr_33_cm_fat_hh20_histgbm_test.parquet\n",
      "fat_hh20_xgb\n",
      "pr_33_cm_fat_hh20_xgb_calib.parquet\n",
      "pr_33_cm_fat_hh20_xgb_test.parquet\n",
      "fat_hh20_lgb\n",
      "pr_33_cm_fat_hh20_lgb_calib.parquet\n",
      "pr_33_cm_fat_hh20_lgb_test.parquet\n",
      "fat_all_pca3_xgb\n",
      "pr_33_cm_fat_all_pca3_xgb_calib.parquet\n",
      "pr_33_cm_fat_all_pca3_xgb_test.parquet\n",
      "fat_all_pca3_lgb\n",
      "pr_33_cm_fat_all_pca3_lgb_calib.parquet\n",
      "pr_33_cm_fat_all_pca3_lgb_test.parquet\n",
      "fat_topics_pca3_xgb\n",
      "pr_33_cm_fat_topics_pca3_xgb_calib.parquet\n",
      "pr_33_cm_fat_topics_pca3_xgb_test.parquet\n",
      "fat_topics_pca3_lgb\n",
      "pr_33_cm_fat_topics_pca3_lgb_calib.parquet\n",
      "pr_33_cm_fat_topics_pca3_lgb_test.parquet\n",
      "fat_vdem_pca3_lgb\n",
      "pr_33_cm_fat_vdem_pca3_lgb_calib.parquet\n",
      "pr_33_cm_fat_vdem_pca3_lgb_test.parquet\n",
      "fat_wdi_pca3_lgb\n",
      "pr_33_cm_fat_wdi_pca3_lgb_calib.parquet\n",
      "pr_33_cm_fat_wdi_pca3_lgb_test.parquet\n",
      "fat_hh20_Markov_glm\n",
      "pr_33_cm_fat_hh20_Markov_glm_calib.parquet\n",
      "pr_33_cm_fat_hh20_Markov_glm_test.parquet\n",
      "fat_hh20_Markov_rf\n",
      "pr_33_cm_fat_hh20_Markov_rf_calib.parquet\n",
      "pr_33_cm_fat_hh20_Markov_rf_test.parquet\n"
     ]
    }
   ],
   "source": [
    "# Retrieving the predictions\n",
    "# The EnsembleList contains the predictions organized by model\n",
    "i=0\n",
    "RetrieveFuture = False\n",
    "stepcols = ['ln_ged_sb_dep']\n",
    "for step in steps:\n",
    "    stepcols.append('step_pred_' + str(step))\n",
    "level = 'cm'\n",
    "for model in EnsembleList:\n",
    "    print(i, model['modelname'])\n",
    "    stored_modelname_calib = level + '_' + model['modelname'] + '_calib'\n",
    "    stored_modelname_test = level + '_' + model['modelname'] + '_test'\n",
    "    stored_modelname_future = level +  '_' + model['modelname'] + '_f' + str(FutureStart)\n",
    "    model['predictions_calib_df'] = pd.DataFrame.forecasts.read_store(stored_modelname_calib, run=run_id)[stepcols]\n",
    "    model['predictions_calib_df'].replace([np.inf, -np.inf], 0, inplace=True)\n",
    "    model['predictions_test_df'] = pd.DataFrame.forecasts.read_store(stored_modelname_test, run=run_id)[stepcols]\n",
    "    model['predictions_test_df'].replace([np.inf, -np.inf], 0, inplace=True)\n",
    "    if RetrieveFuture:\n",
    "        model['predictions_future_df'] = pd.DataFrame.forecasts.read_store(stored_modelname_future, run=run_id)\n",
    "        model['predictions_future_df'].replace([np.inf, -np.inf], 0, inplace=True)\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d63b69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ViewsMetadata().mine().with_name('Markov').fetch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1bddaa39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pr_33_cm_fat_baseline_rf_calib.parquet\n",
      "pr_33_cm_fat_baseline_rf_test.parquet\n"
     ]
    }
   ],
   "source": [
    "# Prediction target\n",
    "# In this particular ensemble available in model 0\n",
    "stored_modelname_calib = level + '_' + EnsembleList[0]['modelname'] + '_calib'\n",
    "stored_modelname_test = level + '_' + EnsembleList[0]['modelname'] + '_test'\n",
    "target = {\n",
    "        'y_calib':  pd.DataFrame.forecasts.read_store(stored_modelname_calib, run=run_id)['ln_ged_sb_dep'],\n",
    "        'y_test':  pd.DataFrame.forecasts.read_store(stored_modelname_test, run=run_id)['ln_ged_sb_dep']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d26d808",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "60611323",
   "metadata": {},
   "source": [
    "# To do with calibration:\n",
    "Coordinate with Jim to incorporate his improvements, and to work the retrieveal of the gam object into his version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28d77f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAM-based calibration function\n",
    "def gam_calibrated(y_true_calpart,y_pred_calpart,y_pred_test,n_splines):\n",
    "    ''' \n",
    "    Calibrates predictions using GAM.\n",
    "    Expects the input columns from calibration partition to be without infinity values\n",
    "    '''\n",
    "    from pygam import LogisticGAM, LinearGAM, s, te\n",
    "    gam = LinearGAM(s(0, constraints='monotonic_inc',n_splines = n_splines)).fit(y_pred_calpart, y_true_calpart)\n",
    "\n",
    "    calibrated_pred = gam.predict(y_pred_test)\n",
    "#    gam_summary = gam.summary()\n",
    "    return (calibrated_pred, gam)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62883d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fat_baseline_rf\n",
      "fat_conflicthistory_srf\n",
      "fat_conflicthistory_rf\n",
      "fat_conflicthistory_gbm\n",
      "fat_conflicthistory_hurdle_rf\n",
      "fat_conflicthistory_hurdle_xgb\n",
      "fat_conflicthistory_hurdle_lgb\n",
      "fat_conflicthistory_histgbm\n",
      "fat_conflicthistory_xgb\n",
      "fat_conflicthistory_lgb\n",
      "fat_conflicthistory_long_rf\n",
      "fat_conflicthistory_long_xgb\n",
      "fat_vdem_rf\n",
      "fat_vdem_xgb\n",
      "fat_vdem_hurdle_xgb\n",
      "fat_wdi_rf\n",
      "fat_wdi_xgb\n",
      "fat_wdi_hurdle_xgb\n",
      "fat_topics_rf\n",
      "fat_topics_histgbm\n",
      "fat_topics_xgb\n",
      "fat_topics_hurdle_xgb\n",
      "fat_prs_rf\n",
      "fat_prs_xgb\n",
      "fat_prs_hurdle_xgb\n",
      "fat_broad_rf\n",
      "fat_broad_xgb\n",
      "fat_broad_hurdle_xgb\n",
      "fat_greatest_hits_rf\n",
      "fat_greatest_hits_xgb\n",
      "fat_greatest_hits_hurdle_xgb\n",
      "fat_greatest_hits_lgb\n",
      "fat_hh20_srf\n",
      "fat_hh20_rf\n",
      "fat_hh20_gbm\n",
      "fat_hh20_hurdle_rf\n",
      "fat_hh20_hurdle_xgb\n",
      "fat_hh20_hurdle_lgb\n",
      "fat_hh20_histgbm\n",
      "fat_hh20_xgb\n",
      "fat_hh20_lgb\n",
      "fat_all_pca3_xgb\n",
      "fat_all_pca3_lgb\n",
      "fat_topics_pca3_xgb\n",
      "fat_topics_pca3_lgb\n",
      "fat_vdem_pca3_lgb\n",
      "fat_wdi_pca3_lgb\n",
      "fat_hh20_Markov_glm\n",
      "fat_hh20_Markov_rf\n"
     ]
    }
   ],
   "source": [
    "# Calibration\n",
    "IncludeFuture = False\n",
    "for model in EnsembleList:   \n",
    "    model['calib_df_cal_expand'] = model['predictions_calib_df'].copy()\n",
    "    model['test_df_cal_expand'] = model['predictions_test_df'].copy()\n",
    "    if IncludeFuture:\n",
    "        model['future_df_cal_expand'] = model['predictions_future_df'].copy()\n",
    "    model['calib_df_calibrated'] = model['predictions_calib_df'].copy()\n",
    "    model['test_df_calibrated'] = model['predictions_test_df'].copy()\n",
    "    if IncludeFuture:\n",
    "        model['future_df_calibrated'] = model['predictions_future_df'].copy()\n",
    "    print(model['modelname'])\n",
    "    model['calibration_gams'] = [] # Will hold calibration GAM objects, one for each step\n",
    "    for col in stepcols[1:]:\n",
    "        thisstep = int(col[10:])\n",
    "        thismonth = FutureStart + thisstep\n",
    "        calibration_gam_dict = {\n",
    "            'Step': thisstep,\n",
    "            'GAM': []\n",
    "        }\n",
    "        # Remove from model dfs rows where [col] has infinite values (due to the 2011 split of Sudan)\n",
    "        df_calib = model['predictions_calib_df'][~np.isinf(model['predictions_calib_df'][col])].fillna(0)\n",
    "        df_test = model['predictions_test_df'][~np.isinf(model['predictions_test_df'][col])].fillna(0)\n",
    "        if IncludeFuture:\n",
    "            df_future = model['predictions_future_df'][~np.isinf(model['predictions_future_df']['step_combined'])].fillna(0)\n",
    "        \n",
    "        (model['calib_df_cal_expand'][col],model['expanded'],model['shiftsize']) = mean_sd_calibrated(\n",
    "            y_true_calpart = df_calib['ln_ged_sb_dep'], \n",
    "            y_pred_calpart = df_calib[col], \n",
    "            y_pred_test = df_calib[col], \n",
    "            shift=False, \n",
    "            threshold = 0\n",
    "        )\n",
    "        (model['test_df_cal_expand'][col],model['expanded'],model['shiftsize']) = mean_sd_calibrated(\n",
    "            y_true_calpart = df_calib['ln_ged_sb_dep'], \n",
    "            y_pred_calpart = df_calib[col], \n",
    "            y_pred_test = df_test[col], \n",
    "            shift=False, \n",
    "            threshold = 0\n",
    "        )\n",
    "        if IncludeFuture:\n",
    "            (model['future_df_cal_expand'].loc[thismonth]['step_combined'], model['expanded'],model['shiftsize']) = mean_sd_calibrated(\n",
    "                y_true_calpart = df_calib['ln_ged_sb_dep'], \n",
    "                y_pred_calpart = df_calib[col], \n",
    "                y_pred_test = df_future.loc[thismonth]['step_combined'], \n",
    "                shift=False, \n",
    "                threshold = 0\n",
    "            )\n",
    "        if model['modelname'] == 'fat_hh20_Markov_glm' or model['modelname'] == 'fat_hh20_Markov_rf':\n",
    "            model['calib_df_calibrated'][col] = model['calib_df_cal_expand'][col]\n",
    "            model['test_df_calibrated'][col] = model['test_df_cal_expand'][col]\n",
    "        else:\n",
    "            (model['calib_df_calibrated'][col], calibration_gam_dict['calibration_GAM']) = gam_calibrated(\n",
    "                    y_true_calpart = df_calib['ln_ged_sb_dep'], \n",
    "                    y_pred_calpart = df_calib[col], \n",
    "                    y_pred_test = df_calib[col], \n",
    "                    n_splines = 15\n",
    "            )\n",
    "            #print(model['calibration_gam'].summary())\n",
    "            (model['test_df_calibrated'][col], gam) = gam_calibrated(\n",
    "                    y_true_calpart = df_calib['ln_ged_sb_dep'], \n",
    "                    y_pred_calpart = df_calib[col], \n",
    "                    y_pred_test = df_test[col], \n",
    "                    n_splines = 15\n",
    "            )\n",
    "            if IncludeFuture:\n",
    "                (model['future_df_calibrated'].loc[thismonth]['step_combined'], gam) = gam_calibrated(\n",
    "                        y_true_calpart = df_calib['ln_ged_sb_dep'], \n",
    "                        y_pred_calpart = df_calib[col], \n",
    "                        y_pred_test = df_future.loc[thismonth]['step_combined'], \n",
    "                        n_splines = 15\n",
    "                )\n",
    "        model['calibration_gams'].append(calibration_gam_dict)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10d1cdfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Step': 1,\n",
       "  'GAM': [],\n",
       "  'calibration_GAM': LinearGAM(callbacks=[Deviance(), Diffs()], fit_intercept=True, \n",
       "     max_iter=100, scale=None, terms=s(0) + intercept, tol=0.0001, \n",
       "     verbose=False)},\n",
       " {'Step': 2,\n",
       "  'GAM': [],\n",
       "  'calibration_GAM': LinearGAM(callbacks=[Deviance(), Diffs()], fit_intercept=True, \n",
       "     max_iter=100, scale=None, terms=s(0) + intercept, tol=0.0001, \n",
       "     verbose=False)},\n",
       " {'Step': 3,\n",
       "  'GAM': [],\n",
       "  'calibration_GAM': LinearGAM(callbacks=[Deviance(), Diffs()], fit_intercept=True, \n",
       "     max_iter=100, scale=None, terms=s(0) + intercept, tol=0.0001, \n",
       "     verbose=False)},\n",
       " {'Step': 4,\n",
       "  'GAM': [],\n",
       "  'calibration_GAM': LinearGAM(callbacks=[Deviance(), Diffs()], fit_intercept=True, \n",
       "     max_iter=100, scale=None, terms=s(0) + intercept, tol=0.0001, \n",
       "     verbose=False)},\n",
       " {'Step': 5,\n",
       "  'GAM': [],\n",
       "  'calibration_GAM': LinearGAM(callbacks=[Deviance(), Diffs()], fit_intercept=True, \n",
       "     max_iter=100, scale=None, terms=s(0) + intercept, tol=0.0001, \n",
       "     verbose=False)},\n",
       " {'Step': 6,\n",
       "  'GAM': [],\n",
       "  'calibration_GAM': LinearGAM(callbacks=[Deviance(), Diffs()], fit_intercept=True, \n",
       "     max_iter=100, scale=None, terms=s(0) + intercept, tol=0.0001, \n",
       "     verbose=False)},\n",
       " {'Step': 7,\n",
       "  'GAM': [],\n",
       "  'calibration_GAM': LinearGAM(callbacks=[Deviance(), Diffs()], fit_intercept=True, \n",
       "     max_iter=100, scale=None, terms=s(0) + intercept, tol=0.0001, \n",
       "     verbose=False)},\n",
       " {'Step': 8,\n",
       "  'GAM': [],\n",
       "  'calibration_GAM': LinearGAM(callbacks=[Deviance(), Diffs()], fit_intercept=True, \n",
       "     max_iter=100, scale=None, terms=s(0) + intercept, tol=0.0001, \n",
       "     verbose=False)},\n",
       " {'Step': 9,\n",
       "  'GAM': [],\n",
       "  'calibration_GAM': LinearGAM(callbacks=[Deviance(), Diffs()], fit_intercept=True, \n",
       "     max_iter=100, scale=None, terms=s(0) + intercept, tol=0.0001, \n",
       "     verbose=False)},\n",
       " {'Step': 10,\n",
       "  'GAM': [],\n",
       "  'calibration_GAM': LinearGAM(callbacks=[Deviance(), Diffs()], fit_intercept=True, \n",
       "     max_iter=100, scale=None, terms=s(0) + intercept, tol=0.0001, \n",
       "     verbose=False)},\n",
       " {'Step': 11,\n",
       "  'GAM': [],\n",
       "  'calibration_GAM': LinearGAM(callbacks=[Deviance(), Diffs()], fit_intercept=True, \n",
       "     max_iter=100, scale=None, terms=s(0) + intercept, tol=0.0001, \n",
       "     verbose=False)},\n",
       " {'Step': 12,\n",
       "  'GAM': [],\n",
       "  'calibration_GAM': LinearGAM(callbacks=[Deviance(), Diffs()], fit_intercept=True, \n",
       "     max_iter=100, scale=None, terms=s(0) + intercept, tol=0.0001, \n",
       "     verbose=False)},\n",
       " {'Step': 13,\n",
       "  'GAM': [],\n",
       "  'calibration_GAM': LinearGAM(callbacks=[Deviance(), Diffs()], fit_intercept=True, \n",
       "     max_iter=100, scale=None, terms=s(0) + intercept, tol=0.0001, \n",
       "     verbose=False)},\n",
       " {'Step': 14,\n",
       "  'GAM': [],\n",
       "  'calibration_GAM': LinearGAM(callbacks=[Deviance(), Diffs()], fit_intercept=True, \n",
       "     max_iter=100, scale=None, terms=s(0) + intercept, tol=0.0001, \n",
       "     verbose=False)},\n",
       " {'Step': 15,\n",
       "  'GAM': [],\n",
       "  'calibration_GAM': LinearGAM(callbacks=[Deviance(), Diffs()], fit_intercept=True, \n",
       "     max_iter=100, scale=None, terms=s(0) + intercept, tol=0.0001, \n",
       "     verbose=False)},\n",
       " {'Step': 16,\n",
       "  'GAM': [],\n",
       "  'calibration_GAM': LinearGAM(callbacks=[Deviance(), Diffs()], fit_intercept=True, \n",
       "     max_iter=100, scale=None, terms=s(0) + intercept, tol=0.0001, \n",
       "     verbose=False)},\n",
       " {'Step': 17,\n",
       "  'GAM': [],\n",
       "  'calibration_GAM': LinearGAM(callbacks=[Deviance(), Diffs()], fit_intercept=True, \n",
       "     max_iter=100, scale=None, terms=s(0) + intercept, tol=0.0001, \n",
       "     verbose=False)},\n",
       " {'Step': 18,\n",
       "  'GAM': [],\n",
       "  'calibration_GAM': LinearGAM(callbacks=[Deviance(), Diffs()], fit_intercept=True, \n",
       "     max_iter=100, scale=None, terms=s(0) + intercept, tol=0.0001, \n",
       "     verbose=False)},\n",
       " {'Step': 19,\n",
       "  'GAM': [],\n",
       "  'calibration_GAM': LinearGAM(callbacks=[Deviance(), Diffs()], fit_intercept=True, \n",
       "     max_iter=100, scale=None, terms=s(0) + intercept, tol=0.0001, \n",
       "     verbose=False)},\n",
       " {'Step': 20,\n",
       "  'GAM': [],\n",
       "  'calibration_GAM': LinearGAM(callbacks=[Deviance(), Diffs()], fit_intercept=True, \n",
       "     max_iter=100, scale=None, terms=s(0) + intercept, tol=0.0001, \n",
       "     verbose=False)},\n",
       " {'Step': 21,\n",
       "  'GAM': [],\n",
       "  'calibration_GAM': LinearGAM(callbacks=[Deviance(), Diffs()], fit_intercept=True, \n",
       "     max_iter=100, scale=None, terms=s(0) + intercept, tol=0.0001, \n",
       "     verbose=False)},\n",
       " {'Step': 22,\n",
       "  'GAM': [],\n",
       "  'calibration_GAM': LinearGAM(callbacks=[Deviance(), Diffs()], fit_intercept=True, \n",
       "     max_iter=100, scale=None, terms=s(0) + intercept, tol=0.0001, \n",
       "     verbose=False)},\n",
       " {'Step': 23,\n",
       "  'GAM': [],\n",
       "  'calibration_GAM': LinearGAM(callbacks=[Deviance(), Diffs()], fit_intercept=True, \n",
       "     max_iter=100, scale=None, terms=s(0) + intercept, tol=0.0001, \n",
       "     verbose=False)},\n",
       " {'Step': 24,\n",
       "  'GAM': [],\n",
       "  'calibration_GAM': LinearGAM(callbacks=[Deviance(), Diffs()], fit_intercept=True, \n",
       "     max_iter=100, scale=None, terms=s(0) + intercept, tol=0.0001, \n",
       "     verbose=False)},\n",
       " {'Step': 25,\n",
       "  'GAM': [],\n",
       "  'calibration_GAM': LinearGAM(callbacks=[Deviance(), Diffs()], fit_intercept=True, \n",
       "     max_iter=100, scale=None, terms=s(0) + intercept, tol=0.0001, \n",
       "     verbose=False)},\n",
       " {'Step': 26,\n",
       "  'GAM': [],\n",
       "  'calibration_GAM': LinearGAM(callbacks=[Deviance(), Diffs()], fit_intercept=True, \n",
       "     max_iter=100, scale=None, terms=s(0) + intercept, tol=0.0001, \n",
       "     verbose=False)},\n",
       " {'Step': 27,\n",
       "  'GAM': [],\n",
       "  'calibration_GAM': LinearGAM(callbacks=[Deviance(), Diffs()], fit_intercept=True, \n",
       "     max_iter=100, scale=None, terms=s(0) + intercept, tol=0.0001, \n",
       "     verbose=False)},\n",
       " {'Step': 28,\n",
       "  'GAM': [],\n",
       "  'calibration_GAM': LinearGAM(callbacks=[Deviance(), Diffs()], fit_intercept=True, \n",
       "     max_iter=100, scale=None, terms=s(0) + intercept, tol=0.0001, \n",
       "     verbose=False)},\n",
       " {'Step': 29,\n",
       "  'GAM': [],\n",
       "  'calibration_GAM': LinearGAM(callbacks=[Deviance(), Diffs()], fit_intercept=True, \n",
       "     max_iter=100, scale=None, terms=s(0) + intercept, tol=0.0001, \n",
       "     verbose=False)},\n",
       " {'Step': 30,\n",
       "  'GAM': [],\n",
       "  'calibration_GAM': LinearGAM(callbacks=[Deviance(), Diffs()], fit_intercept=True, \n",
       "     max_iter=100, scale=None, terms=s(0) + intercept, tol=0.0001, \n",
       "     verbose=False)},\n",
       " {'Step': 31,\n",
       "  'GAM': [],\n",
       "  'calibration_GAM': LinearGAM(callbacks=[Deviance(), Diffs()], fit_intercept=True, \n",
       "     max_iter=100, scale=None, terms=s(0) + intercept, tol=0.0001, \n",
       "     verbose=False)},\n",
       " {'Step': 32,\n",
       "  'GAM': [],\n",
       "  'calibration_GAM': LinearGAM(callbacks=[Deviance(), Diffs()], fit_intercept=True, \n",
       "     max_iter=100, scale=None, terms=s(0) + intercept, tol=0.0001, \n",
       "     verbose=False)},\n",
       " {'Step': 33,\n",
       "  'GAM': [],\n",
       "  'calibration_GAM': LinearGAM(callbacks=[Deviance(), Diffs()], fit_intercept=True, \n",
       "     max_iter=100, scale=None, terms=s(0) + intercept, tol=0.0001, \n",
       "     verbose=False)},\n",
       " {'Step': 34,\n",
       "  'GAM': [],\n",
       "  'calibration_GAM': LinearGAM(callbacks=[Deviance(), Diffs()], fit_intercept=True, \n",
       "     max_iter=100, scale=None, terms=s(0) + intercept, tol=0.0001, \n",
       "     verbose=False)},\n",
       " {'Step': 35,\n",
       "  'GAM': [],\n",
       "  'calibration_GAM': LinearGAM(callbacks=[Deviance(), Diffs()], fit_intercept=True, \n",
       "     max_iter=100, scale=None, terms=s(0) + intercept, tol=0.0001, \n",
       "     verbose=False)},\n",
       " {'Step': 36,\n",
       "  'GAM': [],\n",
       "  'calibration_GAM': LinearGAM(callbacks=[Deviance(), Diffs()], fit_intercept=True, \n",
       "     max_iter=100, scale=None, terms=s(0) + intercept, tol=0.0001, \n",
       "     verbose=False)}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EnsembleList[46]['calibration_gams']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e983b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model['predictions_future_df'].loc[510]['step_combined'].describe(), \n",
    "model['future_df_calibrated'].loc[510]['step_combined'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5fbcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in stepcols[1:]:\n",
    "    print(int(col[10:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9d1b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Illustrating calibration\n",
    "model = EnsembleList[5]\n",
    "print(model['modelname'])\n",
    "col = 'step_pred_1'\n",
    "period = 'test'\n",
    "\n",
    "print(model[f'{period}_df_calibrated'][col].describe())\n",
    "print(model[f'predictions_{period}_df'][col].describe())\n",
    "\n",
    "plt.scatter(model[f'predictions_{period}_df'][col],model[f'{period}_df_calibrated'][col])\n",
    "#plt.show()\n",
    "\n",
    "overleafpath = '/Users/havardhegre/Dropbox (ViEWS)/Apps/Overleaf/ViEWS predicting fatalities/Figures/PredictionPlots/'\n",
    "filename = overleafpath + 'Calibration_example_' + model['modelname'] + '.png'\n",
    "plt.savefig(filename, dpi=300)\n",
    "#overleafpath = '~/Dropbox (ViEWS)/Apps/Overleaf/ViEWS predicting fatalities/Figures/PredictionPlots/'\n",
    "#filename = overleafpath + 'Calibration_example' + model['modelname'] + '.png'\n",
    "#plt.savefig(filename, dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34bc8c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save EnsembleList with all content locally and on dropbox\n",
    "localpath = '/Users/havardhegre/temp/'\n",
    "dbpath = '/Users/havardhegre/Dropbox (ViEWS)/ViEWS/Projects/PredictingFatalities/Predictions/'\n",
    "picklename = localpath + 'EnsembleList_cm_' + run_id + '.p'\n",
    "pkl.dump(EnsembleList, open(picklename, \"wb\" ) )\n",
    "picklename = dbpath + 'EnsembleList_db_cm_' + run_id + '.p'\n",
    "pkl.dump(EnsembleList, open(picklename, \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0ccaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentiles = [.75,.9,.95,.99]\n",
    "print(model['modelname'], \n",
    "      model['predictions_calib_df']['step_pred_1'].describe(percentiles = percentiles),\n",
    "      model['calib_df_calibrated']['step_pred_1'].describe(percentiles = percentiles),\n",
    "      model['predictions_test_df']['ln_ged_sb_dep'].describe(percentiles = percentiles),\n",
    "      model['predictions_test_df']['step_pred_1'].describe(percentiles = percentiles),\n",
    "      model['test_df_calibrated']['step_pred_1'].describe(percentiles = percentiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40cee84",
   "metadata": {},
   "outputs": [],
   "source": [
    "EnsembleList[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d74ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_inspect = ['step_pred_1','step_pred_6','step_pred_12','step_pred_36']\n",
    "EnsembleList[12]['predictions_calib_df'][cols_to_inspect].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f483cb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create unweighted average ensemble\n",
    "# The gam calibrated is basis currently\n",
    " \n",
    "IncludeFuture = False\n",
    " # Model: Ensemble, GED logged dependent variable,\n",
    "if IncludeFuture:      \n",
    "    ensemble = {\n",
    "        'modelname': 'ensemble_unweighted',\n",
    "        'depvar': \"ln_ged_sb_dep\",\n",
    "        'loggeddepvar': True,\n",
    "        'predictions_file_calib': \"\",\n",
    "        'predictions_file_test': \"\",\n",
    "        'calib_df_calibrated':  EnsembleList[0]['calib_df_calibrated'].copy(),\n",
    "        'test_df_calibrated':   EnsembleList[0]['test_df_calibrated'].copy(),\n",
    "        'future_df_calibrated': EnsembleList[0]['future_df_calibrated'].copy(),\n",
    "        'calib_df_cal_expand':  EnsembleList[0]['calib_df_cal_expand'].copy(),\n",
    "        'test_df_cal_expand':   EnsembleList[0]['test_df_cal_expand'].copy(),\n",
    "        'future_df_cal_expand': EnsembleList[0]['future_df_cal_expand'].copy(),\n",
    "    }\n",
    "else:      \n",
    "    ensemble = {\n",
    "        'modelname': 'ensemble_unweighted',\n",
    "        'depvar': \"ln_ged_sb_dep\",\n",
    "        'loggeddepvar': True,\n",
    "        'predictions_file_calib': \"\",\n",
    "        'predictions_file_test': \"\",\n",
    "        'calib_df_calibrated':  EnsembleList[0]['calib_df_calibrated'].copy(),\n",
    "        'test_df_calibrated':   EnsembleList[0]['test_df_calibrated'].copy(),\n",
    "#        'future_df_calibrated': EnsembleList[0]['future_df_calibrated'].copy(),\n",
    "        'calib_df_cal_expand':  EnsembleList[0]['calib_df_cal_expand'].copy(),\n",
    "        'test_df_cal_expand':   EnsembleList[0]['test_df_cal_expand'].copy(),\n",
    "#        'future_df_cal_expand': EnsembleList[0]['future_df_cal_expand'].copy(),\n",
    "        'calibration_gam': []\n",
    "    }\n",
    "\n",
    "n_models = 1\n",
    "for model in EnsembleList:\n",
    "    ensemble['calib_df_calibrated'] = ensemble['calib_df_calibrated'].add(model['calib_df_calibrated'])\n",
    "    ensemble['test_df_calibrated'] = ensemble['test_df_calibrated'].add(model['test_df_calibrated'])\n",
    "    if IncludeFuture:\n",
    "        ensemble['future_df_calibrated'] = ensemble['future_df_calibrated'].add(model['future_df_calibrated'])\n",
    "    ensemble['calib_df_cal_expand'] = ensemble['calib_df_cal_expand'].add(model['calib_df_cal_expand'])\n",
    "    ensemble['test_df_cal_expand'] = ensemble['test_df_cal_expand'].add(model['test_df_cal_expand'])\n",
    "    if IncludeFuture:\n",
    "        ensemble['future_df_cal_expand'] = ensemble['future_df_cal_expand'].add(model['future_df_cal_expand'])\n",
    "    n_models = n_models + 1\n",
    "#n_models = 1\n",
    "    \n",
    "ensemble['calib_df_calibrated'] = ensemble['calib_df_calibrated'].divide(n_models)\n",
    "ensemble['test_df_calibrated'] = ensemble['test_df_calibrated'].divide(n_models)\n",
    "if IncludeFuture:\n",
    "    ensemble['future_df_calibrated'] = ensemble['future_df_calibrated'].divide(n_models)\n",
    "ensemble['calib_df_cal_expand'] = ensemble['calib_df_cal_expand'].divide(n_models)\n",
    "ensemble['test_df_cal_expand'] = ensemble['test_df_cal_expand'].divide(n_models)\n",
    "if IncludeFuture:\n",
    "    ensemble['future_df_cal_expand'] = ensemble['future_df_cal_expand'].divide(n_models)\n",
    "\n",
    "EnsembleList.append(ensemble)\n",
    "\n",
    "# Save ensemble predictions\n",
    "\n",
    "predstore_calib = level +  '_' + ensemble['modelname'] + '_calib'\n",
    "ensemble['calib_df_calibrated'].forecasts.set_run(run_id)\n",
    "ensemble['calib_df_calibrated'].forecasts.to_store(name=predstore_calib, overwrite = True)\n",
    "predstore_test = level +  '_' + ensemble['modelname'] + '_test'\n",
    "ensemble['test_df_calibrated'].forecasts.set_run(run_id)\n",
    "ensemble['test_df_calibrated'].forecasts.to_store(name=predstore_test, overwrite = True)\n",
    "if IncludeFuture:\n",
    "    predstore_future = level +  '_' + ensemble['modelname'] + '_future'\n",
    "    ensemble['future_df_calibrated'].forecasts.set_run(run_id)\n",
    "    ensemble['future_df_calibrated'].forecasts.to_store(name=predstore_future, overwrite = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0894d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "EnsembleList[-10]['future_df_calibrated'].xs(78, level=1).plot(kind='line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69270698",
   "metadata": {},
   "outputs": [],
   "source": [
    "CreateAblatedEnsemble = False\n",
    "if CreateAblatedEnsemble:\n",
    "    # Create ablated unweighted average ensemble\n",
    "    AblatedEnsembleList = EnsembleList.copy()\n",
    "    AblatedEnsembleList.pop(0)\n",
    "    AblatedEnsembleList.pop(0)\n",
    "    AblatedEnsembleList.pop(19)\n",
    "    AblatedEnsembleList.pop(23)\n",
    "    AblatedEnsembleList.pop(23)\n",
    "    AblatedEnsembleList.pop(25)\n",
    "    AblatedEnsembleList.pop(-1)\n",
    "\n",
    "    m = 0\n",
    "    for model in AblatedEnsembleList:\n",
    "        print(m, model['modelname'])\n",
    "        m = m + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd0ee98",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Model: Ensemble, ablated\n",
    "if CreateAblatedEnsemble:\n",
    "    ensemble = {\n",
    "        'modelname': 'ensemble_unw_ablated',\n",
    "        'run': '',\n",
    "        'depvar': \"ln_ged_sb_dep\",\n",
    "        'data': '',\n",
    "        'loggeddepvar': True,\n",
    "        'predictions_file_calib': \"\",\n",
    "        'predictions_file_test': \"\",\n",
    "        'fi': \"\",\n",
    "        'trained': False,\n",
    "        'calib_df_calibrated': EnsembleList[0]['calib_df_calibrated'].copy(),\n",
    "        'test_df_calibrated': EnsembleList[0]['test_df_calibrated'].copy()\n",
    "    }\n",
    "\n",
    "    n_models = 1\n",
    "    for model in AblatedEnsembleList:\n",
    "        ensemble['calib_df_calibrated'] = ensemble['calib_df_calibrated'].add(model['calib_df_calibrated'])\n",
    "        ensemble['test_df_calibrated'] = ensemble['test_df_calibrated'].add(model['test_df_calibrated'])\n",
    "        n_models = n_models + 1\n",
    "    #n_models = 1\n",
    "\n",
    "    ensemble['calib_df_calibrated'] = ensemble['calib_df_calibrated'].divide(n_models)\n",
    "    ensemble['test_df_calibrated'] = ensemble['test_df_calibrated'].divide(n_models)\n",
    "    EnsembleList.append(ensemble)\n",
    "    # Save ensemble predictions\n",
    "    path = Mydropbox + '/Projects/PredictingFatalities/Predictions/'\n",
    "    filename_test = path + 'EnsembleEqualWeights_ablated' + '_test.csv'\n",
    "    ensemble['test_df_calibrated'].to_csv(filename_test)\n",
    "\n",
    "    ensemble['test_df_calibrated'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0d69a6",
   "metadata": {},
   "source": [
    "# Create two 'mask' dfs to emulate 'onset' categories\n",
    "Based on predictions from the baseline model: 'Onsets' are defined as country years where the baseline model assigns a low expected count of deaths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5b8836",
   "metadata": {},
   "outputs": [],
   "source": [
    "onset_mask_test = EnsembleList[0]['predictions_test_df'].copy()\n",
    "onset_mask_calib = EnsembleList[0]['predictions_calib_df'].copy()\n",
    "cut_bins = [0,0.001,0.1,1,10]\n",
    "for df in [onset_mask_test, onset_mask_calib]:\n",
    "    for col in df.columns:\n",
    "        df[col] = pd.cut(df[col], cut_bins,right=False,labels=('grp0','grp1','grp2','grp3'))\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90c6596",
   "metadata": {},
   "source": [
    "# Estimating ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ed4067",
   "metadata": {},
   "outputs": [],
   "source": [
    "target['y_calib'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ea364d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking missingness\n",
    "N=51\n",
    "df = EnsembleList[0]['predictions_test_df']\n",
    "#df = pd.DataFrame(target['y_test'])\n",
    "for col in df.iloc[: , :N].columns:\n",
    "    print(col,len(df[col]), 'missing:', df[col].isnull().sum(), 'infinity:', np.isinf(df).values.sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d4ef1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ablation MSE, calibration partition\n",
    "\n",
    "from numpy import array\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def ensemble_predictions(yhats, weights):\n",
    "    # make predictions\n",
    "    yhats = np.array(yhats)\n",
    "    # weighted sum across ensemble members\n",
    "    result = np.dot(weights,yhats)\n",
    "    return result\n",
    "\n",
    "def evaluate_ensemble(yhats, weights, test_y):\n",
    "    ensemble_y = ensemble_predictions(yhats,weights)\n",
    "    return mean_squared_error(ensemble_y, test_y)\n",
    "\n",
    "# normalize a vector to have unit norm\n",
    "def normalize(weights):\n",
    "    # calculate l1 vector norm\n",
    "    result = norm(weights, 1)\n",
    "    # check for a vector of all zeros\n",
    "    if result == 0.0:\n",
    "        return weights\n",
    "    # return normalized vector (unit norm)\n",
    "    return weights / result\n",
    "\n",
    "\n",
    "ensemble_mses = [] # List to hold unweighted ensemble mses \n",
    "\n",
    "# Count models, set up lists\n",
    "number_of_models = 0\n",
    "mlist = []\n",
    "for model in EnsembleList[0:-1]:\n",
    "    number_of_models = number_of_models + 1\n",
    "    model['Ablation_MSE']=[0] * (len(steps)+1)\n",
    "    mlist.append(model['modelname'])\n",
    "print('Models:',number_of_models)        \n",
    "\n",
    "# Compute unweighted ensemble mses\n",
    "for col in stepcols:\n",
    "#    print(col)\n",
    "    yhats = []\n",
    "    weights = []\n",
    "    for model in EnsembleList[0:-1]:\n",
    "        df_calib = model['calib_df_calibrated'][~np.isinf(model['calib_df_calibrated'][col])].fillna(0)\n",
    "        yhats.append(df_calib[col])\n",
    "        weights.append(1/number_of_models)\n",
    "    emse = evaluate_ensemble(yhats, weights, df_calib['ln_ged_sb_dep'])\n",
    "    ensemble_mses.append(emse)\n",
    "\n",
    "#print('Unweighted ensemble MSEs:',ensemble_mses)\n",
    "\n",
    "# Compute ablation scores\n",
    "colno = 0\n",
    "for col in stepcols:\n",
    "    print('Step',col)\n",
    "    weights = []\n",
    "    for model in EnsembleList[0:-1]: # Assuming the ablated ensemble exists!\n",
    "        model['calib_df_calibrated'] = model['calib_df_calibrated'].fillna(0)\n",
    "#        print('Model to compute ablation MSE for',model['modelname'])\n",
    "        yhats = []\n",
    "        weights = []\n",
    "        for abl_model in EnsembleList[0:-1]:\n",
    "            abl_model['calib_df_calibrated'] = abl_model['calib_df_calibrated'].fillna(0) # Not sure what is best to do with NAs\n",
    "            y = model['calib_df_calibrated']['ln_ged_sb_dep'][~np.isinf(model['calib_df_calibrated'][col])]\n",
    "            if model['modelname'] != abl_model['modelname']:\n",
    "#                print('Model in ablated ensemble', abl_model['modelname'])\n",
    "                df_calib = abl_model['calib_df_calibrated'][~np.isinf(abl_model['calib_df_calibrated'][col])]\n",
    "                yhats.append(df_calib[col])\n",
    "                weights.append(1/(number_of_models-1))\n",
    "        ablated_mse = evaluate_ensemble(yhats, weights, y)\n",
    "        Ablation_MSE = ensemble_mses[colno] - ablated_mse\n",
    "        \n",
    "#        print(model['modelname'], 'ablated_mse:', ablated_mse, 'ensemble mse:',ensemble_mses[colno],'Ablation:' ,Ablation_MSE)\n",
    "        model['Ablation_MSE'][colno] = Ablation_MSE\n",
    "    colno = colno + 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ab1814",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "mean(EnsembleList[0]['Ablation_MSE'][1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c35098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go through the ablation MSEfor pre-screening of ensemble inclusion\n",
    "# model['Include'] set to False if contribution is not positive in any of four step segments\n",
    "\n",
    "from statistics import mean, stdev\n",
    "m = 0\n",
    "for model in EnsembleList[:-1]:\n",
    "    model['Include'] = False\n",
    "    m_all = mean(model['Ablation_MSE'][1:36])\n",
    "    m_1 = mean(model['Ablation_MSE'][1:6])\n",
    "    m_2 = mean(model['Ablation_MSE'][7:12])\n",
    "    m_3 = mean(model['Ablation_MSE'][13:24])\n",
    "    m_4 = mean(model['Ablation_MSE'][25:36])\n",
    "    for pmean in [m_all, m_1, m_2, m_3, m_4]:\n",
    "        if pmean < 0:\n",
    "            model['Include'] = True\n",
    "    \n",
    "    print(m, model['Include'], model['modelname'], ', aMSE steps all, 1-6, 7-12, 13-24, 25-36', \n",
    "          f'{m_all:.4f}',f'{m_1:.4f}',f'{m_2:.4f}',f'{m_3:.4f}',f'{m_4:.4f}',)\n",
    "    m = m + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd255f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing dfs to hold the predictions\n",
    "# A list of dictionaries organizing predictions and information as one step per entry,\n",
    "# including a dataframe for each step with one column per prediction model\n",
    "StepEnsembles = []\n",
    "for col in stepcols[1:]:\n",
    "    Step_prediction = {\n",
    "        'step_pred': col,\n",
    "        'df_calib': pd.DataFrame(target['y_calib']),\n",
    "        'df_test': pd.DataFrame(target['y_test']),\n",
    "        'ensembles_calib': pd.DataFrame(target['y_calib']),\n",
    "        'ensembles_test': pd.DataFrame(target['y_test'])\n",
    "    }\n",
    "    for model in EnsembleList:\n",
    "        modelname = model['modelname']\n",
    "        Step_prediction['df_calib'][modelname] = model['calib_df_calibrated'][col]\n",
    "        Step_prediction['df_test'][modelname] = model['test_df_calibrated'][col]\n",
    "    StepEnsembles.append(Step_prediction)\n",
    "\n",
    "# Calculating unweighted average ensembles\n",
    "i = 0\n",
    "for col in stepcols[1:]:\n",
    "    # Unweighted average\n",
    "    StepEnsembles[i]['ensembles_test']['unweighted_average'] = StepEnsembles[i]['df_test'].drop('ln_ged_sb_dep', axis=1).mean(axis=1)\n",
    "    StepEnsembles[i]['ensembles_calib']['unweighted_average'] = StepEnsembles[i]['df_calib'].drop('ln_ged_sb_dep', axis=1).mean(axis=1)\n",
    "    i = i + 1\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1244628",
   "metadata": {},
   "source": [
    "# Genetic algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fa392d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the import\n",
    "model = EnsembleList[0]\n",
    "model.keys()\n",
    "len(EnsembleList)\n",
    "print(RunGeneticAlgo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e75ea3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from genetic2 import *\n",
    "\n",
    "def make_run_from_step (\n",
    "    step,\n",
    "    e_set,\n",
    "    df_name = 'calib_df_calibrated',\n",
    "    target = 'ln_ged_sb_dep',\n",
    "    population_count = 100,\n",
    "    initial_population = None,\n",
    "    base_genes = np.array([0,1]),\n",
    "    number_of_generations = 500\n",
    "):\n",
    "    \"\"\"\n",
    "    step : step you want as an int,\n",
    "    ensemble_set : structure of the EnsembleList type,\n",
    "    target = Y in prediction,\n",
    "    df_name = name of the df in the ensemble set you want.\n",
    "    \"\"\"\n",
    "    \n",
    "    df_step = f'step_pred_{step}'\n",
    "    \n",
    "    try: \n",
    "        del aggregate_df\n",
    "    except NameError:\n",
    "        pass \n",
    "    \n",
    "    for i_ens in EnsembleList:\n",
    "        try:\n",
    "            #Join the step from the model into the ensemble df if it exists.\n",
    "            aggregate_df = aggregate_df.join(i_ens[df_name][[df_step]], rsuffix=f'_{i_ens[\"modelname\"]}')\n",
    "        except NameError:\n",
    "            #If the ensemble df does not exist create it and include the target.\n",
    "            aggregate_df = i_ens[df_name][[target,df_step]].copy()\n",
    "            aggregate_df = aggregate_df.rename(columns = {df_step : f'{df_step}_{i_ens[\"modelname\"]}'})\n",
    "    \n",
    "    aggregate_df = aggregate_df.dropna()\n",
    "    aggregate_df = aggregate_df[aggregate_df.columns[~aggregate_df.columns.str.contains('ensemble')]]\n",
    "    \n",
    "    X = aggregate_df.copy(); del X[target]\n",
    "    Y = aggregate_df[target]\n",
    "    \n",
    "    inst_mse = partial(weighted_mse_score, Y, X, mean_squared_error)\n",
    "    if initial_population is None:\n",
    "        population =  init_population_sum(population_count,base_genes,X.shape[1],0.5,3)\n",
    "    else: \n",
    "        population = initial_population\n",
    "    \n",
    "    from genetic2 import temp_file_name\n",
    "    import os\n",
    "    Path('./exploration_pickle/').mkdir(parents=True, exist_ok=True) \n",
    "    pd.DataFrame({'step':[step], 'memoization_id':[temp_file_name]}).to_csv(f'exploration_pickle/id_{temp_file_name}.csv', index=False)\n",
    "    \n",
    "    generation = genetic_algorithm(population, \n",
    "                                   inst_mse, \n",
    "                                   base_genes, \n",
    "                                   f_thres=None, \n",
    "                                   ngen=number_of_generations, \n",
    "                                   pmut=0.2)\n",
    "    return {'step':step, 'memoization_id':temp_file_name, 'generation':generation}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da33992a",
   "metadata": {},
   "outputs": [],
   "source": [
    "EnsembleRun = []\n",
    "m = 0\n",
    "for model in EnsembleList[:-1]:\n",
    "    if model['Include'] == True:\n",
    "        EnsembleRun.append(model)\n",
    "        print(m,model['modelname'])\n",
    "        m = m + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9bcfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "super_walrus_genes = np.array([0, 0.001, 0.002, 0.003, 0.005, 0.007, 0.010, 0.015, 0.020, 0.025, 0.030, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.10, 0.12, 0.14, 0.16, 0.18, 0.20, 0.25])\n",
    "steps_to_optimize = [1,2,3,4,6,9,12,15,18,24,30,36]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c5c5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "filled_function = partial(make_run_from_step, \n",
    "    e_set = EnsembleRun,\n",
    "    df_name = 'calib_df_calibrated',\n",
    "    target = 'ln_ged_sb_dep',\n",
    "    population_count = 100,\n",
    "    initial_population = None,\n",
    "    base_genes = super_walrus_genes,\n",
    "    number_of_generations = 500\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bb3af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cpus = cpu_count()-4 if cpu_count()>2 else 1\n",
    "cpus - len(steps_to_optimize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd3e0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RunGeneticAlgo:\n",
    "    generations = Parallel(n_jobs=cpus)(delayed(filled_function)(i) for i in steps_to_optimize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7416da00",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('exploration_pickle/full_gen.pickle', 'wb') as handle:\n",
    "    pkl.dump(generations, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39249a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The output contains a list of dictionaries. Each dictionary contains three elements:\n",
    "# 1. the step that was optimized.\n",
    "# 2. the memoization ID.\n",
    "# 3. A generation object, containing a tuple: (a list of organisms sorted by score descending; scores)\n",
    "generations[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81e6abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained weights from file so that it doesn't die.\n",
    "with open('exploration_pickle/full_gen.pickle', 'rb') as f:\n",
    "    generations = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38014622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the memoization id's so that you can explore the training process in the visualizer\n",
    "for i in generations:\n",
    "    print (i['step'], i['memoization_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68101aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the best organism.\n",
    "for gen in generations:\n",
    "    print ('\\nStep: ',gen['step'],'\\n','*'*24,'\\n')\n",
    "    print (gen['generation'][0])\n",
    "    #The best is always the top organism. You can get the top 20 by slicing gen['generation'][0:20] and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3eccc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results:\n",
    "\n",
    "GeneticAlgoResult = [\n",
    "{'Org': [0.2, 0.0, 0.0, 0.18, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05, 0.0, 0.02, 0.0, 0.09, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25],'Fitness': 3.735356478485664,'Step':36},\n",
    "{'Org': [0.0, 0.0, 0.02, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.003, 0.0, 0.03, 0.0, 0.003, 0.25, 0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25],'Fitness':4.317382478784119,'Step':30},\n",
    "{'Org': [0.0, 0.0, 0.001, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.14, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.2, 0.0, 0.001, 0.015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18],'Fitness':4.9754311051495375,'Step':24},\n",
    "{'Org': [0.0, 0.0, 0.09, 0.09, 0.14, 0.0, 0.16, 0.0, 0.0, 0.001, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005, 0.0, 0.01, 0.0, 0.0, 0.0, 0.18, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.05, 0.0, 0.0, 0.0, 0.025, 0.0],'Fitness':6.379993395323842,'Step':18},\n",
    "{'Org': [0.03, 0.0, 0.1, 0.2, 0.0, 0.0, 0.16, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015, 0.0, 0.05, 0.002, 0.01, 0.1, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.003, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.007, 0.0, 0.0, 0.09, 0.0],'Fitness':7.45380689233033,'Step':15},\n",
    "{'Org': [0.003, 0.0, 0.16, 0.16, 0.003, 0.0, 0.03, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05, 0.0, 0.001, 0.007, 0.001, 0.001, 0.0, 0.0, 0.003, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.002, 0.05, 0.0, 0.04, 0.0, 0.0, 0.0, 0.007, 0.16, 0.0, 0.0, 0.08, 0.01],'Fitness':9.288744127966044,'Step':12},\n",
    "{'Org': [0.1, 0.001, 0.16, 0.04, 0.025, 0.0, 0.18, 0.0, 0.0, 0.14, 0.12, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03, 0.0, 0.0, 0.002, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015, 0.0, 0.0, 0.02, 0.0, 0.0, 0.025, 0.12],'Fitness':10.003185652932892,'Step':9},\n",
    "{'Org': [0.08, 0.08, 0.2, 0.05, 0.0, 0.0, 0.0, 0.005, 0.0, 0.1, 0.18, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.025, 0.001, 0.03, 0.001, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.14],'Fitness':12.062857149691187,'Step':6},\n",
    "{'Org': [0.0, 0.0, 0.14, 0.18, 0.0, 0.0, 0.003, 0.06, 0.0, 0.01, 0.005, 0.0, 0.0, 0.002, 0.0, 0.0, 0.0, 0.0, 0.09, 0.0, 0.12, 0.0, 0.001, 0.0, 0.0, 0.06, 0.05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.003, 0.0, 0.0, 0.05, 0.0, 0.0, 0.14, 0.08],'Fitness':14.844532043805717,'Step':4},\n",
    "{'Org': [0.0, 0.0, 0.003, 0.01, 0.0, 0.0, 0.0, 0.2, 0.0, 0.005, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12, 0.001, 0.001, 0.1, 0.0, 0.0, 0.25, 0.06, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.14, 0.09],'Fitness':16.607509430086104,'Step':3},\n",
    "{'Org': [0.01, 0.0, 0.06, 0.003, 0.0, 0.0, 0.18, 0.18, 0.0, 0.0, 0.007, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04, 0.0, 0.0, 0.0, 0.2, 0.18, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.007, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.025, 0.0, 0.0, 0.1, 0.01],'Fitness':23.24984770618241,'Step':2},\n",
    "{'Org': [0.025, 0.0, 0.001, 0.015, 0.12, 0.0, 0.07, 0.2, 0.0, 0.1, 0.12, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08, 0.0, 0.003, 0.002, 0.0, 0.007, 0.07, 0.005, 0.002, 0.001, 0.0, 0.0, 0.0, 0.007, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015, 0.0, 0.001, 0.1, 0.0, 0.0, 0.07, 0.0],'Fitness':38.53772692394461,'Step':1},\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04772c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(GeneticAlgoResult[0]['Org'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d019396a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list some weights\n",
    "modelno = 48\n",
    "for line in GeneticAlgoResult: \n",
    "    print('Weight model', modelno, 'step', line['Step'], ':', line['Org'][modelno])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a620d94",
   "metadata": {},
   "source": [
    "# Assignment of the genetic weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81bd114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading from GeneticAlgoResult:\n",
    "w_step = [None] * 37\n",
    "for line in GeneticAlgoResult:\n",
    "    w_step[line['Step']] = line['Org']\n",
    "w_step[1]\n",
    "print(sum(w_step[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131c8115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear interpolation of weights:\n",
    "print(steps_to_optimize)\n",
    "WeightMatrix = [None] * 37\n",
    "modelnames = []\n",
    "for model in EnsembleList[:-1]: \n",
    "    modelnames.append(model['modelname'])\n",
    "for step in steps:\n",
    "    if step in steps_to_optimize:\n",
    "#        print(step, 'is optimized')\n",
    "        WeightMatrix[step] = w_step[step]\n",
    "    else:\n",
    "        WeightMatrix[step] = np.nan * len(w_step[1])\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf349626",
   "metadata": {},
   "outputs": [],
   "source": [
    "WeightMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccc1a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for model in EnsembleList[:-1]:\n",
    "    if model['Include'] == True:\n",
    "        EnsembleRun.append(model)\n",
    "        print(m,model['modelname'])\n",
    "        m = m + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae2afde",
   "metadata": {},
   "outputs": [],
   "source": [
    "StepAssigner = [1,2,3,4,4,6,6,9,9,9,12,12,12,15,15,15,18,18,18,18,18,24,24,24,24,24,24,30,30,30,30,30,30,36,36,36]\n",
    "WeightMatrix = [None] * 37\n",
    "modelnames = []\n",
    "for model in EnsembleList[:-1]: \n",
    "    modelnames.append(model['modelname'])\n",
    "\n",
    "for step in steps:\n",
    "#    print('Step',step,'assigned',StepAssigner[step-1])\n",
    "    WeightMatrix[step] = w_step[StepAssigner[step-1]]\n",
    "wmt = np.array(WeightMatrix[1:]).T\n",
    "weights_df = pd.DataFrame(wmt,columns=stepcols[1:],index=modelnames)\n",
    "weights_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a31695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolated weights\n",
    "i_weights_df = weights_df.copy()\n",
    "for step in steps:\n",
    "    col = 'step_pred_' + str(step)\n",
    "    if step == 5:\n",
    "        prestepcol = 'step_pred_' + str(step-1)\n",
    "        \n",
    "        poststepcol = 'step_pred_' + str(step+1)\n",
    "        i_weights_df[col] = (i_weights_df[prestepcol] + i_weights_df[poststepcol]) / 2\n",
    "    if step == 7 or step == 10 or step == 13 or step == 16:\n",
    "        prestepcol = 'step_pred_' + str(step-1)\n",
    "        poststepcol = 'step_pred_' + str(step+2)\n",
    "        i_weights_df[col] = ((i_weights_df[prestepcol]*2) + (i_weights_df[poststepcol]*1)) / 3\n",
    "    if step == 8 or step == 11 or step == 14 or step == 17:\n",
    "        prestepcol = 'step_pred_' + str(step-2)\n",
    "        poststepcol = 'step_pred_' + str(step+1)\n",
    "        i_weights_df[col] = ((i_weights_df[prestepcol]*1) + (i_weights_df[poststepcol]*2)) / 3\n",
    "    if step == 19 or step == 25 or step == 31:\n",
    "        prestepcol = 'step_pred_' + str(step-1)\n",
    "        poststepcol = 'step_pred_' + str(step+5)\n",
    "        i_weights_df[col] = ((i_weights_df[prestepcol]*5) + (i_weights_df[poststepcol]*1)) / 6\n",
    "    if step == 20 or step == 26 or step == 32:\n",
    "        prestepcol = 'step_pred_' + str(step-2)\n",
    "        poststepcol = 'step_pred_' + str(step+3)\n",
    "        i_weights_df[col] = ((i_weights_df[prestepcol]*4) + (i_weights_df[poststepcol]*2)) / 6\n",
    "    if step == 21 or step == 27 or step == 33:\n",
    "        prestepcol = 'step_pred_' + str(step-3)\n",
    "        poststepcol = 'step_pred_' + str(step+3)\n",
    "        i_weights_df[col] = ((i_weights_df[prestepcol]*3) + (i_weights_df[poststepcol]*3)) / 6\n",
    "    if step == 22 or step == 28 or step == 34:\n",
    "        prestepcol = 'step_pred_' + str(step-4)\n",
    "        poststepcol = 'step_pred_' + str(step+2)\n",
    "        i_weights_df[col] = ((i_weights_df[prestepcol]*2) + (i_weights_df[poststepcol]*4)) / 6\n",
    "    if step == 23 or step == 29 or step == 35:\n",
    "        prestepcol = 'step_pred_' + str(step-5)\n",
    "        poststepcol = 'step_pred_' + str(step+1)\n",
    "        i_weights_df[col] = ((i_weights_df[prestepcol]*1) + (i_weights_df[poststepcol]*5)) / 6\n",
    "        \n",
    "print(steps_to_optimize)\n",
    "i_weights_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b9aa6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export weights \n",
    "i_weights_df.to_csv('GeneticWeights.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78a1d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "palette = 'vlag'\n",
    "palette = sns.color_palette('BrBG',n_colors=50)\n",
    "palette = sns.cubehelix_palette(start=2, rot=0, dark=0, light=1, n_colors=100)\n",
    "overleafpath = '/Users/havardhegre/Dropbox (ViEWS)/Apps/Overleaf/ViEWS predicting fatalities/Figures/Pred_Eval/'\n",
    "\n",
    "fig, ax =plt.subplots(1,figsize=(16,11))\n",
    "ax = sns.heatmap(i_weights_df, xticklabels=2, linewidths=.5, cmap=palette,square=True)\n",
    "filename = overleafpath + 'genetic_weights.png'\n",
    "plt.savefig(filename, dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff32635",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculating weighted average ensembles\n",
    "# Based on the weights_df dataframe filled with Mihai's weights above\n",
    "\n",
    "def ensemble_predictions(yhats, weights):\n",
    "    # make predictions\n",
    "    yhats = np.array(yhats)\n",
    "    # weighted sum across ensemble members\n",
    "    result = np.dot(weights,yhats)\n",
    "    return result\n",
    "\n",
    "# normalize a vector to have unit norm\n",
    "def normalize(weights):\n",
    "    # calculate l1 vector norm\n",
    "    result = norm(weights, 1)\n",
    "    # check for a vector of all zeros\n",
    "    if result == 0.0:\n",
    "        return weights\n",
    "    # return normalized vector (unit norm)\n",
    "    return weights / result\n",
    "\n",
    "i = 0\n",
    "for col in stepcols[1:]:\n",
    "    # Unweighted average\n",
    "    df_calib = StepEnsembles[i]['df_calib'].drop('ln_ged_sb_dep', axis=1)\n",
    "    df_test = StepEnsembles[i]['df_test'].drop('ln_ged_sb_dep', axis=1)\n",
    "    StepEnsembles[i]['ensembles_calib']['weighted_average'] = (df_calib*i_weights_df[col]).sum(axis=1)\n",
    "    StepEnsembles[i]['ensembles_test']['weighted_average'] =  (df_test*i_weights_df[col]).sum(axis=1)\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9358871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the weights dfs\n",
    "dflist = [\n",
    "    (i_weights_df,'i_weights_df'), \n",
    "]\n",
    "\n",
    "path = '/Users/havardhegre/Dropbox (ViEWS)/ViEWS/Projects/PredictingFatalities/MSEs/'\n",
    "for df in dflist:\n",
    "    filename = path + df[1] + '.csv'\n",
    "    df[0].to_csv(filename)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5f571a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating some ensembles\n",
    "from sklearn import datasets, linear_model\n",
    "if False:\n",
    "\n",
    "    i = 0\n",
    "    for col in stepcols[1:]:    # Linear regression\n",
    "        regr = linear_model.LinearRegression()\n",
    "        df = StepEnsembles[i]['df_calib'][~np.isinf(StepEnsembles[i]['df_calib'])].fillna(0)\n",
    "        regr.fit(df.drop('ln_ged_sb_dep', axis=1), df['ln_ged_sb_dep'])\n",
    "    #    regr.fit(StepEnsembles[i]['df_calib'].drop('lndepvar', axis=1), StepEnsembles[i]['df_calib']['lndepvar'])\n",
    "        StepEnsembles[i]['ensembles_calib']['linear_regression'] = regr.predict(StepEnsembles[i]['df_calib'].drop('ln_ged_sb_dep', axis=1))\n",
    "        StepEnsembles[i]['ensembles_test']['linear_regression'] = regr.predict(StepEnsembles[i]['df_test'].drop('ln_ged_sb_dep', axis=1))\n",
    "        # Random forest regression\n",
    "        rf = RandomForestRegressor()\n",
    "        rf.fit(StepEnsembles[i]['df_calib'].drop('ln_ged_sb_dep', axis=1), StepEnsembles[i]['df_calib']['ln_ged_sb_dep'])\n",
    "        StepEnsembles[i]['ensembles_calib']['rf_regression'] = rf.predict(StepEnsembles[i]['df_calib'].drop('ln_ged_sb_dep', axis=1))\n",
    "        StepEnsembles[i]['ensembles_test']['rf_regression'] = rf.predict(StepEnsembles[i]['df_test'].drop('ln_ged_sb_dep', axis=1))\n",
    "        i = i + 1\n",
    "\n",
    "    ensemble_models = ['unweighted_average','linear_regression','rf_regression']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a179e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the ensemble predictions\n",
    "\n",
    "if False: \n",
    "    linear = {\n",
    "        'modelname': 'ensemble_linear',\n",
    "        'algorithm': '',\n",
    "        'depvar': \"ln_ged_sb_dep\",\n",
    "        'calib_df_calibrated': EnsembleList[0]['calib_df_calibrated'].copy(),\n",
    "        'test_df_calibrated': EnsembleList[0]['test_df_calibrated'].copy()\n",
    "    }\n",
    "    rf = {\n",
    "        'modelname': 'ensemble_rf',\n",
    "        'algorithm': '',\n",
    "        'depvar': \"ln_ged_sb_dep\",\n",
    "        'calib_df_calibrated': EnsembleList[0]['calib_df_calibrated'].copy(),\n",
    "        'test_df_calibrated': EnsembleList[0]['test_df_calibrated'].copy()\n",
    "    }\n",
    "genetic = {\n",
    "        'modelname': 'ensemble_genetic',\n",
    "        'algorithm': '',\n",
    "        'depvar': \"ln_ged_sb_dep\",\n",
    "        'calib_df_calibrated': EnsembleList[0]['calib_df_calibrated'].copy(),\n",
    "        'test_df_calibrated': EnsembleList[0]['test_df_calibrated'].copy(),\n",
    "        'calibration_gam': []\n",
    "    }    \n",
    "\n",
    "for step in StepEnsembles:\n",
    "    colname = step['step_pred']\n",
    "    print(colname)\n",
    "#    linear['calib_df_calibrated'][colname] = step['ensembles_calib']['linear_regression']\n",
    "#    linear['test_df_calibrated'][colname] = step['ensembles_test']['linear_regression']\n",
    "#    rf['calib_df_calibrated'][colname] = step['ensembles_calib']['rf_regression']\n",
    "#    rf['test_df_calibrated'][colname] = step['ensembles_test']['rf_regression']\n",
    "    genetic['calib_df_calibrated'][colname] = step['ensembles_calib']['weighted_average']\n",
    "    genetic['test_df_calibrated'][colname] = step['ensembles_test']['weighted_average']\n",
    "   \n",
    "#EnsembleList.append(linear)\n",
    "#EnsembleList.append(rf)\n",
    "# Adding placeholder data for the old calibration method for the weighted ensemble\n",
    "\n",
    "genetic['calib_df_cal_expand'] = genetic['calib_df_calibrated']\n",
    "genetic['test_df_cal_expand'] = genetic['test_df_calibrated']\n",
    "EnsembleList.append(genetic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ae7ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save ensemble predictions\n",
    "IncludeFuture = False\n",
    "predstore_calib = level +  '_' + genetic['modelname'] + '_calib'\n",
    "genetic['calib_df_calibrated'].forecasts.set_run(run_id)\n",
    "genetic['calib_df_calibrated'].forecasts.to_store(name=predstore_calib, overwrite = True)\n",
    "predstore_test = level +  '_' + genetic['modelname'] + '_test'\n",
    "genetic['test_df_calibrated'].forecasts.set_run(run_id)\n",
    "genetic['test_df_calibrated'].forecasts.to_store(name=predstore_test, overwrite = True)\n",
    "if IncludeFuture:\n",
    "    predstore_future = level +  '_' + genetic['modelname'] + '_future'\n",
    "    genetic['future_df_calibrated'].forecasts.set_run(run_id)\n",
    "    genetic['future_df_calibrated'].forecasts.to_store(name=predstore_future, overwrite = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9fc71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check they are there\n",
    "ViewsMetadata().mine().with_name('genetic').fetch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5ec6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving revised model list, stripped down in dropbox: \n",
    "dbpath = '/Users/havardhegre/Dropbox (ViEWS)/ViEWS/Projects/PredictingFatalities/Predictions/'\n",
    "db_EnsembleList = []\n",
    "for model in EnsembleList:\n",
    "    print(model['modelname'])\n",
    "    if 'Markov' in model['modelname']:\n",
    "        db_dict = {\n",
    "            'modelname':       model['modelname'],\n",
    "            'algorithm':       '',\n",
    "            'depvar':          model['depvar'],\n",
    "            'queryset':        model['queryset'],\n",
    "            'calibration_gam': []\n",
    "        }\n",
    "    elif 'ensemble' in model['modelname']:\n",
    "        db_dict = {\n",
    "            'modelname':       model['modelname'],\n",
    "            'algorithm':       '',\n",
    "            'depvar':          model['depvar'],\n",
    "            'queryset':        '',\n",
    "            'calibration_gam': []\n",
    "        }\n",
    "    else:\n",
    "        db_dict = {\n",
    "            'modelname':       model['modelname'],\n",
    "            'algorithm':       str(model['algorithm']),\n",
    "            'depvar':          model['depvar'],\n",
    "            'queryset':        model['queryset'],\n",
    "            'calibration_gam': model['calibration_gam']\n",
    "        }\n",
    "    db_EnsembleList.append(db_dict)\n",
    "picklename = dbpath + 'ModelList_calibrated_db_cm_' + run_id + '.p'\n",
    "#pkl.dump(ModelList, open(picklename, \"wb\" ) )\n",
    "pkl.dump(db_EnsembleList, open(picklename, \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0547d009",
   "metadata": {},
   "outputs": [],
   "source": [
    "EnsembleList[1]['calibration_gam']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e18061",
   "metadata": {},
   "source": [
    "# Correlation of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd21970d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate \n",
    "meancorr_df = pd.DataFrame(StepEnsembles[0]['df_calib'].corr().mean(axis=1))\n",
    "for step in [2,5,11,23,35]:\n",
    "    colname = 'step_' + str(step+1)\n",
    "    meancorr_df[colname] = StepEnsembles[step]['df_calib'].corr().mean(axis=1)\n",
    "meancorr_df['average'] = meancorr_df.mean(axis=1)\n",
    "meancorr_df\n",
    "# Save the corr dfs\n",
    "dflist = [\n",
    "    (meancorr_df,'meancorr_df'), \n",
    "]\n",
    "\n",
    "path = '/Users/havardhegre/Dropbox (ViEWS)/ViEWS/Projects/PredictingFatalities/MSEs/'\n",
    "for df in dflist:\n",
    "    filename = path + df[1] + '.csv'\n",
    "    df[0].to_csv(filename)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc713bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "level = 'cm'\n",
    "\n",
    "cols_to_see = ['lndepvar', 'fat_hh20_xgb', 'fat_hh20_hurdle_xgb','fat_hh20_rf','fat_hh20_xgbrf']\n",
    "\n",
    "sns.set_context(\"notebook\")\n",
    "sns.set() # Setting seaborn as default style even if use only matplotlib\n",
    "path = '/Users/havardhegre/Dropbox (ViEWS)/ViEWS/Projects/PredictingFatalities/PredictionPlots/'\n",
    "overleafpath = '/Users/havardhegre/Dropbox (ViEWS)/Apps/Overleaf/ViEWS predicting fatalities/Figures/'\n",
    "cm = \"YlGnBu\" \n",
    "#cm = 'mako'\n",
    "#cm = 'rocket'\n",
    "fig, ((ax1, ax3), (ax6,ax12), (ax24,ax36)) = plt.subplots(3, 2, figsize=(30,30))\n",
    "for subplot in [(ax1,0),(ax3,2),(ax6,5),(ax12,11),(ax24,23),(ax36,35)]:\n",
    "    subplot[0].set_box_aspect(1)\n",
    "    sns.heatmap(StepEnsembles[subplot[1]]['df_calib'].corr(), ax=subplot[0], cmap=cm) \n",
    "    subplot_title=('step ' + str(subplot[1]+1))\n",
    "    subplot[0].set_title = subplot_title\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "figname = overleafpath + 'Correlations/PredictionCorrelations_calib_' + level + '.png'\n",
    "fig.savefig(figname, dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35aa39e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrdf = StepEnsembles[0]['df_test'].corr()\n",
    "corrdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b224ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "meancorr_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f09026d",
   "metadata": {},
   "source": [
    "# Create sc predictions and country prediction tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0df8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "EnsembleList[0]['test_df_calibrated'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c33eff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a set of step-combined series starting from a series of start months\n",
    "# -- to see how predictions react to events at different calendar times\n",
    "for model in EnsembleList:\n",
    "    print(model['modelname'])\n",
    "    df = model['test_df_calibrated']\n",
    "    last_in_training = 444\n",
    "    last_in_test = 492\n",
    "    # t_range specifies the duration of the step-combined series to construct\n",
    "    t_range = range(0, 48)\n",
    "    step_range = range(1,36)\n",
    "    model['sc_df'] = pd.DataFrame(df['ln_ged_sb_dep'])\n",
    "    for month in t_range:\n",
    "        # Create a column to hold predictions starting from a given \"last observed\" month\n",
    "        col = 'sc_' + str(last_in_training + month)\n",
    "#        print(col)\n",
    "        model['sc_df'][col] = np.NaN\n",
    "        for step in step_range:\n",
    "            if (last_in_training + month+step) <= last_in_test: # To avoid generating series beyond last month in partition\n",
    "                predcol = 'step_pred_' + str(step) # The column in the in-df that contains predictions for this step\n",
    "#                print('For month', last_in_training + month + step, 'use', predcol, last_in_training + step + month)\n",
    "                model['sc_df'][col].loc[[last_in_training + month + step], :] = df[predcol].loc[last_in_training + step + month,:].values\n",
    "    model['sc_df_smooth']=model['sc_df'].rolling(3,center=True).mean().groupby(level=1)   \n",
    "    # Sub-list of predictions by country \n",
    "\n",
    "    model['CountryList'] = []\n",
    "    countries = model['test_df_calibrated'].index.unique(level='country_id').tolist()\n",
    "    for cnt in range(250):\n",
    "        cntdict = {\n",
    "        'country_id': cnt,\n",
    "        'country_name': ''\n",
    "        }\n",
    "        if cnt in countries:\n",
    "            cntdict['country_id'] = cnt\n",
    "            cntdict['test_df_calibrated'] = model['test_df_calibrated'].xs(cnt, level='country_id').copy()\n",
    "            cntdict['sc_df'] = model['sc_df'].xs(cnt, level='country_id')\n",
    "            cntdict['sc_df_smooth'] = cntdict['sc_df'].copy()\n",
    "            predcols = cntdict['sc_df_smooth'].columns[1:]\n",
    "            cntdict['sc_df_smooth'][predcols] = cntdict['sc_df'][predcols].rolling(3,center=True).mean()\n",
    "            cntdict['sc_df_smooth'] = cntdict['sc_df_smooth'].fillna(cntdict['sc_df'])\n",
    "        model['CountryList'].append(cntdict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378e1e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(last_in_training, step, month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1031ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 1\n",
    "month = 1\n",
    "col = 'sc_445'\n",
    "df[predcol].loc[last_in_training + step + month,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60677876",
   "metadata": {},
   "outputs": [],
   "source": [
    "model['sc_df'][col].loc[[last_in_training + month + step], :] = df[predcol].loc[last_in_training + step + month,:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c91ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model['sc_df'][col].loc[[last_in_training + month + step], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddd6cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "EnsembleList[0]['test_df_calibrated'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8d6a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 246\n",
    "EnsembleList[0]['sc_df'].xs(cnt, level='country_id').head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b54733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select models from this data structure:\n",
    "modelname = 'baseline_rf'\n",
    "cid = 50\n",
    "step_to_inspect = 'step_pred_1'\n",
    "\n",
    "thismodel = [mod for mod in EnsembleList if mod['modelname'] == modelname][0]\n",
    "print(thismodel['test_df_calibrated'][step_to_inspect].describe())\n",
    "\n",
    "# Select country predictions for model:\n",
    "\n",
    "thiscm = [cnt for cnt in thismodel['CountryList'] if cnt['country_id'] == cid][0]\n",
    "print(thiscm['test_df_calibrated'][step_to_inspect].describe())\n",
    "\n",
    "thiscm2 = thismodel['CountryList'][cid]\n",
    "print(thiscm2['test_df_calibrated'][step_to_inspect].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447698b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect calibration dfs for the models in the ensemble:\n",
    "cols = ['ln_ged_sb_dep','step_pred_1','step_pred_12','step_pred_24','step_pred_36']\n",
    "pti = [.75,.90,.95,.99]\n",
    "for m in EnsembleList:\n",
    "    print(m['modelname'])\n",
    "    print('Before calibration')\n",
    "    print(m['predictions_calib_df'][cols].describe(percentiles=pti))\n",
    "    print(m['predictions_test_df'][cols].describe(percentiles=pti))\n",
    "#    print('Expanded parameter:',m['expanded'],'shiftsize parameter:',m['shiftsize'])\n",
    "    print('After calibration, calibration partition (top), test partition (bottom)')\n",
    "    print(m['calib_df_calibrated'][cols].describe(percentiles=pti))\n",
    "    print(m['test_df_calibrated'][cols].describe(percentiles=pti))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36c2c73",
   "metadata": {},
   "source": [
    "## Identify escalation and de-escalation periods based on sc for lndepvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746327e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(EnsembleList[-1]['modelname'])\n",
    "sc_df = EnsembleList[-1]['sc_df']\n",
    "\n",
    "# Rolling means\n",
    "sc_df['rolldep_3'] =  sc_df.groupby(level=1)['ln_ged_sb_dep'].transform(lambda x: x.rolling(3, 3).mean())\n",
    "sc_df['rolldep_6'] =  sc_df.groupby(level=1)['ln_ged_sb_dep'].transform(lambda x: x.rolling(6, 6).mean())\n",
    "sc_df['rolldep_10'] =  sc_df.groupby(level=1)['ln_ged_sb_dep'].transform(lambda x: x.rolling(10, 10).mean())\n",
    "sc_df['rolldep_3_lag'] =  sc_df.groupby(level=1)['rolldep_3'].shift(1)\n",
    "sc_df['rolldep_10_lag'] =  sc_df.groupby(level=1)['rolldep_10'].shift(1)\n",
    "\n",
    "# Escalation: \n",
    "# If log monthly fatalities is escalation_threshold = 2.398 (appr. log(10)) higher than last month's 10-month mean\n",
    "# or rolling 3-month mean is escalation_threshold = 2.398 higher than last month's 10-month mean and monthly > 0\n",
    "escalation_threshold = 1\n",
    "sc_df['escalation'] = np.nan\n",
    "sc_df['escalation'].loc[457:] = 0\n",
    "sc_df['escalation'][sc_df['ln_ged_sb_dep'] > sc_df['rolldep_3_lag'] + escalation_threshold] = 1\n",
    "sc_df['escalation'][sc_df['rolldep_3'] > sc_df['rolldep_10_lag'] + (escalation_threshold)] = 1\n",
    "\n",
    "cols_to_inspect = ['ln_ged_sb_dep','rolldep_3','rolldep_6','rolldep_10','escalation']\n",
    "# 57: Ethiopia\n",
    "# 47 Burkina Faso\n",
    "# 50 Mali\n",
    "# 79 Nigeria\n",
    "cnt= 79\n",
    "sc_df[cols_to_inspect].xs(cnt, level='country_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cd811e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_inspect = ['lndepvar','escalation']\n",
    "sc_df[cols_to_inspect].xs(cnt, level='country_id').plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d936eeba",
   "metadata": {},
   "source": [
    "## Evaluation of constituent models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72912449",
   "metadata": {},
   "outputs": [],
   "source": [
    "model['calib_df_calibrated'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b880099b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation of constituent models\n",
    "calculate_grpMSEs = False\n",
    "\n",
    "MSE_calib_all = []\n",
    "MSE_calib_zeros = []\n",
    "MSE_calib_nonzeros = []\n",
    "if calculate_grpMSEs:\n",
    "    MSE_calib_grp0 = []\n",
    "    MSE_calib_grp1 = []\n",
    "    MSE_calib_grp2 = []\n",
    "    MSE_calib_grp3 = []\n",
    "MSE_test_all = []\n",
    "MSE_test_zeros = []\n",
    "MSE_test_nonzeros = []\n",
    "MSE_test_exp_all = []\n",
    "MSE_test_exp_zeros = []\n",
    "MSE_test_exp_nonzeros = []\n",
    "\n",
    "for model in EnsembleList:\n",
    "    calib_all_line = [model['modelname']]\n",
    "    calib_zeros_line = [model['modelname']]\n",
    "    calib_nonzeros_line = [model['modelname']]\n",
    "    test_all_line = [model['modelname']]\n",
    "    test_zeros_line = [model['modelname']]\n",
    "    test_nonzeros_line = [model['modelname']]\n",
    "    test_exp_all_line = [model['modelname']]\n",
    "    test_exp_zeros_line = [model['modelname']]\n",
    "    test_exp_nonzeros_line = [model['modelname']]\n",
    "    print(model['modelname'])\n",
    "    model['mse_calib'] = []\n",
    "    model['mse_calib_zeros'] = []\n",
    "    model['mse_calib_nonzeros'] = []\n",
    "    model['mse_test'] = []\n",
    "    model['mse_test_zeros'] = []\n",
    "    model['mse_test_nonzeros'] = []\n",
    "    model['mse_test_exp'] = []\n",
    "    model['mse_test_exp_zeros'] = []\n",
    "    model['mse_test_exp_nonzeros'] = []\n",
    "    if calculate_grpMSEs:\n",
    "        calib_grp0_line = [model['modelname']]\n",
    "        calib_grp1_line = [model['modelname']]\n",
    "        calib_grp2_line = [model['modelname']]\n",
    "        calib_grp3_line = [model['modelname']]\n",
    "        model['mse_calib_grp0'] = []\n",
    "        model['mse_calib_grp1'] = []\n",
    "        model['mse_calib_grp2'] = []\n",
    "        model['mse_calib_grp3'] = []\n",
    "    for cnt in model['CountryList']:\n",
    "        if cnt['country_id'] in countries:\n",
    "            cnt['mse'] = []\n",
    "    for col in stepcols[1:]:\n",
    "        # Remove from evaluation rows where [col] has infinite values (due to the 2011 split of Sudan)\n",
    "        df_calib = model['calib_df_calibrated'][~np.isinf(model['calib_df_calibrated'][col])]\n",
    "        df_test = model['test_df_calibrated'][~np.isinf(model['test_df_calibrated'][col])]\n",
    "        df_test_exp = model['test_df_cal_expand'][~np.isinf(model['test_df_cal_expand'][col])]\n",
    "\n",
    "        mse_calib = mean_squared_error(df_calib[col], df_calib['ln_ged_sb_dep'])\n",
    "        model['mse_calib'].append(mse_calib)\n",
    "        calib_all_line.append(mse_calib)\n",
    "        \n",
    "        mse_calib_zeros = mean_squared_error(df_calib[col].loc[df_calib['ln_ged_sb_dep'] == 0], df_calib['ln_ged_sb_dep'].loc[df_calib['ln_ged_sb_dep'] == 0])\n",
    "        model['mse_calib_zeros'].append(mse_calib_zeros)\n",
    "        calib_zeros_line.append(mse_calib_zeros)\n",
    "        \n",
    "        mse_calib_nonzeros = mean_squared_error(df_calib[col].loc[df_calib['ln_ged_sb_dep'] > 0], df_calib['ln_ged_sb_dep'].loc[df_calib['ln_ged_sb_dep'] > 0])\n",
    "        model['mse_calib_nonzeros'].append(mse_calib_nonzeros)\n",
    "        calib_nonzeros_line.append(mse_calib_nonzeros)\n",
    "        \n",
    "        \n",
    "        if calculate_grpMSEs:\n",
    "            # MSE for groups of cases based on baseline model predictions:\n",
    "            # Group 0\n",
    "            df_calib_grp0 = df_calib[onset_mask_calib[col]=='grp0']\n",
    "            mse_calib_grp0 = mean_squared_error(df_calib_grp0[col], df_calib_grp0['ln_ged_sb_dep'])\n",
    "            model['mse_calib_grp0'].append(mse_calib_grp0)\n",
    "            calib_grp0_line.append(mse_calib_grp0)\n",
    "\n",
    "            # Group 1\n",
    "            df_calib_grp1 = df_calib[onset_mask_calib[col]=='grp1']\n",
    "            mse_calib_grp1 = mean_squared_error(df_calib_grp1[col], df_calib_grp1['ln_ged_sb_dep'])\n",
    "            model['mse_calib_grp1'].append(mse_calib_grp1)\n",
    "            calib_grp1_line.append(mse_calib_grp1)\n",
    "\n",
    "            # Group 2\n",
    "            df_calib_grp2 = df_calib[onset_mask_calib[col]=='grp2']\n",
    "            mse_calib_grp2 = mean_squared_error(df_calib_grp2[col], df_calib_grp2['ln_ged_sb_dep'])\n",
    "            model['mse_calib_grp2'].append(mse_calib_grp2)\n",
    "            calib_grp2_line.append(mse_calib_grp2)\n",
    "\n",
    "            # Group 3\n",
    "            df_calib_grp3 = df_calib[onset_mask_calib[col]=='grp3']\n",
    "            mse_calib_grp3 = mean_squared_error(df_calib_grp3[col], df_calib_grp3['ln_ged_sb_dep'])\n",
    "            model['mse_calib_grp3'].append(mse_calib_grp3)\n",
    "            calib_grp3_line.append(mse_calib_grp3)\n",
    "        \n",
    "        \n",
    "#        mse_test = mean_squared_error(model['predictions_test_df'][col], model['predictions_test_df']['ln_ged_sb_dep'])\n",
    "        mse_test = mean_squared_error(df_test[col], target['y_test'])\n",
    "        model['mse_test'].append(mse_test)\n",
    "        test_all_line.append(mse_test)\n",
    "        \n",
    "        mse_zeros = mean_squared_error(df_test[col].loc[df_test['ln_ged_sb_dep'] == 0], df_test['ln_ged_sb_dep'].loc[df_test['ln_ged_sb_dep'] == 0])\n",
    "        model['mse_test_zeros'].append(mse_zeros)\n",
    "        test_zeros_line.append(mse_zeros)\n",
    "        \n",
    "        mse_nonzeros = mean_squared_error(df_test[col].loc[df_test['ln_ged_sb_dep'] > 0], df_test['ln_ged_sb_dep'].loc[df_test['ln_ged_sb_dep'] > 0])\n",
    "        model['mse_test_nonzeros'].append(mse_nonzeros)\n",
    "        test_nonzeros_line.append(mse_nonzeros)\n",
    "\n",
    "        mse_test_exp = mean_squared_error(df_test_exp[col], target['y_test'])\n",
    "        model['mse_test_exp'].append(mse_test_exp)\n",
    "        test_exp_all_line.append(mse_test_exp)\n",
    "        \n",
    "        mse_exp_zeros = mean_squared_error(df_test_exp[col].loc[df_test_exp['ln_ged_sb_dep'] == 0], df_test_exp['ln_ged_sb_dep'].loc[df_test_exp['ln_ged_sb_dep'] == 0])\n",
    "        model['mse_test_exp_zeros'].append(mse_exp_zeros)\n",
    "        test_exp_zeros_line.append(mse_exp_zeros)\n",
    "        \n",
    "        mse_exp_nonzeros = mean_squared_error(df_test_exp[col].loc[df_test_exp['ln_ged_sb_dep'] > 0], df_test_exp['ln_ged_sb_dep'].loc[df_test_exp['ln_ged_sb_dep'] > 0])\n",
    "        model['mse_test_exp_nonzeros'].append(mse_exp_nonzeros)\n",
    "        test_exp_nonzeros_line.append(mse_exp_nonzeros)\n",
    "\n",
    "\n",
    "        countries = model['test_df_calibrated'].index.unique(level='country_id').tolist()\n",
    "        for cnt in model['CountryList']:\n",
    "            if cnt['country_id'] in countries:\n",
    "                df_test = cnt['test_df_calibrated'][~np.isinf(cnt['test_df_calibrated'][col])]            \n",
    "                cnt_mse = mean_squared_error(df_test[col], df_test['ln_ged_sb_dep'])\n",
    "                cnt['mse'].append(cnt_mse)\n",
    "        \n",
    "    MSE_calib_all.append(calib_all_line)\n",
    "    MSE_calib_zeros.append(calib_zeros_line)\n",
    "    MSE_calib_nonzeros.append(calib_nonzeros_line)\n",
    "    if calculate_grpMSEs:\n",
    "        MSE_calib_grp0.append(calib_grp0_line)\n",
    "        MSE_calib_grp1.append(calib_grp1_line)\n",
    "        MSE_calib_grp2.append(calib_grp2_line)\n",
    "        MSE_calib_grp3.append(calib_grp3_line)\n",
    "    MSE_test_all.append(test_all_line)\n",
    "    MSE_test_zeros.append(test_zeros_line)\n",
    "    MSE_test_nonzeros.append(test_nonzeros_line)\n",
    "    MSE_test_exp_all.append(test_exp_all_line)\n",
    "    MSE_test_exp_zeros.append(test_exp_zeros_line)\n",
    "    MSE_test_exp_nonzeros.append(test_exp_nonzeros_line)\n",
    "    \n",
    "MSE_calib_all_df = pd.DataFrame(MSE_calib_all, columns=stepcols) \n",
    "MSE_calib_all_df.set_index('ln_ged_sb_dep', inplace=True)\n",
    "MSE_calib_zeros_df = pd.DataFrame(MSE_calib_zeros, columns=stepcols) \n",
    "MSE_calib_zeros_df.set_index('ln_ged_sb_dep', inplace=True)\n",
    "MSE_calib_nonzeros_df = pd.DataFrame(MSE_calib_nonzeros, columns=stepcols) \n",
    "MSE_calib_nonzeros_df.set_index('ln_ged_sb_dep', inplace=True)\n",
    "if calculate_grpMSEs:\n",
    "    MSE_calib_grp0_df = pd.DataFrame(MSE_calib_grp0, columns=stepcols) \n",
    "    MSE_calib_grp1_df = pd.DataFrame(MSE_calib_grp1, columns=stepcols) \n",
    "    MSE_calib_grp2_df = pd.DataFrame(MSE_calib_grp2, columns=stepcols) \n",
    "    MSE_calib_grp3_df = pd.DataFrame(MSE_calib_grp3, columns=stepcols) \n",
    "MSE_test_all_df = pd.DataFrame(MSE_test_all, columns=stepcols)  \n",
    "MSE_test_zeros_df = pd.DataFrame(MSE_test_zeros, columns=stepcols)  \n",
    "MSE_test_nonzeros_df = pd.DataFrame(MSE_test_nonzeros, columns=stepcols)  \n",
    "MSE_test_exp_all_df = pd.DataFrame(MSE_test_exp_all, columns=stepcols)  \n",
    "MSE_test_exp_zeros_df = pd.DataFrame(MSE_test_exp_zeros, columns=stepcols)  \n",
    "MSE_test_exp_nonzeros_df = pd.DataFrame(MSE_test_exp_nonzeros, columns=stepcols)  \n",
    "\n",
    "print('All models done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f385c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the MSE dfs\n",
    "dflist = [\n",
    "    (MSE_calib_all_df,'MSE_calib_all_df'),\n",
    "    (MSE_calib_zeros_df,'MSE_calib_zeros_df'),\n",
    "    (MSE_calib_nonzeros_df,'MSE_calib_nonzeros_df'),\n",
    "    (MSE_test_all_df,'MSE_test_all_df'),\n",
    "    (MSE_test_zeros_df,'MSE_test_zeros_df'),\n",
    "    (MSE_test_nonzeros_df,'MSE_test_nonzeros_df'),   \n",
    "    (MSE_test_exp_all_df,'MSE_test_exp_all_df'),\n",
    "    (MSE_test_exp_zeros_df,'MSE_test_exp_zeros_df'),\n",
    "    (MSE_test_exp_nonzeros_df,'MSE_test_exp_nonzeros_df')   \n",
    "]\n",
    "\n",
    "path = '/Users/havardhegre/Dropbox (ViEWS)/ViEWS/Projects/PredictingFatalities/MSEs/'\n",
    "for df in dflist:\n",
    "    filename = path + df[1] + '.csv'\n",
    "    df[0].to_csv(filename)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71298f9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13fee1d2",
   "metadata": {},
   "source": [
    "## List global MSEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc975641",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in EnsembleList:\n",
    "    print(model['modelname'])\n",
    "    print('MSE calibration partition:', model['mse_calib'])\n",
    "    print('MSE test partition:', model['mse_test'])\n",
    "    print('MSE test partition, zeros:', model['mse_test_zeros'])\n",
    "    print('MSE test partition, non-zeros:', model['mse_test_nonzeros'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57335b04",
   "metadata": {},
   "source": [
    "# Plotting performance as heatmaps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5dc04ba",
   "metadata": {},
   "source": [
    "## Ablation MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a726329a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of ablation MSEs\n",
    "# df to hold ablation MSEs\n",
    "import seaborn as sns\n",
    "abl_df = pd.DataFrame(0.0, index=np.arange(len(EnsembleList[0]['Ablation_MSE'])), columns=mlist)    \n",
    "for model in EnsembleList[0:-2]: # Assuming the ablated ensemble exists\n",
    "    abl_df[model['modelname']] = model['Ablation_MSE']\n",
    "abl_df = abl_df[1:].T\n",
    "abl_df.columns=stepcols[1:]\n",
    "\n",
    "#palette = 'Spectral'\n",
    "#plt.figure()\n",
    "palette = 'vlag'\n",
    "fig, ax =plt.subplots(1,figsize=(16,11))\n",
    "ax = sns.heatmap(abl_df, center=0, xticklabels=2, linewidths=.5, cmap=palette,square=True)\n",
    "\n",
    "overleafpath = '/Users/havardhegre/Dropbox (ViEWS)/Apps/Overleaf/ViEWS predicting fatalities/Figures/Pred_Eval/'\n",
    "filename = overleafpath + 'Ablation_MSEs.png'\n",
    "plt.savefig(filename, dpi=300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a4c8d9",
   "metadata": {},
   "source": [
    "## Dataframes with country-specific MSEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52370ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a dataframe for each model, plotting with Matshow\n",
    "Countries_to_plot = [41,42,43,47,48,49,50,52,53,54,55,56,60,62,67,69,70,124,155,156,157,162]\n",
    "Namelist = ['Cote dIvoire','Ghana','Liberia','47','48','49','50',\n",
    "            '52','53','54','55','56','60','62','67',\n",
    "            '69','70','124','155','156','157','162']\n",
    "\n",
    "\n",
    "for model in EnsembleList:\n",
    "    listdata = []\n",
    "#    countries = list(EnsembleList[0]['test_df_calibrated'].index.unique(level='country_id'))\n",
    "    for cnt in Countries_to_plot:\n",
    "        row = [cnt] + model['CountryList'][cnt]['mse']\n",
    "        listdata.append(row)\n",
    "    colnames = ['Country'] + stepcols[1:]\n",
    "    model['CC_MSEs'] = pd.DataFrame(listdata,columns=colnames) \n",
    "    \n",
    "    plt.matshow(model['CC_MSEs'][stepcols[1:]])\n",
    "    cb = plt.colorbar()\n",
    "    cb.ax.tick_params(labelsize=14)\n",
    "    plt.xticks(steps)\n",
    "#    plt.yticks(Namelist)\n",
    "#    ax.set_yticks(Countries_to_plot)\n",
    "#    ax.set_yticklabels(NameList)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17157c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(EnsembleList[24]['modelname'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631990a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting prediction vs actuals\n",
    "#ModelSelection=[0,2,3,6,8,9,13,17:24,26:27,31,35, 37:38]\n",
    "\n",
    "\n",
    "plx = 2\n",
    "ply = 2\n",
    "fig, axs = plt.subplots(plx, ply, sharey=True,sharex=True,figsize=(13,12))\n",
    "#to do log scales, use numpy package for the scale value to plot in xticks\n",
    "log_scale_value = np.array([np.log1p(0), np.log1p(1), np.log1p(10), np.log1p(100), np.log1p(1000), np.log1p(10000)])\n",
    "log_scale_naming = ['0','1', '10', '100', '1000','10000']\n",
    "\n",
    "month = [487,488,489,490,491,492]\n",
    "step = 13\n",
    "predvar = 'step_pred_' + str(step)\n",
    "size = 20 \n",
    "\n",
    "fpx = 0\n",
    "fpy = 0\n",
    "model_data = []\n",
    "for model in [EnsembleList[47],EnsembleList[47],EnsembleList[47],EnsembleList[47]]:\n",
    "    print(model['modelname'], fpx, fpy)\n",
    "    print('Prediction mean: ', model['test_df_calibrated'][predvar].mean())\n",
    "    axs[fpx,fpy].scatter( model['test_df_calibrated']['ln_ged_sb_dep'].loc[month], model['test_df_calibrated'][predvar].loc[month],s=size, alpha=0.5)\n",
    "    axs[fpx,fpy].set_ylabel(model['modelname'], fontsize=10)\n",
    "    axs[fpx,fpy].set_xlabel('Actually observed', fontsize=10)\n",
    "    plt.xticks(log_scale_value, log_scale_naming, rotation=30)\n",
    "    plt.yticks(log_scale_value, log_scale_naming, rotation=30)\n",
    "    axs[fpx,fpy].grid(True)\n",
    "#    axs[fpx,fpy].\n",
    "#    axs[fpx,fpy].\n",
    "    if fpx==plx-1:\n",
    "        fpx = 0\n",
    "        fpy = fpy + 1\n",
    "    else:\n",
    "        fpx = fpx + 1\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "#plt.show()\n",
    "\n",
    "overleafpath = '/Users/havardhegre/Dropbox (ViEWS)/Apps/Overleaf/ViEWS predicting fatalities/Figures/PredictionPlots/'\n",
    "filename = overleafpath + 'PredictionVsActuals_cm_s' + str(step) + '.png'\n",
    "plt.savefig(filename, dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7ad7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model['test_df_calibrated']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b24eb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the outliers\n",
    "df = model['test_df_calibrated']\n",
    "cols = ['ln_ged_sb_dep','step_pred_2']\n",
    "print(df[cols].loc[df['ln_ged_sb_dep']>np.log1p(1000)].loc[month].head(20))\n",
    "# 57: Ethiopia\n",
    "# 120: Somalia\n",
    "# 126: Azerbaijan (Nagorno-Karabakh)\n",
    "# 133: Afghanistan\n",
    "# 167: DRC\n",
    "# 220: Mali\n",
    "\n",
    "for i in [np.log1p(10), np.log1p(25), np.log1p(100), np.log1p(1000)]:\n",
    "    print('exp(', str(i),')', np.rint(np.expm1(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a99db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping the predictions\n",
    "engine = sa.create_engine(source_db_path)\n",
    "gdf = gpd.GeoDataFrame.from_postgis(\n",
    "    \"SELECT id as country_id, in_africa, in_me, geom FROM prod.country\", \n",
    "    engine, \n",
    "    geom_col='geom'\n",
    ")\n",
    "\n",
    "\n",
    "month = [480]\n",
    "step = 6\n",
    "predvar = 'step_pred_' + str(step)\n",
    "\n",
    "for model in EnsembleList:\n",
    "    print(model['modelname'])\n",
    "    monthdata = model['test_df_calibrated'].loc[month]\n",
    "    data = monthdata.join(gdf.set_index(\"country_id\"))\n",
    "    gdf = gpd.GeoDataFrame(data, geometry=\"geom\")\n",
    "\n",
    "# From Malika\n",
    "sb_457 = mapper.Mapper(\n",
    "    width=10,\n",
    "    height=10,\n",
    "    frame_on=True,\n",
    "    title=\"State based fatalities, Jan 2018\",\n",
    "    bbox=[-18.5, 64.0, -35.5, 43.0], \n",
    ").add_layer(\n",
    "    gdf=gdf.loc[457],\n",
    "    cmap=\"viridis\",\n",
    "    edgecolor=\"black\",\n",
    "    linewidth=0.5,\n",
    "    column=\"ln_ged_sb\"\n",
    ").add_colorbar(\n",
    "cmap = 'viridis', \n",
    "vmin=gdf.loc[457]['ln_ged_sb'].min(), \n",
    "vmax=gdf.loc[457]['ln_ged_sb'].max()\n",
    ")\n",
    "\n",
    "sb_457.cbar.set_ticks(scale)\n",
    "sb_457.cbar.set_ticklabels(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd35ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model['sc_df'].head()\n",
    "for month_id, cnt_df in model['sc_df'].groupby(level=1):\n",
    "    country_id = cnt_df.index.values[0][1]\n",
    "    print(country_id)\n",
    "    model['sc_df_smooth']=model['sc_df'].rolling(3,center=True).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edb4fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Make a smoothed version\n",
    "    model['sc_df_smooth']=model['sc_df'].rolling(3,center=True).mean().groupby(level=1)\n",
    "#    model['sc_df_smooth'] = model['sc_df'].copy()\n",
    "#    for col in model['sc_df_smooth'].columns[1:]:\n",
    "#        model['sc_df_smooth'].col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a275a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c790a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "CountryData = pd.DataFrame([\n",
    "    [1,'Guyana',211982.004988,'Georgetown',-58.2,6.8,0,110,1966,5,26,2016,6,30,110,1966,5,26,2016,6,30,'Guyana',328,'GY','GUY'\n",
    "],[2,'Suriname',145952.274029,'Paramaribo',-55.2,5.833333,1,115,1975,11,25,2016,6,30,115,1975,11,25,2016,6,30,'Suriname',740,'SR','SUR'\n",
    "],[3,'Trinidad and Tobago',5041.72895193,'Port-of-Spain',-61.5,10.65,2,52,1962,8,31,2016,6,30,52,1962,8,31,2016,6,30,'Trinidad and Tobago',780,'TT','TTO'\n",
    "],[4,'Venezuela',916782.217193,'Caracas',-66.9,10.5,3,101,1946,1,1,2016,6,30,101,1946,1,1,2016,6,30,'Venezuela',862,'VE','VEN'\n",
    "],[5,'Samoa',2955.21236649,'Apia',-172,-13.8,4,990,1976,12,15,2016,6,30,990,1962,1,1,2016,6,30,'Samoa',882,'WS','WSM'\n",
    "],[6,'Tonga',464.74733142,'Nukualofa',-175,-21.1,5,955,1999,9,14,2016,6,30,972,1970,6,4,2016,6,30,'Tonga',776,'TO','TON'\n",
    "],[7,'Argentina',2787442.09772,'Buenos Aires',-58.7,-34.6,6,160,1946,1,1,2016,6,30,160,1946,1,1,2016,6,30,'Argentina',32,'AR','ARG'\n",
    "],[8,'Bolivia',1092697.43562,'La Paz',-68.2,-16.5,7,145,1946,1,1,2016,6,30,145,1946,1,1,2016,6,30,'Bolivia',68,'BO','BOL'\n",
    "],[9,'Brazil',8523619.57145,'Brasilia',-47.9,-15.8,8,140,1960,4,21,2016,6,30,140,1960,4,21,2016,6,30,'Brazil',76,'BR','BRA'\n",
    "],[10,'Chile',745808.493557,'Santiago',-70.7,-33.5,9,155,1946,1,1,2016,6,30,155,1946,1,1,2016,6,30,'Chile',152,'CL','CHL'\n",
    "],[11,'Ecuador',257026.690766,'Quito',-78.5,-0.2166667,10,130,1946,1,1,2016,6,30,130,1946,1,1,2016,6,30,'Ecuador',218,'EC','ECU'\n",
    "],[12,'Paraguay',400653.400582,'Asuncion',-57.7,-25.3,11,150,1946,1,1,2016,6,30,150,1946,1,1,2016,6,30,'Paraguay',600,'Py','PRY'\n",
    "],[13,'Peru',1299028.07763,'Lima',-77,-12.1,12,135,1946,1,1,2016,6,30,135,1946,1,1,2016,6,30,'Peru',604,'Pe','PER'\n",
    "],[14,'Uruguay',178357.24144,'Montevideo',-56.2,-34.9,13,165,1946,1,1,2016,6,30,165,1946,1,1,2016,6,30,'Uruguay',858,'Uy','URY'\n",
    "],[15,'Canada',9526178.12487,'Ottawa',-75.7,45.42083,14,20,1946,1,1,1948,6,30,20,1946,1,1,1948,6,30,'',0,'',''\n",
    "],[27,'Belize',22208.8585502,'Belmopan',-88.8,17.25,26,80,1981,9,21,2016,6,30,80,1981,9,21,2016,6,30,'Belize',84,'BZ','BLZ'\n",
    "],[28,'Colombia',1142725.71315,'Bogota',-74.1,4.6,27,100,1946,1,1,2016,6,30,100,1946,1,1,2016,6,30,'colombia',170,'co','cOL'\n",
    "],[16,'Guatemala',109653.931118,'Guatemala City',-90.5,14.62111,15,90,1946,1,1,2016,6,30,90,1946,1,1,2016,6,30,'Guatemala',320,'GT','GTM'\n",
    "],[17,'Mexico',1965660.69014,'Mexico City',-99.1,19.43417,16,70,1946,1,1,2016,6,30,70,1946,1,1,2016,6,30,'Mexico',484,'MX','MEX'\n",
    "],[18,'Barbados',448.945911949,'Bridgetown',-59.6,13.1,17,53,1966,11,30,2016,6,30,53,1966,11,30,2016,6,30,'Barbados',52,'Bb','BRB'\n",
    "],[19,'Dominica',774.491216087,'Roseau',-61.4,15.3,18,54,1978,11,3,2016,6,30,54,1978,11,3,2016,6,30,'Dominica',212,'DM','DMA'\n",
    "],[20,'Grenada',348.318551633,'Saint Georges',-61.8,12.05,19,55,1974,2,7,2016,6,30,55,1974,2,7,2016,6,30,'Grenada',308,'GD','GRD'\n",
    "],[21,'St. Lucia',639.059498815,'castries',-61,14,20,56,1979,2,22,2016,6,30,56,1979,2,22,2016,6,30,'Saint Lucia',662,'LC','LCA'\n",
    "],[22,'St. Vincent and the Grenadines',344.300049243,'Kingstown',-61.2,13.16667,21,57,1979,10,27,2016,6,30,57,1979,10,27,2016,6,30,'Saint Vincent and the Grenadines',670,'VC','VCT'\n",
    "],[36,'St. Kitts and Nevis',196.699148006,'Basseterre',-62.7,17.3,35,60,1983,9,19,2016,6,30,60,1983,9,19,2016,6,30,'Saint Kitts and Nevis',659,'Kn','KNA'\n",
    "],[37,'Iceland',102510.041259,'Reykjavik',-21.9,64.15,36,395,1946,1,1,2016,6,30,395,1946,1,1,2016,6,30,'Iceland',352,'Is','ISL'\n",
    "],[38,'Ireland',69507.08947,'Dublin',-6.248889,53.33306,37,205,1946,1,1,2016,6,30,205,1946,1,1,2016,6,30,'Ireland',372,'Ie','IRL'\n",
    "],[39,'United Kingdom',243845.075909,'London',-0.1166667,51.5,38,200,1946,1,1,2016,6,30,200,1946,1,1,2016,6,30,'United Kingdom',826,'Gb','GBR'\n",
    "],[23,'Dominican Republic',48625.6990635,'Santo Domingo',-69.9,18.46667,22,42,1946,1,1,2016,6,30,42,1946,1,1,2016,6,30,'Dominican Republic',214,'Do','DOM'\n",
    "],[140,'Brunei',5785.18039944,'Bandar Seri Begawan',115,4.883333,141,835,1984,1,1,2016,6,30,835,1984,1,1,2016,6,30,'Brunei Darussalam',96,'Bn','BRN'\n",
    "],[141,'China',9424689.74644,'Beijing',116,39.92889,142,710,1946,1,1,1949,12,7,710,1946,1,1,1949,12,7,'',0,'',''\n",
    "],[142,'Japan',371862.299662,'Tokyo',140,35.685,143,740,1952,4,28,2016,6,30,740,1946,1,1,2016,6,30,'Japan',392,'JP','JPN'\n",
    "],[24,'HaitI',27315.4591425,'Port-au-Prince',-72.3,18.5392,23,41,1946,1,1,2016,6,30,41,1946,1,1,2016,6,30,'HaitI',332,'Ht','HTI'\n",
    "],[25,'Jamaica',11097.4197969,'Kingston',-76.8,18,24,51,1962,8,6,2016,6,30,51,1962,8,6,2016,6,30,'Jamaica',388,'JM','JAM'\n",
    "],[26,'Bahamas',12192.0404721,'Nassau',-77.3,25.08333,25,31,1973,7,10,2016,6,30,31,1973,7,10,2016,6,30,'Bahamas',44,'Bs','BHS'\n",
    "],[29,'Costa Rica',51402.6253926,'San Jose',-84.1,9.933333,28,94,1946,1,1,2016,6,30,94,1946,1,1,2016,6,30,'Costa Rica',188,'cr','cRI'\n",
    "],[30,'Cuba',109740.925034,'Havana',-82.4,23.13194,29,40,1946,1,1,2016,6,30,40,1946,1,1,2016,6,30,'cuba',192,'cu','cUB'\n",
    "],[31,'El Salvador',20692.8390883,'San Salvador',-89.2,13.70861,30,92,1946,1,1,2016,6,30,92,1946,1,1,2016,6,30,'El Salvador',222,'SV','SLV'\n",
    "],[153,'Micronesia',519.642298193,'Palikir',158,6.916667,154,987,1991,9,17,2016,6,30,987,1986,11,3,2016,6,30,\"Micronesia', Federated States of\",583,'FM','FSM'\n",
    "],[32,'Honduras',112871.255492,'Tegucigalpa',-87.2,14.1,31,91,1946,1,1,2016,6,30,91,1946,1,1,2016,6,30,'Honduras',340,'Hn','HND'\n",
    "],[155,'BurundI',27367.749959,'Bujumbura',29.36,-3.376111,156,516,1962,7,1,2016,6,30,516,1962,7,1,2016,6,30,'BurundI',108,'BI','BDI'\n",
    "],[33,'Nicaragua',128883.087518,'Managua',-86.3,12.15083,32,93,1946,1,1,2016,6,30,93,1946,1,1,2016,6,30,'Nicaragua',558,'NI','NIC'\n",
    "],[40,'Cape Verde',4054.30324666,'Praia',-23.5,14.91667,39,402,1975,7,5,2016,6,30,402,1975,7,5,2016,6,30,'Cape Verde',132,'cV','cPV'\n",
    "],[41,'Cote dIvoire',323412.228545,'Yamoussoukro',-5.283333,6.816667,40,437,1960,8,7,2016,6,30,437,1960,8,7,2016,6,30,'Cote dIvoire',384,'cI','cIV'\n",
    "],[34,'Panama',74612.5675615,'Panama City',-79.5,8.966667,33,95,1946,1,1,2016,6,30,95,1946,1,1,2016,6,30,'Panama',591,'Pa','PAN'\n",
    "],[35,'Antigua and Barbuda',539.862118904,'St. Johns',-61.9,17.11667,34,58,1981,11,1,2016,6,30,58,1981,11,1,2016,6,30,'Antigua and Barbuda',28,'Ag','ATG'\n",
    "],[42,'Ghana',240585.519658,'Accra',-0.2166667,5.55,41,452,1957,3,6,2016,6,30,452,1957,3,6,2016,6,30,'Ghana',288,'GH','GHA'\n",
    "],[156,'Rwanda',25305.6244393,'KigalI',30.06056,-1.953611,158,517,1962,7,1,2016,6,30,517,1962,1,7,2016,6,30,'Rwanda',646,'Rw','RWA'\n",
    "],[157,'Zambia',756428.205434,'Lusaka',28.28333,-15.4,160,551,1964,10,24,2016,6,30,551,1964,10,24,2016,6,30,'Zambia',894,'ZM','ZMB'\n",
    "],[43,'Liberia',96634.5174739,'Monrovia',-10.8,6.310555,42,450,1946,1,1,2016,6,30,450,1946,1,1,2016,6,30,'Liberia',430,'Lr','LBR'\n",
    "],[53,'Sierra Leone',72952.3426516,'Freetown',-13.2,8.49,52,451,1961,4,27,2016,6,30,451,1961,4,27,2016,6,30,'Sierra Leone',694,'Sl','SLE'\n",
    "],[44,'Morocco',401444.80109,'Rabat',-6.83,34.02,43,600,1956,3,2,1958,3,31,600,1956,3,2,1958,3,31,'',0,'',''\n",
    "],[45,'Portugal',92026.5855825,'Lisbon',-9.133333,38.71667,44,235,1946,1,1,2016,6,30,235,1946,1,1,2016,6,30,'Portugal',620,'Pt','PRT'\n",
    "],[46,'Spain',506733.603989,'Madrid',-3.683333,40.4,45,230,1946,1,1,2016,6,30,230,1946,1,1,2016,6,30,'Spain',724,'Es','ESP'\n",
    "],[47,'Burkina Faso',274009.474052,'Ouagadougou',-1.524722,12.37028,46,439,1960,8,5,2016,6,30,439,1960,8,5,2016,6,30,'Burkina Faso',854,'BF','BFA'\n",
    "],[76,'Equatorial Guinea',27102.4666914,'Malabo',8.783333,3.75,76,411,1968,10,12,2016,6,30,411,1968,10,12,2016,6,30,'Equatorial Guinea',226,'GQ','GNQ'\n",
    "],[48,'Guinea',246594.677066,'conakry',-13.7,9.509167,47,438,1958,10,2,2016,6,30,438,1958,10,2,2016,6,30,'Guinea',324,'Gn','GIN'\n",
    "],[49,'Guinea-Bissau',33462.1154205,'Bissau',-15.6,11.85,48,404,1974,9,10,2016,6,30,404,1974,9,10,2016,6,30,'Guinea-Bissau',624,'Gw','GNB'\n",
    "],[50,'MalI',1259194.50123,'Bamako',-8,12.65,49,432,1960,8,20,2016,6,30,432,1960,9,22,2016,6,30,'MalI',466,'Ml','MLI'\n",
    "],[77,'KiribatI',425.447053045,'Tarawa',173,1.316667,77,946,1999,9,14,2016,6,30,970,1979,7,12,2016,6,30,'KiribatI',296,'KI','KIR'\n",
    "],[51,'Mauritania',1043783.42747,'Nouakchott',-16,18.11944,50,435,1960,11,28,1976,3,31,435,1960,11,28,1976,3,31,'Mauritania',478,'Mr','MRT'\n",
    "],[52,'Senegal',197172.573237,'Dakar',-17.5,14.70889,51,433,1960,8,20,2016,6,30,433,1960,4,4,2016,6,30,'Senegal',686,'Sn','SEN'\n",
    "],[54,'The Gambia',10787.1404565,'Banjul',-16.6,13.45306,53,420,1965,2,18,2016,6,30,420,1965,2,18,2016,6,30,'Gambia',270,'GM','GMB'\n",
    "],[55,'DjiboutI',21567.9097045,'DjiboutI',43.14806,11.595,54,522,1977,6,27,2016,6,30,522,1977,6,27,2016,6,30,'DjiboutI',262,'DJ','DJI'\n",
    "],[82,'Albania',28680.7144592,'Tirane',19.81889,41.3275,82,339,1946,1,1,2016,6,30,339,1946,1,1,2016,6,30,'Albania',8,'Al','ALB'\n",
    "],[56,'Eritrea',121605.399367,'Asmara',38.93333,15.33333,55,531,1993,5,23,2016,6,30,531,1993,5,24,2016,6,30,'Eritrea',232,'Er','ERI'\n",
    "],[57,'Ethiopia',1134773.29692,'Addis Ababa',38.7,9.033333,56,530,1993,5,23,2016,6,30,530,1993,5,24,2016,6,30,'Ethiopia',231,'Et','ETH'\n",
    "],[58,'Mongolia',1562323.42683,'Ulan Bator',107,47.91667,57,712,1946,1,1,2016,6,30,712,1946,1,1,2016,6,30,'Mongolia',496,'Mn','MNG'\n",
    "],[59,'Sudan',2501656.1935,'Khartoum',32.53417,15.58806,58,625,1956,1,1,2011,7,8,625,1956,1,1,2011,7,8,'Sudan',736,'Sd','SDN'\n",
    "],[60,'Iraq',437462.661411,'Baghdad',44.39389,33.33861,60,645,1946,1,1,2016,6,30,645,1946,1,1,2016,6,30,'Iraq',368,'Iq','IRQ'\n",
    "],[118,'Maldives',33.8538943201,'Male',73.5,4.166667,119,781,1965,7,26,2016,6,30,781,1965,5,26,2016,6,30,'Maldives',462,'MV','MDV'\n",
    "],[61,'Israel',20783.9644258,'Jerusalem',35.22361,31.77917,61,666,1948,5,14,1967,5,31,666,1948,5,14,1967,5,31,'',0,'',''\n",
    "],[62,'Jordan',89491.0684856,'Amman',35.93333,31.95,62,663,1946,3,22,2016,6,30,663,1946,5,25,2016,6,30,'Jordan',400,'Jo','JOR'\n",
    "],[63,'Kazakhstan',2721207.07916,'Astana',71.42778,51.18111,63,705,1991,12,26,2016,6,30,705,1991,12,16,2016,6,30,'Kazakhstan',398,'KZ','KAZ'\n",
    "],[64,'Norway',319586.663464,'Oslo',10.75,59.91667,64,385,1946,1,1,2016,6,30,385,1946,1,1,2016,6,30,'Norway',578,'No','NOR'\n",
    "],[65,'Russia',16827198.0013,'Moscow',37.61555,55.75222,65,365,1991,12,26,2016,6,30,365,1991,12,21,2016,6,30,'Russian Federation',643,'Ru','RUS'\n",
    "],[66,'Sweden',444337.592324,'Stockholm',18.05,59.33333,66,380,1946,1,1,2016,6,30,380,1946,1,1,2016,6,30,'Sweden',752,'Se','SWE'\n",
    "],[67,'Algeria',2326148.44131,'Algiers',3.050556,36.76305,67,615,1962,7,5,2016,6,30,615,1962,7,5,2016,6,30,'Algeria',12,'DZ','DZA'\n",
    "],[68,'Andorra',507.451028558,'Andorra la Vella',1.516667,42.5,68,232,1993,7,28,2016,6,30,232,1946,1,1,2016,6,30,'Andorra',20,'Ad','AND'\n",
    "],[69,'cameroon',467816.059699,'Yaounde',11.51667,3.866667,69,471,1961,10,1,2016,6,30,471,1961,10,1,2016,6,30,'cameroon',120,'cm','cMR'\n",
    "],[70,'Central African Republic',622696.58145,'BanguI',18.58333,4.366667,70,482,1960,8,13,2016,6,30,482,1960,8,13,2016,6,30,'Central African Republic',140,'cF','cAF'\n",
    "],[71,'Libya',1623932.59444,'TripolI',13.18,32.8925,71,620,1951,12,24,1972,12,31,620,1951,12,24,1972,12,31,'',0,'',''\n",
    "],[72,'Monaco',9.27583711959,'Monaco',7.416667,43.73333,72,221,1993,5,28,2016,6,30,221,1946,1,1,2016,6,30,'Monaco',492,'MC','MCO'\n",
    "],[73,'Tunisia',155771.465695,'Tunis',10.17972,36.80278,73,616,1956,3,20,2016,6,30,616,1956,1,1,2016,6,30,'Tunisia',788,'Tn','TUN'\n",
    "],[74,'Benin',116922.187423,'Porto-Novo',2.616667,6.483333,74,434,1960,8,1,2016,6,30,434,1960,8,1,2016,6,30,'Benin',204,'BJ','BEN'\n",
    "],[81,'Togo',57485.4484335,'Lome',1.6,7.1,81,461,1960,4,27,2016,6,30,461,1960,4,27,2016,6,30,'Togo',768,'Tg','TGO'\n",
    "],[75,'chad',1279118.91278,'NDjamena',15.21667,11.71667,75,483,1960,8,11,1972,12,31,483,1960,8,11,1972,12,31,'',0,'',\n",
    "],[78,'Niger',1188522.5248,'Niamey',2.116667,13.51667,78,436,1960,10,3,2016,6,30,436,1960,8,3,2016,6,30,'Niger',562,'Ne','NER'\n",
    "],[79,'Nigeria',914291.337152,'Abuja',7.533333,9.083333,79,475,1961,6,1,2016,6,30,475,1961,6,1,2016,6,30,'Nigeria',566,'Ng','NGA'\n",
    "],[80,'Sao Tome and Principe',1149.77537193,'Sao Tome',6.681389,0.336111,80,403,1975,7,12,2016,6,30,403,1975,7,12,2016,6,30,'Sao Tome and Principe',678,'St','STP'\n",
    "],[83,'Bosnia and Herzegovina',51537.9748266,'Sarajevo',18.38333,43.85,83,346,1992,4,7,2016,6,30,346,1992,4,27,2016,6,30,'Bosnia and Herzegovina',70,'Ba','BIH'\n",
    "],[84,'croatia',55889.1151742,'Zagreb',16,45.8,84,344,1992,1,15,2016,6,30,344,1992,4,27,2016,6,30,'croatia',191,'Hr','HRV'\n",
    "],[85,'Italy',300242.620682,'Rome',12.48333,41.9,85,325,1946,1,1,2016,6,30,325,1946,1,1,2016,6,30,'Italy',380,'It','ITA'\n",
    "],[86,'Macedonia',25483.1932119,'Skopje',21.43333,42,86,343,1993,4,8,2016,6,30,343,1991,11,20,2016,6,30,\"Macedonia', the former Yugoslav Republic of\",807,'Mk','MKD'\n",
    "],[87,'Malta',295.019435295,'Valletta',14.51472,35.89972,87,338,1964,9,21,2016,6,30,338,1964,9,21,2016,6,30,'Malta',470,'Mt','MLT'\n",
    "],[88,'San Marino',59.8811750005,'San Marino',12.45,43.93333,88,331,1992,3,2,2016,6,30,331,1946,1,1,2016,6,30,'San Marino',674,'Sm','SMR'\n",
    "],[160,'Lesotho',30621.4287343,'Maseru',27.48333,-29.3,163,570,1966,10,4,2016,6,30,570,1966,10,4,2016,6,30,'Lesotho',426,'Ls','LSO'\n",
    "],[89,'Bulgaria',111083.068328,'Sofia',23.31667,42.68333,90,355,1946,1,1,2016,6,30,355,1946,1,1,2016,6,30,'Bulgaria',100,'Bg','BGR'\n",
    "],[90,'cyprus',9157.70860517,'Nicosia',33.36666,35.16667,91,352,1960,8,16,2016,6,30,352,1960,8,16,2016,6,30,'cyprus',196,'cy','cYP'\n",
    "],[91,'Egypt',1002473.97844,'cairo',31.25,30.05,92,651,1946,1,1,1958,1,31,651,1946,1,1,1967,5,31,'',0,'',''\n",
    "],[170,'Namibia',828714.948494,'Windhoek',17.08361,-22.6,173,565,1990,3,21,2016,6,30,565,1990,3,21,2016,6,30,'Namibia',516,'Na','NAM'\n",
    "],[171,'New Zealand',268943.318871,'Wellington',175,-41.3,174,920,1946,1,1,2016,6,30,920,1946,1,1,2016,6,30,'New Zealand',554,'Nz','NZL'\n",
    "],[92,'Georgia',70004.6046417,'TbilisI',44.79083,41.725,93,372,1991,12,26,2016,6,30,372,1991,9,6,2016,6,30,'Georgia',268,'Ge','GEO'\n",
    "],[93,'Greece',130247.978372,'Athens',23.73333,37.98333,94,350,1946,1,1,2016,6,30,350,1946,1,1,2016,6,30,'Greece',300,'Gr','GRC'\n",
    "],[94,'Lebanon',10240.2335568,'Beirut',35.50972,33.87194,95,660,1946,3,10,2016,6,30,660,1946,1,1,2016,6,30,'Lebanon',422,'Lb','LBN'\n",
    "],[172,'Madagascar',596098.591457,'Antananarivo',47.51667,-18.9,175,580,1960,6,20,2016,6,30,580,1960,6,26,2016,6,30,'Madagascar',450,'Mg','MDG'\n",
    "],[173,'Mauritius',2155.89713857,'Port Louis',57.49889,-20.2,176,590,1968,3,12,2016,6,30,590,1968,3,12,2016,6,30,'Mauritius',480,'Mu','MUS'\n",
    "],[95,'Syria',188436.319945,'Damascus',36.3,33.5,96,652,1946,4,17,1958,2,1,652,1946,1,1,1967,5,31,'',0,'',''\n",
    "]\n",
    "    \n",
    "])\n",
    "CountryData.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9f6606",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1330ece4",
   "metadata": {},
   "outputs": [],
   "source": [
    ",[96,'Turkey',781079.220751,'Ankara',32.86444,39.92722,97,640,1946,1,1,2016,6,30,640,1946,1,1,2016,6,30,'Turkey',792,'Tr','TUR\n",
    "],[125,'Armenia',29702.4992641,'Yerevan',44.51361,40.18111,126,371,1991,12,26,2016,6,30,371,1991,12,21,2016,6,30,'Armenia',51,'Am','ARM\n",
    "],[97,'Austria',83896.0779033,'Vienna',16.36667,48.2,98,305,1955,7,27,2016,6,30,305,1946,1,1,2016,6,30,'Austria',40,'At','AUT\n",
    "],[128,'Iran',1626149.96028,'Tehran',51.42445,35.67194,129,630,1946,1,1,2016,6,30,630,1946,1,1,2016,6,30,\"Iran', Islamic Republic of\",364,'Ir','IRN\n",
    "],[174,'Seychelles',380.172711433,'Victoria',55.45,-4.616667,177,591,1976,6,29,2016,6,30,591,1976,6,29,2016,6,30,'Seychelles',690,'SC','SYC\n",
    "],[175,'Indonesia',1478049.02833,'Jakarta',107,-6.174444,178,850,1949,12,27,1963,4,30,850,1946,1,1,1963,4,30,'',0,'',''\n",
    "],[98,'Czech Republic',78668.4117434,'Prague',14.46667,50.08333,99,316,1993,1,1,2016,6,30,316,1993,1,1,2016,6,30,'Czech Republic',203,'cz','cZE\n",
    "],[99,'Denmark',42604.6743577,'copenhagen',12.58333,55.66667,100,390,1946,1,1,2016,6,30,390,1946,1,1,2016,6,30,'Denmark',208,'Dk','DNK\n",
    "],[129,'Kuwait',16798.1110648,'Kuwait',47.97833,29.36972,130,690,1961,6,19,2016,6,30,690,1961,6,19,2016,6,30,'Kuwait',414,'Kw','KWT\n",
    "],[100,'Hungary',92949.1325297,'Budapest',19.08333,47.5,101,310,1946,1,1,2016,6,30,310,1946,1,1,2016,6,30,'Hungary',348,'Hu','HUN\n",
    "],[101,'Poland',311161.237356,'Warsaw',21,52.25,102,290,1946,1,1,2016,6,30,290,1946,1,1,2016,6,30,'Poland',616,'Pl','POL\n",
    "],[130,'Qatar',11143.6634192,'Doha',51.53333,25.28667,131,694,1971,9,3,2016,6,30,694,1971,9,3,2016,6,30,'Qatar',634,'Qa','QAT\n",
    "],[131,'Saudi Arabia',1933234.38469,'Riyadh',46.77278,24.64083,132,670,2000,6,12,2016,6,30,670,2000,6,12,2016,6,30,'Saudi Arabia',682,'Sa','SAU\n",
    "],[102,'Slovakia',48884.7200983,'Bratislava',17.11667,48.15,103,317,1993,1,1,2016,6,30,317,1993,1,1,2016,6,30,'Slovakia',703,'Sk','SVK\n",
    "],[103,'Slovenia',20415.6125511,'Ljubljana',14.51444,46.05528,104,349,1992,1,15,2016,6,30,349,1992,4,27,2016,6,30,'Slovenia',705,'SI','SVN\n",
    "],[104,'Belgium',30611.6991041,'Brussels',4.333333,50.83333,105,211,1946,1,1,2016,6,30,211,1946,1,1,2016,6,30,'Belgium',56,'Be','BEL\n",
    "],[105,'France',547871.255355,'Paris',2.333333,48.86666,106,220,1946,1,1,2016,6,30,220,1946,1,1,2016,6,30,'France',250,'Fr','FRA\n",
    "],[106,'Liechtenstein',176.309935955,'Vaduz',9.516666,47.13334,107,223,1990,9,18,2016,6,30,223,1946,1,1,2016,6,30,'Liechtenstein',438,'LI','LIE\n",
    "],[107,'Luxembourg',2577.8092797,'Luxembourg',6.13,49.61167,108,212,1946,1,1,2016,6,30,212,1946,1,1,2016,6,30,'Luxembourg',442,'Lu','LUX\n",
    "],[108,'Netherlands',35485.5749945,'Amsterdam',4.916667,52.35,109,210,1946,1,1,2016,6,30,210,1946,1,1,2016,6,30,'Netherlands',528,'Nl','NLD\n",
    "],[109,'Switzerland',41475.5712863,'Bern',7.466667,46.91667,110,225,1946,1,1,2016,6,30,225,1946,1,1,2016,6,30,'Switzerland',756,'ch','cHE\n",
    "],[176,'Timor Leste',15138.8240257,'DilI',126,-8.558611,179,860,2002,9,27,2016,6,30,860,2002,5,20,2016,6,30,'Timor-Leste',626,'Tl','TLS\n",
    "],[177,'Australia',7718924.6969,'canberra',149,-35.3,180,900,1946,1,1,2016,6,30,900,1946,1,1,2016,6,30,'Australia',36,'Au','AUS\n",
    "],[110,'Belarus',207315.892132,'Minsk',27.56667,53.9,111,370,1991,12,26,2016,6,30,370,1991,8,25,2016,6,30,'Belarus',112,'By','BLR\n",
    "],[111,'Estonia',45791.5800937,'Tallinn',24.72806,59.43389,112,366,1991,9,6,2016,6,30,366,1991,9,6,2016,6,30,'Estonia',233,'Ee','EST\n",
    "],[112,'Finland',333891.198261,'HelsinkI',24.93417,60.17556,113,375,1946,1,1,2016,6,30,375,1946,1,1,2016,6,30,'Finland',246,'FI','FIN\n",
    "],[178,'Nauru',27.3444606991,'Yaren',167,-0.543425,181,970,1999,9,14,2016,6,30,971,1968,12,31,2016,6,30,'Nauru',520,'Nr','NRU\n",
    "],[179,'Papua New Guinea',465827.907822,'Port Moresby',147,-9.464723,182,910,1975,9,16,2016,6,30,910,1975,9,16,2016,6,30,'Papua New Guinea',598,'Pg','PNG\n",
    "],[180,'Solomon Is.',27157.4690545,'Honiara',160,-9.433333,183,940,1978,7,7,2016,6,30,940,1978,7,7,2016,6,30,'Solomon Islands',90,'Sb','SLB\n",
    "],[113,'Latvia',64469.1914752,'Riga',24.13333,56.96667,114,367,1991,9,6,2016,6,30,367,1991,9,6,2016,6,30,'Latvia',428,'Lv','LVA\n",
    "],[181,'Tuvalu',29.2116662587,'FunafutI',179,-8.516666,184,947,2000,9,5,2016,6,30,973,1978,10,1,2016,6,30,'Tuvalu',798,'Tv','TUV\n",
    "],[114,'Lithuania',64857.5568131,'Vilnius',25.31667,54.68333,115,368,1991,9,6,2016,6,30,368,1991,9,6,2016,6,30,'Lithuania',440,'Lt','LTU\n",
    "],[115,'Moldova',33670.8700671,'chisinau',28.8575,47.00555,116,359,1991,12,26,2016,6,30,359,1991,8,27,2016,6,30,'Moldova',498,'Md','MDA\n",
    "],[116,'Romania',237334.065117,'Bukarest',26.1,44.43333,117,360,1946,1,1,2016,6,30,360,1946,1,1,2016,6,30,'Romania',642,'Ro','ROU\n",
    "],[117,'Ukraine',596958.708921,'Kiev',30.51667,50.43333,118,369,1991,12,26,2016,6,30,369,1991,12,1,2016,6,30,'Ukraine',804,'Ua','UKR\n",
    "],[119,'Oman',310330.681626,'Masqat',58.59333,23.61333,120,698,1971,10,7,2016,6,30,698,1946,1,1,2016,6,30,'Oman',512,'Om','OMN\n",
    "],[120,'Somalia',640483.58889,'Mogadishu',45.36666,2.066667,121,520,1960,7,1,2016,6,30,520,1960,7,1,2016,6,30,'Somalia',706,'So','SOM\n",
    "],[121,'Sri Lanka',66468.3589613,'colombo',79.84778,6.931944,122,780,1948,2,4,2016,6,30,780,1948,2,4,2016,6,30,'Sri Lanka',144,'Lk','LKA\n",
    "],[122,'Turkmenistan',472337.174738,'Ashgabat',58.38334,37.95,123,701,1991,12,26,2016,6,30,701,1991,10,27,2016,6,30,'Turkmenistan',795,'Tm','TKM\n",
    "],[123,'Uzbekistan',446971.059951,'Tashkent',69.25,41.31667,124,704,1991,12,26,2016,6,30,704,1991,8,31,2016,6,30,'Uzbekistan',860,'Uz','UZB\n",
    "],[190,'Zanzibar',2632.47294664,'Zanzibar City',39.18333,-6.166667,194,511,1963,12,19,1964,4,26,511,1963,12,19,1964,4,26,'',0,'',''\n",
    "],[191,'Ethiopia',1256378.69628,'Addis Ababa',38.7,9.033333,195,530,1946,1,1,1993,5,22,530,1946,1,1,1993,5,23,'Ethiopia',230,'Et','ETH\n",
    "],[192,'South Africa',2053300.74589,'Pretoria',28.22944,-25.7,196,560,1946,1,1,1990,2,28,560,1946,1,1,1990,3,20,'South Africa',710,'Za','ZAF\n",
    "],[193,'Egypt',1002473.97844,'cairo',31.25,30.05,197,651,1961,9,1,1967,5,31,-1,-1,-1,-1,-1,-1,-1,'',0,'',''\n",
    "],[124,'Yemen',456146.698895,'Sanaa',44.18333,15.28333,125,679,2000,6,12,2016,6,30,678,2000,6,12,2016,6,30,'Yemen',887,'Ye','YEM\n",
    "],[126,'Azerbaijan',86045.8745881,'Baku',49.88222,40.39528,127,373,1991,12,26,2016,6,30,373,1991,12,21,2016,6,30,'Azerbaijan',31,'Az','AZE\n",
    "],[127,'Bahrain',642.915228571,'Manama',50.58306,26.23611,128,692,1971,8,15,2016,6,30,692,1971,8,15,2016,6,30,'Bahrain',48,'Bh','BHR\n",
    "],[132,'United Arab Emirates',70685.1741153,'Abu Dhabi',54.36666,24.46667,133,696,1971,12,2,2016,6,30,696,1971,12,2,2016,6,30,'United Arab Emirates',784,'Ae','ARE\n",
    "],[133,'Afghanistan',643557.140913,'Kabul',69.18333,34.51667,134,700,1946,1,1,2016,6,30,700,1946,1,1,2016,6,30,'Afghanistan',4,'AF,'AFG\n",
    "],[134,'Kyrgyzstan',199732.539747,'Bishkek',74.60028,42.87305,135,703,1991,12,26,2016,6,30,703,1991,8,31,2016,6,30,'Kyrgyzstan',417,'Kg','KGZ\n",
    "],[139,'Bhutan',39991.3561241,'Thimphu',89.6,27.48333,140,760,1971,9,21,2016,6,30,760,1949,1,1,2016,6,30,'Bhutan',64,'Bt','BTN\n",
    "],[135,'Nepal',147710.194614,'Kathmandu',85.31667,27.71667,136,790,1946,1,1,2016,6,30,790,1946,1,1,2016,6,30,'Nepal',524,'NP,'NPL\n",
    "],[136,'Pakistan',879492.89693,'Islamabad',73.16666,33.7,137,770,1971,12,16,2016,6,30,770,1971,12,16,2016,6,30,'Pakistan',586,'Pk','PAK\n",
    "],[137,'Tajikistan',142643.839584,'Dushanbe',68.77389,38.56,138,702,1991,12,26,2016,6,30,702,1991,9,9,2016,6,30,'Tajikistan',762,'TJ,'TJK\n",
    "],[138,'Bangladesh',138505.328342,'Dhaka',90.40861,23.72305,139,771,1971,12,16,2016,6,30,771,1971,12,16,2016,6,30,'Bangladesh',50,'Bd','BGD\n",
    "],[143,'North Korea',122351.726844,'Pyongyang',126,39.01944,144,731,1948,9,9,2016,6,30,731,1948,9,9,2016,6,30,\"Korea', Democratic People's Republic of\",408,'KP,'PRK\n",
    "],[144,'Palau',382.603448129,'Koror',134,7.340556,145,986,1994,12,15,2016,6,30,986,1994,10,1,2016,6,30,'Palau',585,'Pw','PLW\n",
    "],[145,'Philippines',294201.601138,'Manila',121,14.60417,146,840,1946,7,4,2016,6,30,840,1946,7,4,2016,6,30,'Philippines',608,'Ph','PHL\n",
    "],[146,'South Korea',97429.7163986,'Seoul',127,37.56639,147,732,1949,6,29,2016,6,30,732,1948,8,15,2016,6,30,\"Korea', Republic of\",410,'Kr','KOR\n",
    "],[147,'Cambodia',182847.295534,'Phnom Penh',105,11.55,148,811,1953,11,9,2016,6,30,811,1953,11,9,2016,6,30,'cambodia',116,'Kh','KHM\n",
    "],[148,'Laos',231120.578521,'Vientiane',103,17.96667,149,812,1953,10,23,2016,6,30,812,1954,5,1,2016,6,30,'Lao Peoples Democratic Republic',418,'La','LAO\n",
    "],[149,'Myanmar',670372.095395,'Yangon',96.16666,16.78333,150,775,1948,1,4,2016,6,30,775,1948,1,4,2016,6,30,'Myanmar',104,'Mm','MMR\n",
    "],[150,'Thailand',515247.086971,'Bangkok',101,13.75,151,800,1946,1,1,2016,6,30,800,1946,1,1,2016,6,30,'Thailand',764,'Th','THA\n",
    "],[151,'Vietnam',326086.275122,'HanoI',106,21.03333,152,816,1975,5,1,2016,6,30,816,1975,5,1,2016,6,30,'Viet Nam',704,'Vn','VNM\n",
    "],[152,'Marshall Is.',34.9181835654,'Majuro',171,7.1,153,983,1991,9,17,2016,6,30,983,1986,10,21,2016,6,30,'Marshall Islands',584,'Mh','MHL\n",
    "],[154,'Botswana',581127.216322,'Gaborone',25.91195,-24.6,155,571,1966,9,30,2016,6,30,571,1966,9,30,2016,6,30,'Botswana',72,'Bw','BWA\n",
    "],[158,'Zimbabwe',391930.531069,'Harare',31.04472,-17.8,161,552,1965,11,11,2016,6,30,552,1965,11,11,2016,6,30,'Zimbabwe',716,'Zw','ZWE\n",
    "],[159,'comoros',1726.73596857,'MoronI',43.24028,-11.7,162,581,1975,12,31,2016,6,30,581,1975,7,6,2016,6,30,'comoros',174,'Km','cOM\n",
    "],[161,'MalawI',119198.864167,'Lilongwe',33.78333,-14,164,553,1964,7,6,2016,6,30,553,1964,7,6,2016,6,30,'MalawI',454,'Mw','MWI\n",
    "],[162,'Mozambique',790679.445056,'Maputo',32.58917,-26,165,541,1975,6,25,2016,6,30,541,1975,6,25,2016,6,30,'Mozambique',508,'Mz','MOZ\n",
    "],[163,'South Africa',1224585.7974,'Pretoria',28.22944,-25.7,166,560,1990,3,1,2016,6,30,560,1990,3,21,2016,6,30,'South Africa',710,'Za','ZAF\n",
    "],[164,'Swaziland',17179.4175602,'Mbabane',31.13333,-26.3,167,572,1968,9,6,2016,6,30,572,1968,9,6,2016,6,30,'Swaziland',748,'Sz','SWZ\n",
    "],[165,'Angola',1254965.95582,'Luanda',13.23444,-8.838333,168,540,1975,11,11,2016,6,30,540,1975,11,11,2016,6,30,'Angola',24,'Ao','AGO\n",
    "],[166,'congo',346333.853886,'Brazzaville',15.28472,-4.259167,169,484,1960,8,15,2016,6,30,484,1960,8,15,2016,6,30,'congo',178,'cg','cOG\n",
    "],[167,\"CongoDRC\",2342042.84456,'Kinshasa',15.315,-4.329722,170,490,1960,6,30,2016,6,30,490,1960,6,30,2016,6,30,\"Congo', Democratic Republic of the\",180,'cd','cOD\n",
    "],[168,'Fiji',18172.8656291,'Suva',178,-18.1,171,950,1970,10,10,2016,6,30,950,1970,10,10,2016,6,30,'FijI',242,'FJ,'FJI\n",
    "],[197,'Yemen Peoples Republic',289055.34435,'Aden',45.03667,12.77944,201,680,1967,11,30,1990,5,21,680,1967,11,30,1990,5,21,'Democratic Yemen',720,'Yd','YMD\n",
    "],[169,'Gabon',262447.341739,'Libreville',9.45,0.3833333,172,481,1960,8,17,2016,6,30,481,1960,8,17,2016,6,30,'Gabon',266,'Ga','GAB\n",
    "],[182,'Vanuatu',12335.1176704,'Port-Vila',168,-17.7,185,935,1981,9,15,2016,6,30,935,1980,6,30,2016,6,30,'Vanuatu',548,'Vu','VUT\n",
    "],[183,'canada',9923995.40515,'Ottawa',-75.7,45.42083,186,20,1948,7,1,2016,6,30,20,1948,7,1,2016,6,30,'canada',124,'ca','cAN\n",
    "],[184,'Germany',356448.186123,'Berlin',13.4,52.51667,187,255,1990,10,3,2016,6,30,260,1990,10,3,2016,6,30,'Germany',276,'De','DEU\n",
    "],[185,'Germany Federal Republic',247366.38371,'Bonn',7.1,50.73333,188,260,1955,5,5,1990,10,2,260,1949,9,21,1990,10,2,\"Germany', Federal Republic of\",280,'De','DEU\n",
    "],[186,'Germany Democratic Republic',109081.236269,'Berlin',13.4,52.51667,189,265,1954,3,25,1990,10,2,265,1949,10,5,1990,10,2,'German Democratic Republic',278,'Dd','DDR\n",
    "],[198,'Taiwan',36184.0535634,'TaipeI',122,25.03917,202,713,1949,12,8,2016,6,30,713,1949,12,8,2016,6,30,\"Taiwan', Province of China\",158,'Tw','TWN\n",
    "],[199,'china',9388505.69287,'Beijing',116,39.92889,203,710,1949,12,8,2016,6,30,710,1949,12,8,2016,6,30,'china',156,'cn','cHN\n",
    "],[187,'czechoslovakia',127553.131842,'Prague',14.46667,50.08333,190,315,1946,1,1,1992,12,31,315,1946,1,1,1992,12,31,'czechoslovakia',200,'cs','cSK\n",
    "],[188,'Yugoslavia',255286.979077,'Belgrade',20.46806,44.81861,191,345,1946,1,1,1992,1,14,345,1946,1,1,1991,11,19,\"Yugoslavia', Socialist Federal Republic of\",890,'Yu','YUG\n",
    "],[189,'USSR',22008906.4725,'Moscow',37.61555,55.75222,192,365,1946,1,1,1991,9,5,365,1946,1,1,1991,8,24,'Union of Soviet Socialist Republics',810,'Su','SUN\n",
    "],[200,'Pakistan',1017998.22527,'Islamabad',73.16666,33.7,204,770,1949,1,1,1971,12,15,770,1949,1,1,1971,12,15,'',0,'',''\n",
    "],[194,'Syria',188436.319945,'Damascus',36.3,33.5,198,652,1961,9,29,1967,5,31,-1,-1,-1,-1,-1,-1,-1,'',0,'',''\n",
    "],[195,'Egypt (United Arab Republic)',1190910.29838,'cairo',31.25,30.05,199,651,1958,2,1,1961,8,31,-1,-1,-1,-1,-1,-1,-1,'',0,'',''\n",
    "],[196,'Yemen Arab Republic',137077.141408,'Sanaa',44.18333,15.28333,200,678,1946,1,1,1990,5,21,678,1946,1,1,1990,5,21,'Yemen',886,'Ye','YEM\n",
    "],[212,'Nigeria',867986.036104,'Abuja',7.533333,9.083333,216,475,1960,10,1,1961,5,31,475,1960,10,1,1961,5,31,'',0,'',''\n",
    "],[213,'Libya',1741136.62485,'TripolI',13.18,32.8925,217,620,1973,1,1,2016,6,30,620,1973,1,1,2016,6,30,'Libyan Arab Jamahiriya',434,'Ly','LBY\n",
    "],[201,'Republic of Vietnam',170073.211147,'Saigon',107,10.75,205,817,1954,6,4,1975,4,30,817,1954,5,1,1975,4,30,'Republic of Viet Nam',714,'Vn','VNM\n",
    "],[202,'Vietnam',155964.80192,'HanoI',106,21.03333,206,816,1954,7,21,1975,4,30,816,1954,5,1,1975,4,30,'Democratic Republic of Viet-Nam',704,'Vd','VDR\n",
    "],[203,'Malaysia',331246.371802,'Kuala Lumpur',102,3.166667,207,820,1963,10,1,1965,7,31,820,1963,10,1,1965,7,31,'',0,'',''\n",
    "],[218,'Israel',28159.60054ma32,'Jerusalem',35.22361,31.77917,222,666,1979,5,1,2016,6,30,666,1979,5,1,2016,6,30,'Israel',376,'Il','ISR\n",
    "],[204,'Malaysia',132392.827233,'Kuala Lumpur',102,3.166667,208,820,1957,8,31,1963,9,30,820,1957,8,31,1963,9,30,'',0,'',''\n",
    "],[205,'Malaysia',330691.511322,'Kuala Lumpur',102,3.166667,209,820,1965,8,1,2016,6,30,820,1965,8,1,2016,6,30,'Malaysia',458,'My','MYS\n",
    "],[206,'Singapore',554.860480541,'Singapore City',104,1.293056,210,830,1965,8,9,2016,6,30,830,1965,8,9,2016,6,30,'Singapore',702,'Sg','SGP\n",
    "],[207,'Indonesia',1890541.85149,'Jakarta',107,-6.174444,211,850,1963,5,1,1976,6,30,850,1963,5,1,1976,6,30,'Indonesia',360,'Id','IDN\n",
    "],[208,'Indonesia',1905680.67552,'Jakarta',107,-6.174444,212,850,1976,7,1,2002,9,26,850,1976,7,1,2002,5,19,'Indonesia',360,'Id','IDN\n",
    "],[209,'Indonesia',1890541.85149,'Jakarta',107,-6.174444,213,850,2002,9,27,2016,6,30,850,2002,5,20,2016,6,30,'Indonesia',360,'Id','IDN\n",
    "],[210,'Mali Federation',1456367.07447,'Dakar',-17.5,14.70889,214,432,1960,6,20,1960,8,19,-1,-1,-1,-1,-1,-1,-1,'',0,'',''\n",
    "],[221,'Egypt',941702.026033,'cairo',31.25,30.05,225,651,1967,6,1,1979,4,30,651,1967,6,1,1979,4,30,'Egypt',818,'Eg','EGY\n",
    "],[222,'Egypt',1002473.97844,'cairo',31.25,30.05,226,651,1979,5,1,2016,6,30,651,1979,5,1,2016,6,30,'Egypt',818,'Eg','EGY\n",
    "],[223,'India',3166803.18858,'New Delhi',77.2,28.6,227,750,1949,1,1,2016,6,30,750,1949,1,1,2016,6,30,'India',356,'In','IND\n",
    "],[224,'Pakistan',936135.398004,'Islamabad',73.16666,33.7,228,770,1947,8,14,1948,12,31,770,1947,8,14,1948,12,31,'',0,'',''\n",
    "],[211,'cameroon',425857.301369,'Yaounde',11.51667,3.866667,215,471,1960,1,1,1961,9,30,471,1960,1,1,1961,9,30,'',0,'',''\n",
    "],[214,'chad',1161920.14763,'NDjamena',15.21667,11.71667,218,483,1973,1,1,2016,6,30,483,1973,1,1,2016,6,30,'chad',148,'Td','TCD\n",
    "],[225,'India',3060680.75594,'New Delhi',77.2,28.6,229,750,1947,8,15,1948,12,31,750,1947,8,15,1948,12,31,'',0,'',''\n",
    "],[215,'Morocco',404454.140456,'Rabat',-6.83,34.02,219,600,1958,4,1,1976,3,31,600,1958,4,1,1976,3,31,'Morocco',504,'Ma','MAR\n",
    "],[216,'Morocco',576351.80116,'Rabat',-6.83,34.02,220,600,1976,4,1,1979,8,4,600,1976,4,1,1979,8,4,'Morocco',504,'Ma','MAR\n",
    "],[217,'Mauritania',1142045.83291,'Nouakchott',-16,18.11944,221,435,1976,4,1,1979,8,4,435,1976,4,1,1979,8,4,'Mauritania',478,'Mr','MRT\n",
    "],[219,'Israel',88932.2151228,'Jerusalem',35.22361,31.77917,223,666,1967,6,1,1979,4,30,666,1967,6,1,1979,4,30,'Israel',376,'Il','ISR\n",
    "],[220,'Syria',187318.011221,'Damascus',36.3,33.5,224,652,1967,6,1,2016,6,30,652,1967,6,1,2016,6,30,'Syrian Arab Republic',760,'Sy','SYR\n",
    "],[226,'Yugoslavia',178982.251352,'Belgrade',20.46806,44.81861,230,345,1992,1,15,1992,4,6,-1,-1,-1,-1,-1,-1,-1,\"Yugoslavia', Socialist Federal Republic of\",890,'Yu','YUG\n",
    "],[227,'Serbia and Montenegro',101961.083314,'Belgrade',20.46806,44.81861,231,345,1993,4,8,2006,6,11,345,1992,4,27,2006,6,2,'Serbia and Montenegro',891,'cs','SCG\n",
    "],[228,'USSR',21833788.1441,'Moscow',37.61555,55.75222,232,365,1991,9,6,1991,12,25,-1,-1,-1,-1,-1,-1,-1,'Union of Soviet Socialist Republics',810,'Su','SUN\n",
    "],[229,'Yugoslavia',127444.276526,'Belgrade',20.46806,44.81861,233,345,1992,4,7,1993,4,7,-1,-1,-1,-1,-1,-1,-1,\"Yugoslavia', Socialist Federal Republic of\",890,'Yu','YUG\n",
    "],[230,'Serbia',87939.1875043,'Belgrade',20.46806,44.81861,234,345,2006,6,12,2008,2,19,340,2006,6,3,2008,2,16,'Serbia',688,'Rs','SRB\n",
    "],[231,'Montenegro',14021.8959544,'Podgorica',19.26361,42.44111,235,341,2006,6,12,2016,6,30,341,2006,6,3,2016,6,30,'Montenegro',499,'Me','MNE\n",
    "],[232,'Kosovo',10737.5275034,'Pristina',21.16667,42.66667,236,347,2008,2,20,2016,6,30,347,2008,2,17,2016,6,30,'',0,'',''\n",
    "],[235,'Uganda',243702.806282,'Kampala',32.56556,0.3155556,59,500,1962,10,9,2016,6,30,500,1962,10,9,2016,6,30,'Uganda',800,'Ug','UGA\n",
    "],[236,'Tanzania',947555.660283,'Dar es Salaam',39.28333,-6.8,159,510,1964,4,1,1996,1,31,510,1964,4,1,1996,1,31,\"Tanzania', United Republic of\",834,'Tz','TZA\n",
    "],[233,'Serbia',77201.6601505,'Belgrade',20.46806,44.81861,237,345,2008,2,20,2016,6,30,340,2008,2,17,2016,6,30,'Serbia',688,'Rs','SRB\n",
    "],[234,'United States',9468306.22016,'Washington',-77,38.895,238,2,1946,1,1,2016,6,30,2,1946,1,1,2016,6,30,'United States',840,'Us','USA\n",
    "],[254,''USSR'',19664153.4543,'Moscow',37.61555,55.75222,255,-1,-1,-1,-1,-1,-1,-1,365,1991,12,1,1991,12,15,'Union of Soviet Socialist Republics',810,'Su','SUN\n",
    "],[255,''USSR'',16942946.3752,'Moscow',37.61555,55.75222,256,-1,-1,-1,-1,-1,-1,-1,365,1991,12,16,1991,12,20,'Union of Soviet Socialist Republics',810,'Su','SUN\n",
    "],[237,'Kenya',585730.013409,'NairobI',36.81667,-1.283333,157,501,1963,12,12,2016,6,30,501,1963,12,12,2016,6,30,'Kenya',404,'Ke','KEN\n",
    "],[238,'Tanzania',944923.187336,'Dar es Salaam',39.28333,-6.8,193,510,1961,12,9,1964,3,31,510,1961,12,9,1964,3,31,'',0,'',''\n",
    "],[239,'Saudi Arabia',1963227.28406,'Riyadh',46.77278,24.64083,239,670,1946,1,1,2000,6,11,670,1946,1,1,2000,6,11,'Saudi Arabia',682,'Sa','SAU\n",
    "],[240,'Yemen',426132.485757,'Sanaa',44.18333,15.28333,240,679,1990,5,22,2000,6,11,678,1990,5,22,2000,6,11,'Yemen',887,'Ye','YEM\n",
    "],[241,'Brazil',8523619.57145,'Rio de Janeiro',-43.2,-22.9,241,140,1946,1,1,1960,4,20,140,1946,1,1,1960,4,20,'',0,'',''\n",
    "],[242,'Tanzania',947555.660283,'Dodoma',35.7419,-6.173,242,510,1996,2,1,2016,6,30,510,1996,2,1,2016,6,30,\"Tanzania', United Republic of\",834,'Tz','TZA\n",
    "],[243,'Morocco',674614.242721,'Rabat',-6.83,34.02,243,600,1979,8,5,2016,6,30,600,1979,8,5,2016,6,30,'Morocco',504,'Ma','MAR\n",
    "],[244,'Mauritania',1043783.42747,'Nouakchott',-16,18.11944,244,435,1979,8,5,2016,6,30,435,1979,8,5,2016,6,30,'Mauritania',478,'Mr','MRT\n",
    "],[245,'Sudan',1870245.08989,'Khartoum',32.53417,15.58806,245,625,2011,7,9,2016,6,30,625,2011,7,9,2016,6,30,'Sudan',729,'Sd','SDN\n",
    "],[246,'South Sudan',631411.105296,'Juba',31.6,4.85,246,626,2011,7,9,2016,6,30,626,2011,7,9,2016,6,30,'South Sudan',728,'Ss','SSD\n",
    "],[247,'Yugoslavia',229803.785868,'Belgrade',20.46806,44.81861,247,-1,-1,-1,-1,-1,-1,-1,345,1991,11,20,1992,4,26,\"Yugoslavia', Socialist Federal Republic of\",890,'Yu','YUG\n",
    "],[248,'USSR',21801590.5804,'Moscow',37.61555,55.75222,249,-1,-1,-1,-1,-1,-1,-1,365,1991,8,25,1991,8,26,'Union of Soviet Socialist Republics',810,'Su','SUN\n",
    "],[249,'USSR',21767919.7103,'Moscow',37.61555,55.75222,250,-1,-1,-1,-1,-1,-1,-1,365,1991,8,27,1991,8,30,'Union of Soviet Socialist Republics',810,'Su','SUN\n",
    "],[250,'USSR',21121216.1106,'Moscow',37.61555,55.75222,251,-1,-1,-1,-1,-1,-1,-1,365,1991,8,31,1991,9,5,'Union of Soviet Socialist Republics',810,'Su','SUN\n",
    "],[251,'USSR',20876093.1776,'Moscow',37.61555,55.75222,252,-1,-1,-1,-1,-1,-1,-1,365,1991,9,6,1991,9,8,'Union of Soviet Socialist Republics',810,'Su','SUN\n",
    "],[252,'USSR',20733449.338,'Moscow',37.61555,55.75222,253,-1,-1,-1,-1,-1,-1,-1,365,1991,9,9,1991,10,26,'Union of Soviet Socialist Republics',810,'Su','SUN\n",
    "],[253,'USSR',20261112.1633,'Moscow',37.61555,55.75222,254,-1,-1,-1,-1,-1,-1,-1,365,1991,10,27,1991,11,30,'Union of Soviet Socialist Republics',810,'Su','SUN]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e62dbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot figures with predictions as they evolve over time\n",
    "# New version\n",
    "\n",
    "from matplotlib import cm\n",
    "\n",
    "#ModelSelection = [1,3,5,9,11]\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (6, 6)\n",
    "overleafpath = '/Users/havardhegre/Dropbox (ViEWS)/ViEWS/Projects/PredictingFatalities/PredictionPlots/'\n",
    "path = '/Users/havardhegre/Dropbox (ViEWS)/ViEWS/Projects/PredictingFatalities/PredictionPlots/'\n",
    "\n",
    "log_scale_value = np.array([np.log1p(0), np.log1p(1), np.log1p(3), np.log1p(10), np.log1p(30), np.log1p(100),np.log1p(300),np.log1p(1000),np.log1p(3000)])\n",
    "log_scale_naming = ['0','1','3','10','30','100','300','1000','3000']\n",
    "month_value = np.array([445,451,457,463,469,475,481,487,492])\n",
    "month_name = ['Jan-17','Jul-17','Jan-18','Jul-18','Jan-19','Jul-19','Jan-20','Jul-20','Dec-20']\n",
    "first_month = 444\n",
    "\n",
    "CountryList = [\n",
    "    ('Angola',165,500),\n",
    "    ('Botswana',154,500),\n",
    "    ('BurkinaFaso',47,2000),\n",
    "    ('Burundi',155,2000),\n",
    "#    ('Cameroon',211,2000),\n",
    "    ('Chad',214,2000),\n",
    "    ('Congo',166,2000),\n",
    "    ('DR Congo',167,20000),\n",
    "    ('Egypt',222,5000),\n",
    "    ('Ethiopia',57,2000),\n",
    "    ('Gabon',169,500),\n",
    "    ('Iran',128,2000),\n",
    "    ('Israel',218,2000),\n",
    "    ('Jordan',62,2000),\n",
    "    ('Kenya',237,2000),\n",
    "    ('Lebanon',94,2000),\n",
    "    ('Libya',213,5000),\n",
    "    ('Madagascar',172,2000),\n",
    "    ('Mali',50,20000),\n",
    "    ('Mauritania',244,500),\n",
    "    ('Morocco',243,500),\n",
    "    ('Mozambique',162,2000),\n",
    "    ('Namibia',170,500),\n",
    "    ('Niger',78,2000),\n",
    "    ('Nigeria',79,20000),\n",
    "    ('Oman',119,2000),\n",
    "    ('Rwanda',156,2000),\n",
    "    ('Saudi Arabia',131,500),\n",
    "    ('South Africa',163,2000),\n",
    "    ('South Sudan',246,5000),\n",
    "    ('Sudan',245,2000),\n",
    "    ('Syria',220,50000),\n",
    "    ('Tanzania',242,500),\n",
    "    ('Uganda',235,2000),\n",
    "    ('Yemen',124,20000),\n",
    "    ('Zimbabwe',158,5000),\n",
    "]\n",
    "#t_range = range(0, 23)\n",
    "t_range = [0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36]\n",
    "#t_range = [0,3,6,9,12,15,18,21,24,27,30,33,36]\n",
    "    \n",
    "for model in EnsembleList:\n",
    "    print(model['modelname'])\n",
    "    # Calculate non-logged and cumulative series\n",
    "    for cnt in CountryList:\n",
    "#        print(cnt)\n",
    "        sc_df = model['CountryList'][cnt[1]]['sc_df_smooth']\n",
    "        months = sc_df.index.to_series()\n",
    "        sc_df_exp = sc_df.copy()\n",
    "        sc_df_cum = sc_df.copy()\n",
    "        # Loop over all steps for each country\n",
    "        for column in sc_df.columns:\n",
    "            sc_df_exp[column]=np.rint(np.expm1(sc_df[column]))\n",
    "            sc_df_cum[column]=sc_df_exp[column].cumsum(axis=0, skipna = True)\n",
    "        sc_df_cumtemp = sc_df_cum.copy()\n",
    "        # Set first value of an sc series to the cumulated count up to t-1 (cct1 below)\n",
    "        cumdepvar = sc_df_cum['ln_ged_sb_dep']#.shift(1) # A  cumulative dependent variable series\n",
    "        i = first_month + 1\n",
    "        for column in sc_df.columns[2:]:\n",
    "            cct1 = cumdepvar[i]\n",
    "            sc_df_cum[column]=sc_df_cum[column]+cct1\n",
    "            i = i + 1\n",
    "            \n",
    "        plt.clf()\n",
    "#        print('Country',cnt[0])\n",
    "        plt.bar(months, 'ln_ged_sb_dep', data=sc_df, color='.8')\n",
    "        for m in t_range:\n",
    "            series = 'sc_' + str(444+m)\n",
    "            plt.plot(months, series, data=sc_df, c=cm.hot(np.abs((m/60)+.2)))\n",
    "        plt.ylabel('Number of fatalities')\n",
    "        plt.yticks(log_scale_value, log_scale_naming, rotation=30)\n",
    "        plt.xticks(month_value, month_name, rotation=30)\n",
    "        plt.grid(axis='y')\n",
    "        plt.ylim([0,np.log1p(3000)])\n",
    "        plt.title = cnt[0]\n",
    "        filename = path + 'OverTime/' + model['modelname'] + '_' + cnt[0] + '.png'\n",
    "        plt.savefig(filename, dpi=200)\n",
    "\n",
    "        plt.clf()\n",
    "        plt.title = cnt[0]\n",
    "        plt.bar(months, 'ln_ged_sb_dep', data=sc_df_cum, color='.8')\n",
    "        for m in t_range:\n",
    "            series = 'sc_' + str(444+m)\n",
    "            plt.plot(months, series, data=sc_df_cum, c=cm.hot(np.abs((m/60)+.2)))\n",
    "        plt.ylabel('Cumulative (non-logged) fatalities')\n",
    "        plt.xticks(month_value, month_name, rotation=30)\n",
    "        plt.grid(axis='y')\n",
    "        plt.ylim([0,cnt[2]])\n",
    "    #    plt.show()\n",
    "        filename = path + 'Cumulative/' + model['modelname'] + '_' + cnt[0] + '.png'\n",
    "        plt.savefig(filename, dpi=200)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa3159f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figures showing development over time for Mali and Burkina Faso\n",
    "\n",
    "CountryList = [\n",
    "    ('BurkinaFaso',47,2000),\n",
    "    ('Mali',50,20000),\n",
    "    ('Ethiopia',57,2000),\n",
    "    ('Nigeria',79,20000),\n",
    "]\n",
    "t_range = [0] + steps\n",
    "\n",
    "   \n",
    "for model in EnsembleList[47:]:\n",
    "    print(model['modelname'])\n",
    "    # Calculate non-logged and cumulative series\n",
    "    for cnt in CountryList:\n",
    "#        print(cnt)\n",
    "        sc_df = model['CountryList'][cnt[1]]['sc_df_smooth']\n",
    "        months = sc_df.index.to_series()\n",
    "        sc_df_exp = sc_df.copy()\n",
    "        sc_df_cum = sc_df.copy()\n",
    "        # Loop over all steps for each country\n",
    "        for column in sc_df.columns:\n",
    "            sc_df_exp[column]=np.rint(np.expm1(sc_df[column]))\n",
    "            sc_df_cum[column]=sc_df_exp[column].cumsum(axis=0, skipna = True)\n",
    "        sc_df_cumtemp = sc_df_cum.copy()\n",
    "        # Set first value of an sc series to the cumulated count up to t-1 (cct1 below)\n",
    "        cumdepvar = sc_df_cum['ln_ged_sb_dep']#.shift(1) # A  cumulative dependent variable series\n",
    "        i = first_month + 1\n",
    "        for column in sc_df.columns[2:]:\n",
    "            cct1 = cumdepvar[i]\n",
    "            sc_df_cum[column]=sc_df_cum[column]+cct1\n",
    "            i = i + 1\n",
    "            \n",
    "        plt.clf()\n",
    "#        print('Country',cnt[0])\n",
    "        plt.ylabel('Number of fatalities')\n",
    "        plt.yticks(log_scale_value, log_scale_naming, rotation=30)\n",
    "        plt.xticks(month_value, month_name, rotation=30)\n",
    "        plt.grid(axis='y')\n",
    "        plt.ylim([0,np.log1p(3000)])\n",
    "        plt.title = cnt[0]\n",
    "        for m in t_range:\n",
    "            \n",
    "            plt.clf()\n",
    "    #        print('Country',cnt[0])\n",
    "            plt.ylabel('Number of fatalities')\n",
    "            plt.yticks(log_scale_value, log_scale_naming, rotation=30)\n",
    "            plt.xticks(month_value, month_name, rotation=30)\n",
    "            plt.grid(axis='y')\n",
    "            plt.ylim([0,np.log1p(3000)])\n",
    "            sc_df['truncated_ged_sb'] = sc_df['ln_ged_sb_dep'][0:m]\n",
    "            predseries = 'sc_' + str(444+m)\n",
    "            plt.title = cnt[0]\n",
    "            plt.bar(months, 'truncated_ged_sb', data=sc_df, color='.8')\n",
    "            plt.plot(months, predseries, data=sc_df, c=cm.hot(np.abs((m/60)+.2)))\n",
    "\n",
    "            filename = path + 'OverTime/Rolling/' + model['modelname'] + '_' + cnt[0] + '_' + str(444+m) + '.png'\n",
    "            plt.savefig(filename, dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23915778",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt[0] + ':'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6899120",
   "metadata": {},
   "source": [
    "# Uncertainty of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1af365d",
   "metadata": {},
   "outputs": [],
   "source": [
    "EnsembleList[-1]['test_df_calibrated'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb83e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "months = [487, 488,489,490,491,492]\n",
    "plt.rcParams[\"figure.figsize\"] = (6, 6)\n",
    "overleafpath = '/Users/havardhegre/Dropbox (ViEWS)/ViEWS/Projects/PredictingFatalities/PredictionPlots/'\n",
    "path = '/Users/havardhegre/Dropbox (ViEWS)/ViEWS/Projects/PredictingFatalities/PredictionPlots/'\n",
    "\n",
    "df = EnsembleList[-1]['test_df_calibrated'].loc[months]\n",
    "                      \n",
    "\n",
    "bins = pd.IntervalIndex.from_tuples([(-1, 0.3), (0.3, 1.05), (1.05, 1.89), (1.89, 2.92), (2.92, 4.02), (4.02, 5.15), (5.15, 6.3), (6.3, 7.45), (7.45, 10)])\n",
    "df['fatalitybins_1'] = pd.cut(df['step_pred_1'],bins)\n",
    "df['fatalitybins_1'].describe()\n",
    "i = 0\n",
    "for value in [1,3,10,30,100,300,1000,3000]:\n",
    "    print('Count:', value, 'logged:', np.log1p(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29ca38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentiles = (0.05, 0.10, 0.25,0.5,0.75, 0.9,0.95, 0.99)\n",
    "\n",
    "df['ged_sb_dep'] = np.expm1(df['ln_ged_sb_dep'])\n",
    "df['ged_sb_dep'].describe(percentiles = percentiles)\n",
    "df['exp_pred_2'] = np.expm1(df['step_pred_2'])\n",
    "df['exp_pred_13'] = np.expm1(df['step_pred_13'])\n",
    "bins2 = pd.IntervalIndex.from_tuples([(0, 3), (3, 10), (10, 30), (30, 100), (100, 300), (300, 1000), (1000, 100000)])\n",
    "bins2 = pd.IntervalIndex.from_tuples([(10, 30), (100, 300),  (1000, 100000)])\n",
    "bins2 = pd.IntervalIndex.from_tuples([(3, 10), (30, 100),  (300, 1000)])\n",
    "\n",
    "\n",
    "df['fatalitybins2_2'] = pd.cut(df['exp_pred_2'],bins2)\n",
    "df['fatalitybins2_2'].describe()\n",
    "\n",
    "df['fatalitybins2_13'] = pd.cut(df['exp_pred_13'],bins2)\n",
    "df['fatalitybins2_13'].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecab336",
   "metadata": {},
   "outputs": [],
   "source": [
    "for bin in bins:\n",
    "    print(bin)\n",
    "    print( df['ln_ged_sb_dep'][df['fatalitybins_1']==bin].describe(percentiles = percentiles))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214dd482",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"ticks\")\n",
    "step=13\n",
    "# Initialize the figure with a logarithmic x axis\n",
    "f, ax = plt.subplots(figsize=(7, 6))\n",
    "ax.set_xscale(\"log\")\n",
    "\n",
    "# Plot the orbital period with horizontal boxes\n",
    "sns.boxplot(x=\"ged_sb_dep\", y=\"fatalitybins2_13\", data=df,\n",
    "            whis=[5, 95], width=.9, palette=\"vlag\")\n",
    "\n",
    "# Add in points to show each observation\n",
    "sns.stripplot(x=\"ged_sb_dep\", y=\"fatalitybins2_13\", data=df,\n",
    "              size=4, color=\".3\", linewidth=0)\n",
    "\n",
    "# Tweak the visual presentation\n",
    "ax.xaxis.grid(True)\n",
    "ax.set(ylabel=\"Predicted number of fatalities\")\n",
    "ax.set(xlabel=\"Observed number of fatalities\")\n",
    "sns.despine(trim=True, left=True)\n",
    "\n",
    "overleafpath = '/Users/havardhegre/Dropbox (ViEWS)/Apps/Overleaf/ViEWS predicting fatalities/Figures/PredictionPlots/'\n",
    "filename = overleafpath + 'PredictionUncertainty_cm_s' + str(step) + '.png'\n",
    "plt.savefig(filename, dpi=300)\n",
    "                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d79823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Horizontal box plots with observations\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set_theme(style=\"ticks\")\n",
    "\n",
    "# Initialize the figure with a logarithmic x axis\n",
    "f, ax = plt.subplots(figsize=(7, 6))\n",
    "ax.set_xscale(\"log\")\n",
    "\n",
    "# Load the example planets dataset\n",
    "planets = sns.load_dataset(\"planets\")\n",
    "\n",
    "# Plot the orbital period with horizontal boxes\n",
    "sns.boxplot(x=\"distance\", y=\"method\", data=planets,\n",
    "            whis=[0, 100], width=.6, palette=\"vlag\")\n",
    "\n",
    "# Add in points to show each observation\n",
    "sns.stripplot(x=\"distance\", y=\"method\", data=planets,\n",
    "              size=4, color=\".3\", linewidth=0)\n",
    "\n",
    "# Tweak the visual presentation\n",
    "ax.xaxis.grid(True)\n",
    "ax.set(ylabel=\"\")\n",
    "sns.despine(trim=True, left=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c34e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hist\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#plt.style.use('_mpl-gallery')\n",
    "\n",
    "# make data\n",
    "np.random.seed(1)\n",
    "x = 4 + np.random.normal(0, 1.5, 200)\n",
    "\n",
    "# plot:\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.hist(x, bins=8, linewidth=0.5, edgecolor=\"white\")\n",
    "\n",
    "ax.set(xlim=(0, 8), xticks=np.arange(1, 8),\n",
    "       ylim=(0, 56), yticks=np.linspace(0, 56, 9))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d20241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2d hist\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.style.use('_mpl-gallery-nogrid')\n",
    "\n",
    "# make data: correlated + noise\n",
    "np.random.seed(1)\n",
    "x = np.random.randn(5000)\n",
    "y = 1.2 * x + np.random.randn(5000) / 3\n",
    "\n",
    "# plot:\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.hist2d(x, y, bins=(np.arange(-3, 3, 0.1), np.arange(-3, 3, 0.1)))\n",
    "\n",
    "ax.set(xlim=(-2, 2), ylim=(-3, 3))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22671f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plot \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.style.use('_mpl-gallery')\n",
    "\n",
    "# make data:\n",
    "np.random.seed(10)\n",
    "D = np.random.normal((3, 5, 4), (1.25, 1.00, 1.25), (100, 3))\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots()\n",
    "VP = ax.boxplot(D, positions=[2, 4, 6], widths=1.5, patch_artist=True,\n",
    "                showmeans=False, showfliers=False,\n",
    "                medianprops={\"color\": \"white\", \"linewidth\": 0.5},\n",
    "                boxprops={\"facecolor\": \"C0\", \"edgecolor\": \"white\",\n",
    "                          \"linewidth\": 0.5},\n",
    "                whiskerprops={\"color\": \"C0\", \"linewidth\": 1.5},\n",
    "                capprops={\"color\": \"C0\", \"linewidth\": 1.5})\n",
    "\n",
    "ax.set(xlim=(0, 8), xticks=np.arange(1, 8),\n",
    "       ylim=(0, 8), yticks=np.arange(1, 8))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede3ef18",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in t_range:\n",
    "    series = 'sc_' + str(444+m)\n",
    "    plt.plot(months, series, data=sc_df, c=cm.hot(np.abs((m/60)+.2)))\n",
    "plt.ylabel('Number of fatalities')\n",
    "plt.yticks(log_scale_value, log_scale_naming, rotation=30)\n",
    "plt.xticks(month_value, month_name, rotation=30)\n",
    "plt.grid(axis='y')\n",
    "plt.ylim([0,np.log1p(3000)])\n",
    "plt.title = cnt[0]\n",
    "filename = path + 'OverTime/' + model['modelname'] + '_' + cnt[0] + '.png'\n",
    "plt.savefig(filename, dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7823d6bd",
   "metadata": {},
   "source": [
    "# Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb1eb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import contextily as ctx\n",
    "\n",
    "from views_dataviz import color\n",
    "from views_dataviz.map import utils\n",
    "from views_dataviz.map.presets import ViewsMap\n",
    "\n",
    "import sqlalchemy as sa\n",
    "from ingester3.config import source_db_path\n",
    "from ingester3.Country import Country\n",
    "from ingester3.extensions import *\n",
    "from ingester3.ViewsMonth import ViewsMonth\n",
    "\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class Mapper2:\n",
    "    \"\"\"\n",
    "    `Map` takes basic properties and allows the user to consecutively add\n",
    "    layers to the Map object. This makes it possible to prepare mapping\n",
    "    \"presets\" at any level of layeredness that can be built on further.\n",
    "    \n",
    "    Mapper2 allows for the customizable addition of scaling to the map. \n",
    "    -re-add the code for labels later when i can test it\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    width: Integer value for width in inches.\n",
    "    height: Integer value for height in inches.\n",
    "    bbox: List for the bbox per [xmin, xmax, ymin, ymax].\n",
    "    frame_on: Bool for whether to draw a frame around the map.\n",
    "    title: Optional default title at matplotlib's default size.\n",
    "    figure: Optional tuple of (fig, size) to use if you want to plot into an\n",
    "        already existing fig and ax, rather than making a new one.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        width,\n",
    "        height,\n",
    "        bbox=None,\n",
    "        cmap=None,\n",
    "        frame_on=True,\n",
    "        title=\"\",  # Default title without customization. (?)\n",
    "        figure=None,\n",
    "    ):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.bbox = bbox  # xmin, xmax, ymin, ymax\n",
    "        self.cmap = cmap\n",
    "        if figure is None:\n",
    "            self.fig, self.ax = plt.subplots(figsize=(self.width, self.height))\n",
    "        else:\n",
    "            self.fig, self.ax = figure\n",
    "        self.texts = []\n",
    "        self.ax.set_title(title)\n",
    "\n",
    "        if frame_on:  # Remove axis ticks only.\n",
    "            self.ax.tick_params(\n",
    "                top=False,\n",
    "                bottom=False,\n",
    "                left=False,\n",
    "                right=False,\n",
    "                labelleft=False,\n",
    "                labelbottom=False,\n",
    "            )\n",
    "        else:\n",
    "            self.ax.axis(\"off\")\n",
    "\n",
    "        if bbox is not None:\n",
    "            self.ax.set_xlim((self.bbox[0], self.bbox[1]))\n",
    "            self.ax.set_ylim((self.bbox[2], self.bbox[3]))\n",
    "\n",
    "    def add_layer(self, gdf, map_scale=False, map_dictionary=False, cmap=None, inform_colorbar=False, **kwargs):\n",
    "        \"\"\"Add a geopandas plot to a new layer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        gdf: Geopandas GeoDataFrame to plot.\n",
    "        cmap: Optional matplotlib colormap object or string reference\n",
    "            (e.g. \"viridis\").\n",
    "        inform_colorbar: Set or overwrite colorbar with the current layer.\n",
    "            Not applicable when `color` is supplied in the kwargs.\n",
    "        map_scale: set a manual scale for the map. If missing defaults to the Remco procedure. \n",
    "        map_dictionary: set manual labels for the map. If missing defaults to the default labels.\n",
    "        **kwargs: Geopandas `.plot` keyword arguments.\n",
    "        \"\"\"\n",
    "        if \"color\" in kwargs:\n",
    "            colormap = None\n",
    "        else:\n",
    "            colormap = self.cmap if cmap is None else cmap\n",
    "            if inform_colorbar and \"column\" in kwargs:\n",
    "                if hasattr(self, \"cax\"):\n",
    "                    self.cax.remove()\n",
    "                if \"vmin\" not in kwargs:\n",
    "                    self.vmin = gdf[kwargs[\"column\"]].min()\n",
    "                else:\n",
    "                    self.vmin = kwargs[\"vmin\"]\n",
    "                if \"vmax\" not in kwargs:\n",
    "                    self.vmax = gdf[kwargs[\"column\"]].max()\n",
    "                else:\n",
    "                    self.vmax = kwargs[\"vmax\"]\n",
    "        \n",
    "        try: Mapper2.add_colorbar(self, colormap, min(map_scale), max(map_scale))\n",
    "        except: Mapper2.add_colorbar(self, colormap, self.vmin, self.vmax)\n",
    "        \n",
    "        try:\n",
    "            self.ax = gdf.plot(ax=self.ax, cmap=colormap, vmin=min(map_scale), vmax=max(map_scale), **kwargs)\n",
    "        except: \n",
    "            self.ax = gdf.plot(ax=self.ax, cmap=colormap, **kwargs)\n",
    "\n",
    "                \n",
    "        return self\n",
    "    \n",
    "    def add_colorbar(\n",
    "        self,\n",
    "        cmap,\n",
    "        vmin,\n",
    "        vmax,\n",
    "        location=\"right\",\n",
    "        size=\"5%\",\n",
    "        pad=0.1,\n",
    "        alpha=1,\n",
    "        labelsize=16,\n",
    "        tickparams=None,\n",
    "    ):\n",
    "        \"\"\"Add custom colorbar to Map.\n",
    "\n",
    "        Needed since GeoPandas legend and plot axes do not align, see:\n",
    "        https://geopandas.readthedocs.io/en/latest/docs/user_guide/mapping.html\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        cmap: Matplotlib colormap object or string reference (e.g. \"viridis\").\n",
    "        vmin: Minimum value of range colorbar.\n",
    "        vmax: Maximum value of range colorbar.\n",
    "        location: String for location of colorbar: \"top\", \"bottom\", \"left\"\n",
    "            or \"right\".\n",
    "        size: Size in either string percentage or number of pixels.\n",
    "        pad: Float for padding between the plot's frame and colorbar.\n",
    "        alpha: Float for alpha to apply to colorbar.\n",
    "        labelsize: Integer value for the text size of the ticklabels.\n",
    "        tickparams: Dictionary containing value-label pairs. For example:\n",
    "            {0.05: \"5%\", 0.1: \"10%\"}\n",
    "        \"\"\"\n",
    "        norm = plt.Normalize(vmin, vmax)\n",
    "        if isinstance(cmap, str):\n",
    "            cmap = plt.get_cmap(cmap)\n",
    "        cmap = color.force_alpha_colormap(cmap=cmap, alpha=alpha)\n",
    "        scalar_to_rgba = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "        divider = make_axes_locatable(self.ax)\n",
    "        self.cax = divider.append_axes(location, size, pad)\n",
    "        self.cax.tick_params(labelsize=labelsize)\n",
    "        tickvalues = (\n",
    "            list(tickparams.keys()) if tickparams is not None else None\n",
    "        )\n",
    "        self.cbar = plt.colorbar(\n",
    "            scalar_to_rgba, cax=self.cax, ticks=tickvalues\n",
    "        )\n",
    "        if tickparams is not None:\n",
    "            self.cbar.set_ticklabels(list(tickparams.values()))\n",
    "        return self\n",
    "    \n",
    "    def save(\n",
    "        self, path, dpi=200, **kwargs\n",
    "    ):  # Just some defaults to reduce work.\n",
    "        \"\"\"Save Map figure to file.\n",
    "        Parameters\n",
    "        ----------\n",
    "        path: String path, e.g. \"./example.png\".\n",
    "        dpi: Integer dots per inch. Increase for higher resolution figures.\n",
    "        **kwargs: Matplotlib `savefig` keyword arguments.\n",
    "        \"\"\"\n",
    "        self.fig.savefig(path, dpi=dpi, bbox_inches=\"tight\", **kwargs)\n",
    "        plt.close(self.fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3242b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vid2date(i):\n",
    "    year=str(ViewsMonth(i).year)\n",
    "    month=str(ViewsMonth(i).month)\n",
    "    return year+'/'+month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89ebbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "times = [445, 447, 450, 468]\n",
    "allsteps = [1, 3, 6, 24]\n",
    "titles = [vid2date(i) for i in times]\n",
    "#note the zip function occured earlier\n",
    "standard_scale = [np.log1p(0),np.log1p(10), np.log1p(50), np.log1p(100), np.log1p(1000), np.log1p(10000)]\n",
    "standard_scale_labels = ['0','10', '50','100', '1000', '10000']\n",
    "\n",
    "small_scale=[np.log1p(0),np.log1p(10), np.log1p(50), np.log1p(100), np.log1p(500)]\n",
    "small_scale_labels = ['0','10', '50','100', '500']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df63153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the gdf\n",
    "engine = sa.create_engine(source_db_path)\n",
    "gdf_base = gpd.GeoDataFrame.from_postgis(\n",
    "    \"SELECT id as country_id, in_africa, in_me, geom FROM prod.country\", \n",
    "    engine, \n",
    "    geom_col='geom'\n",
    ")\n",
    "gdf = gdf_base.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80339ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e7f9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test partition maps, predictions, rolling\n",
    "times_steps = [1, 3]\n",
    "lastmonthwithdata = 444\n",
    "path = '/Users/havardhegre/Dropbox (ViEWS)/ViEWS/Projects/PredictingFatalities/maps/cm_rolling/'\n",
    "\n",
    "model = EnsembleList[-1]\n",
    "\n",
    "gdf2 = gdf_base.copy()\n",
    "df = model['test_df_calibrated'].copy()\n",
    "df = df.join(gdf2.set_index(\"country_id\"))\n",
    "gdf = gpd.GeoDataFrame(df, geometry=\"geom\")\n",
    "\n",
    "for step in times_steps:\n",
    "    for tshift in [0,3,6,9,12,15,18,21,24,27,30,33,36,39,42,45]:\n",
    "        month = step + tshift + lastmonthwithdata\n",
    "        modelname = model['modelname']\n",
    "        m=Mapper2(\n",
    "        width=10,\n",
    "        height=10,\n",
    "        frame_on=True,\n",
    "        title='Model: '+ model['modelname'] + ', predictions as of ' + vid2date(lastmonthwithdata + tshift) + ', ' + str(step) + ' months ahead',\n",
    "        bbox=[-18.5, 64.0, -35.5, 43.0], \n",
    "        ).add_layer(\n",
    "        gdf=gdf.loc[month],\n",
    "        map_scale=standard_scale,\n",
    "        cmap=\"rainbow\",\n",
    "        edgecolor=\"black\",\n",
    "        linewidth=0.5,\n",
    "        column=f\"step_pred_{step}\", \n",
    "        inform_colorbar=True\n",
    "        )\n",
    "        m.cbar.set_ticks(standard_scale)\n",
    "        m.cbar.set_ticklabels(standard_scale_labels)\n",
    "\n",
    "        m.save(f'{path}cm_{modelname}_standard_scale_s{step}_t{tshift}_m{month}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2fe147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test partition maps, actuals\n",
    "lastmonthwithdata = 444\n",
    "path = '/Users/havardhegre/Dropbox (ViEWS)/ViEWS/Projects/PredictingFatalities/maps/cm_actuals/'\n",
    "\n",
    "model = EnsembleList[-1]\n",
    "\n",
    "gdf2 = gdf_base.copy()\n",
    "df = model['test_df_calibrated'].copy()\n",
    "df = df.join(gdf2.set_index(\"country_id\"))\n",
    "gdf = gpd.GeoDataFrame(df, geometry=\"geom\")\n",
    "\n",
    "for step in steps:\n",
    "    month = step + lastmonthwithdata\n",
    "    modelname = model['modelname']\n",
    "    m=Mapper2(\n",
    "    width=10,\n",
    "    height=10,\n",
    "    frame_on=True,\n",
    "    title='Actually recorded fatalities, month ' + vid2date(month),\n",
    "    bbox=[-18.5, 64.0, -35.5, 43.0], \n",
    "    ).add_layer(\n",
    "    gdf=gdf.loc[month],\n",
    "    map_scale=standard_scale,\n",
    "    cmap=\"rainbow\",\n",
    "    edgecolor=\"black\",\n",
    "    linewidth=0.5,\n",
    "    column=f\"ln_ged_sb_dep\", \n",
    "    inform_colorbar=True\n",
    "    )\n",
    "    m.cbar.set_ticks(standard_scale)\n",
    "    m.cbar.set_ticklabels(standard_scale_labels)\n",
    "\n",
    "    m.save(f'{path}cm_actuals_standard_scale_s{step}_m{month}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4428d37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Future maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24062a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Future maps\n",
    "times_steps = [1,3,6,12,24,36]\n",
    "lastmonthwithdata = 492\n",
    "path = '/Users/havardhegre/Dropbox (ViEWS)/ViEWS/Projects/PredictingFatalities/maps/FuturePredictions_cm/'\n",
    "\n",
    "model = EnsembleList[-1] \n",
    "\n",
    "#gdf = gpd.GeoDataFrame.from_postgis(\n",
    "#    \"SELECT id as country_id, in_africa, in_me, geom FROM prod.country\", \n",
    "#    engine, \n",
    "#    geom_col='geom'\n",
    "#)\n",
    "gdf2 = gdf_base.copy()\n",
    "df = model['future_df_calibrated'].copy()\n",
    "df = df.join(gdf2.set_index(\"country_id\"))\n",
    "gdf = gpd.GeoDataFrame(df, geometry=\"geom\")\n",
    "\n",
    "for step in times_steps:\n",
    "    month = step + lastmonthwithdata\n",
    "    modelname = model['modelname']\n",
    "    m=Mapper2(\n",
    "    width=10,\n",
    "    height=10,\n",
    "    frame_on=True,\n",
    "    title='model '+ model['modelname']+ ', predictions, ' + str(step),\n",
    "    bbox=[-18.5, 64.0, -35.5, 43.0], \n",
    "    ).add_layer(\n",
    "    gdf=gdf.loc[month],\n",
    "    map_scale=standard_scale,\n",
    "    cmap=\"rainbow\",\n",
    "    edgecolor=\"black\",\n",
    "    linewidth=0.5,\n",
    "    column=f\"step_combined\", \n",
    "    inform_colorbar=True\n",
    "    )\n",
    "    m.cbar.set_ticks(standard_scale)\n",
    "    m.cbar.set_ticklabels(standard_scale_labels)\n",
    "    \n",
    "    m.save(f'{path}cm_{modelname}_standard_scale_{step}_{month}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29364724",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4ad97f",
   "metadata": {},
   "source": [
    "# Line plots, future predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74566ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = EnsembleList[0]['future_df_calibrated'].copy()\n",
    "df['exp_pred'] = np.expm1(df['step_combined'])\n",
    "df['month'] = df.index.get_level_values('month_id')\n",
    "months_to_plot = [*(range(505,540))]\n",
    "month_labels = [vid2date(i) for i in months_to_plot]\n",
    "\n",
    "CountryList = [\n",
    "    ('Angola',165,500),\n",
    "    ('Botswana',154,500),\n",
    "    ('BurkinaFaso',47,2000),\n",
    "    ('Burundi',155,2000),\n",
    "#    ('Cameroon',211,2000),\n",
    "    ('Chad',214,2000),\n",
    "    ('Congo',166,2000),\n",
    "    ('DR Congo',167,20000),\n",
    "    ('Egypt',222,5000),\n",
    "    ('Ethiopia',57,2000),\n",
    "    ('Gabon',169,500),\n",
    "    ('Iran',128,2000),\n",
    "    ('Israel',218,2000),\n",
    "    ('Jordan',62,2000),\n",
    "    ('Kenya',237,2000),\n",
    "    ('Lebanon',94,2000),\n",
    "    ('Libya',213,5000),\n",
    "    ('Mali',50,20000),\n",
    "    ('Mauritania',244,500),\n",
    "    ('Morocco',243,500),\n",
    "    ('Mozambique',162,2000),\n",
    "    ('Namibia',170,500),\n",
    "    ('Niger',78,2000),\n",
    "    ('Nigeria',79,20000),\n",
    "    ('Oman',119,2000),\n",
    "    ('Rwanda',156,2000),\n",
    "    ('Saudi Arabia',131,500),\n",
    "    ('South Africa',163,2000),\n",
    "    ('South Sudan',246,5000),\n",
    "    ('Sudan',245,2000),\n",
    "    ('Syria',220,50000),\n",
    "    ('Tanzania',242,500),\n",
    "    ('Uganda',235,2000),\n",
    "    ('Yemen',124,20000),\n",
    "    ('Zimbabwe',158,5000),\n",
    "]\n",
    "\n",
    "CountryList = [\n",
    "    ('BurkinaFaso',47,2000),\n",
    "    ('Burundi',155,2000),\n",
    "#    ('Cameroon',211,2000),\n",
    "    ('Chad',214,2000),\n",
    "    ('Congo',166,2000),\n",
    "    ('DR Congo',167,20000),\n",
    "    ('Egypt',222,5000),\n",
    "    ('Ethiopia',57,2000),\n",
    "    ('Libya',213,5000),\n",
    "    ('Mali',50,20000),\n",
    "    ('Mozambique',162,2000),\n",
    "    ('Niger',78,2000),\n",
    "    ('Nigeria',79,20000),\n",
    "    ('Oman',119,2000),\n",
    "    ('Rwanda',156,2000),\n",
    "    ('South Sudan',246,5000),\n",
    "    ('Sudan',245,2000),\n",
    "    ('Syria',220,50000),\n",
    "    ('Tanzania',242,500),\n",
    "    ('Uganda',235,2000),\n",
    "    ('Yemen',124,20000),\n",
    "    ('Zimbabwe',158,5000),\n",
    "]\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "for cnt in CountryList:\n",
    "    print(cnt[0])\n",
    "    df_cnt = df.xs(cnt[1], level='country_id')\n",
    "    sns.lineplot(x=\"month\", y=\"exp_pred\",\n",
    "                 data=df_cnt)\n",
    "    \n",
    "#    cntplot.set_xtickslabels(month_labels)\n",
    "    plt.xlabel(\"Month\")\n",
    "    plt.ylabel(\"Predicted fatalities\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da100d50",
   "metadata": {},
   "source": [
    "# Surrogate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f681d19e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c6621555",
   "metadata": {},
   "source": [
    "# Old stuff from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eff8168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cumulative fatalities\n",
    "\n",
    "\n",
    "cnt[2].head(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e722cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (10, 10)\n",
    "from matplotlib import cm\n",
    "t_range = range(0, 23)\n",
    "t_range = [0,2,4,6,8,10,12,14,16,18,20,22,24]\n",
    "width = .75 # width of a bar\n",
    "fig, ax = plt.subplots()\n",
    "Mozambique['lndepvar'].plot(kind= 'bar', width=width,color='grey')\n",
    "Mozambique['sc_444'].plot(kind='line', color='blue',ms=100)\n",
    "#df['data'].plot(kind='line', marker='*', color='black', ms=10)\n",
    "\n",
    "\n",
    "#ax = plt.gca()\n",
    "#plt.xlim([-width, len(Mozambique['sc_444'])-width])\n",
    "#for m in t_range:\n",
    "#    series = 'sc_' + str(444+m)\n",
    "#    plt.plot('month_id', series, data=Mozambique, color=cm.hot(np.abs(m/48)))\n",
    "ax.set_xticklabels(('Jan-18', '', '', 'Apr-18', '', '', 'Jul-18', '', '', 'Oct-18', '', '',\n",
    "                   'Jan-19', '', '', 'Apr-19', '', '', 'Jul-19', '', '', 'Oct-19', '', '',\n",
    "                   'Jan-20', '', '', 'Apr-20', '', '', 'Jul-20', '', '', 'Oct-20', '', ''))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d567b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect distribution of ensemble predictions\n",
    "percentiles_to_inspect = [.75, .85, .9, .95, .99]\n",
    "StepEnsembles[0]['ensembles_test'].describe(percentiles_to_inspect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1996cb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE_ensemble_all_df\n",
    "#ensemble_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f502b784",
   "metadata": {},
   "outputs": [],
   "source": [
    "overleafpath = '/Users/havardhegre/Dropbox (ViEWS)/Apps/Overleaf/ViEWS predicting fatalities/Tables/'\n",
    "tables = [(MSE_ensemble_all_df,'MSE_ensemble'), (MSE_ensemble_zeros_df,'MSE_ensemble_zeros'), (MSE_ensemble_nonzeros_df,'MSE_ensemble_nonzeros')]\n",
    "columns = ensemble_models\n",
    "for table in tables:\n",
    "    print('Table name',table[1])\n",
    "    filename = overleafpath + table[1] + '.tex'\n",
    "    print('File name: ', filename)\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(table[0].to_latex(float_format=\"%.3f\",index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0191b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.expm1(6.13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61ea40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing for mapping\n",
    "engine = sa.create_engine(source_db_path)\n",
    "gdf = gpd.GeoDataFrame.from_postgis(\n",
    "    \"SELECT id as country_id, in_africa, in_me, geom FROM prod.country\", \n",
    "    engine, \n",
    "    geom_col='geom'\n",
    ")\n",
    "gdf = gdf.to_crs(4326)\n",
    "data = StepEnsembles[0]['ensembles_test'].join(gdf.set_index(\"country_id\"))\n",
    "gdf = gpd.GeoDataFrame(data, geometry=\"geom\")\n",
    "gdf = gdf.loc[480]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99d03fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping the outcome variable\n",
    "\n",
    "#data = StepEnsembles[0]['ensembles_test'].loc[480]\n",
    "\n",
    "ensemble_columns = ['lndepvar'] + ['unweighted_average','linear_regression','rf_regression']\n",
    "\n",
    "for column in ensemble_columns:\n",
    "    m = ViewsMap(\n",
    "        width=10,\n",
    "        label=f\"Ensembles: {column}\",\n",
    "        title=\"2021-8\",\n",
    "        scale=None,\n",
    "        bbox=\"africa_middle_east\"\n",
    "    ).add_layer(\n",
    "        gdf,\n",
    "        edgecolor=\"black\",\n",
    "        linewidth=0.5,\n",
    "        column=\"lndepvar\",\n",
    "        inform_colorbar=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045f14a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Plotting over time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512df3c7",
   "metadata": {},
   "source": [
    "## Inspect the predictions, calibration partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16380702",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_inspect = ['ln_ged_sb_dep','step_pred_1','step_pred_2','step_pred_3','step_pred_4']\n",
    "predictions_test[to_inspect].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262c08c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the predictions in non-logged form to explore calibration \n",
    "for var in ['step_pred_1','step_pred_2','step_pred_3','step_pred_4']:\n",
    "    expvar = 'exp_' + var\n",
    "    predictions_calib[expvar] = np.expm1(predictions_calib[var])\n",
    "    predictions_test[expvar] = np.expm1(predictions_test[var])\n",
    "\n",
    "predictions_calib['ged_sb'] = np.expm1(predictions_calib['ln_ged_sb_dep'])\n",
    "predictions_test['ged_sb'] = np.expm1(predictions_test['ln_ged_sb_dep'])\n",
    "\n",
    "to_inspect = ['ln_ged_sb_dep','ged_sb','exp_step_pred_1','exp_step_pred_2','exp_step_pred_3','exp_step_pred_4']\n",
    "predictions_test[to_inspect].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5141063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 245: Sudan. 91: Egypt. 79: Nigeria. 50: Mali\n",
    "colset = ['ged_sb','exp_step_pred_1','exp_step_pred_3','exp_step_pred_3','exp_step_pred_4']\n",
    "countryset = [50, 79, 91, 245]\n",
    "countryset = [50]\n",
    "idx = pd.IndexSlice\n",
    "predictions_calib.loc[idx[397:408, countryset], :][colset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b126b3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hh_data.loc[idx[380:408, countryset], :]['ln_ged_sb']\n",
    "\n",
    "percentiles_to_inspect = [.75, .85, .9, .95, .99]\n",
    "predictions_calib['ln_ged_sb_dep'].describe(percentiles=percentiles_to_inspect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda834c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibration in terms of logged predictions/actuals:# Non-shifted\n",
    "# \"Old function\"\n",
    "print(\"In terms of logged predictions/actuals, non-shifted, all obs\")\n",
    "# Calibration partition:\n",
    "predictions_calib['step_pred_1_cal_log_simple'] = mean_sd_calibrated_simple(\n",
    "    y_true_calpart = predictions_calib['ln_ged_sb_dep'], \n",
    "    y_pred_calpart = predictions_calib['step_pred_1'], \n",
    "    y_pred_test = predictions_calib['step_pred_1'], \n",
    "    shift=False, \n",
    "    threshold = 0\n",
    ")\n",
    "\n",
    "# Test partition:\n",
    "predictions_test['step_pred_1_cal_log_simple'] = mean_sd_calibrated_simple(\n",
    "    y_true_calpart = predictions_calib['ln_ged_sb_dep'], \n",
    "    y_pred_calpart = predictions_calib['step_pred_1'], \n",
    "    y_pred_test = predictions_test['step_pred_1'],\n",
    "    shift=False, \n",
    "    threshold = 0\n",
    ")\n",
    "# New function\n",
    "# Non-shifted\n",
    "print(\"In terms of logged predictions/actuals, non-shifted, all obs\")\n",
    "# Calibration partition:\n",
    "predictions_calib['step_pred_1_cal_log_nonshifted'] = mean_sd_calibrated(\n",
    "    y_true_calpart = predictions_calib['ln_ged_sb_dep'], \n",
    "    y_pred_calpart = predictions_calib['step_pred_1'], \n",
    "    y_pred_test = predictions_calib['step_pred_1'], \n",
    "    shift=False, \n",
    "    threshold = 0\n",
    ")\n",
    "\n",
    "# Test partition:\n",
    "predictions_test['step_pred_1_cal_log_nonshifted'] = mean_sd_calibrated(\n",
    "    y_true_calpart = predictions_calib['ln_ged_sb_dep'], \n",
    "    y_pred_calpart = predictions_calib['step_pred_1'], \n",
    "    y_pred_test = predictions_test['step_pred_1'],\n",
    "    shift=False, \n",
    "    threshold = 0\n",
    ")\n",
    "    \n",
    "# Shifted\n",
    "print(\"In terms of logged predictions/actuals, shifted, all obs\")\n",
    "# Calibration partition:\n",
    "predictions_calib['step_pred_1_cal_log_shifted'] = mean_sd_calibrated(\n",
    "    y_true_calpart = predictions_calib['ln_ged_sb_dep'], \n",
    "    y_pred_calpart = predictions_calib['step_pred_1'], \n",
    "    y_pred_test = predictions_calib['step_pred_1'], \n",
    "    shift=True, \n",
    "    threshold = 0\n",
    ")\n",
    "\n",
    "# Test partition:\n",
    "predictions_test['step_pred_1_cal_log_shifted'] = mean_sd_calibrated(\n",
    "    y_true_calpart = predictions_calib['ln_ged_sb_dep'], \n",
    "    y_pred_calpart = predictions_calib['step_pred_1'], \n",
    "    y_pred_test = predictions_test['step_pred_1'],\n",
    "    shift=True, \n",
    "    threshold = 0\n",
    ")\n",
    "        \n",
    "# Calibration in terms of log, only \"non-zero\" predictions:\n",
    "# Shifted\n",
    "print(\"In terms of logged predictions/actuals, shifted, only non-zeros\")\n",
    "# Calibration partition:\n",
    "predictions_calib['step_pred_1_cal_log_nonzero_shifted'] = mean_sd_calibrated(\n",
    "    y_true_calpart = predictions_calib['ln_ged_sb_dep'], \n",
    "    y_pred_calpart = predictions_calib['step_pred_1'], \n",
    "    y_pred_test = predictions_calib['step_pred_1'], \n",
    "    shift=True, \n",
    "    threshold = 1\n",
    ")\n",
    "\n",
    "# Test partition:\n",
    "predictions_test['step_pred_1_cal_log_nonzero_shifted'] = mean_sd_calibrated(\n",
    "    y_true_calpart = predictions_calib['ln_ged_sb_dep'], \n",
    "    y_pred_calpart = predictions_calib['step_pred_1'], \n",
    "    y_pred_test = predictions_test['step_pred_1'],\n",
    "    shift=True, \n",
    "    threshold = 1\n",
    ")\n",
    "\n",
    "# Non-shifted\n",
    "print(\"In terms of logged predictions/actuals, non-shifted, only non-zeros\")\n",
    "# Calibration partition:\n",
    "predictions_calib['step_pred_1_cal_log_nonzero_nonshifted'] = mean_sd_calibrated(\n",
    "    y_true_calpart = predictions_calib['ln_ged_sb_dep'], \n",
    "    y_pred_calpart = predictions_calib['step_pred_1'], \n",
    "    y_pred_test = predictions_calib['step_pred_1'], \n",
    "    shift=False, \n",
    "    threshold = 1\n",
    ")\n",
    "\n",
    "# Test partition:\n",
    "predictions_test['step_pred_1_cal_log_nonzero_nonshifted'] = mean_sd_calibrated(\n",
    "    y_true_calpart = predictions_calib['ln_ged_sb_dep'], \n",
    "    y_pred_calpart = predictions_calib['step_pred_1'], \n",
    "    y_pred_test = predictions_test['step_pred_1'],\n",
    "    shift=False, \n",
    "    threshold = 1\n",
    ")\n",
    "\n",
    "# Exponentiating predictions for presentation and for count-based calibration\n",
    "for var in ['step_pred_1_cal_log_simple',\n",
    "            'step_pred_1_cal_log_nonshifted',\n",
    "            'step_pred_1_cal_log_shifted',\n",
    "            'step_pred_1_cal_log_nonzero_shifted',\n",
    "            'step_pred_1_cal_log_nonzero_nonshifted', 'step_pred_1']:\n",
    "    expvar = 'exp_' + var\n",
    "    predictions_test[expvar] = np.expm1(predictions_test[var])\n",
    "    predictions_calib[expvar] = np.expm1(predictions_calib[var])\n",
    "\n",
    "# Calibration in terms of counts:\n",
    "print(\"In terms of non-logged predictions/actuals, non-shifted\")\n",
    "# Calibration partition:\n",
    "predictions_calib['exp_step_pred_1_cal_count'] = mean_sd_calibrated(\n",
    "    y_true_calpart = predictions_calib['ged_sb'], \n",
    "    y_pred_calpart = predictions_calib['exp_step_pred_1'], \n",
    "    y_pred_test = predictions_calib['exp_step_pred_1'],\n",
    "    shift=False, \n",
    "    threshold = 0\n",
    ")\n",
    "# Test partition:\n",
    "predictions_test['exp_step_pred_1_cal_count'] = mean_sd_calibrated(\n",
    "    y_true_calpart = predictions_calib['ged_sb'], \n",
    "    y_pred_calpart = predictions_calib['exp_step_pred_1'], \n",
    "    y_pred_test = predictions_test['exp_step_pred_1'], \n",
    "    shift=False, \n",
    "    threshold = 0\n",
    ")\n",
    "\n",
    "print(\"In terms of non-logged predictions/actuals, non-shifted, non-zeros\")\n",
    "# Calibration partition:\n",
    "predictions_calib['exp_step_pred_1_cal_count_nonzeros'] = mean_sd_calibrated(\n",
    "    y_true_calpart = predictions_calib['ged_sb'], \n",
    "    y_pred_calpart = predictions_calib['exp_step_pred_1'], \n",
    "    y_pred_test = predictions_calib['exp_step_pred_1'],\n",
    "    shift=False, \n",
    "    threshold = 1\n",
    ")\n",
    "# Test partition:\n",
    "predictions_test['exp_step_pred_1_cal_count_nonzeros'] = mean_sd_calibrated(\n",
    "    y_true_calpart = predictions_calib['ged_sb'], \n",
    "    y_pred_calpart = predictions_calib['exp_step_pred_1'], \n",
    "    y_pred_test = predictions_test['exp_step_pred_1'], \n",
    "    shift=False, \n",
    "    threshold = 1\n",
    ")\n",
    "\n",
    "to_inspect = ['ln_ged_sb_dep','ged_sb','step_pred_1','exp_step_pred_1',\n",
    "              'exp_step_pred_1_cal_log_simple',\n",
    "              'exp_step_pred_1_cal_log_nonshifted', 'exp_step_pred_1_cal_log_shifted',\n",
    "              'exp_step_pred_1_cal_log_nonzero_shifted','exp_step_pred_1_cal_log_nonzero_nonshifted',\n",
    "             'exp_step_pred_1_cal_count','exp_step_pred_1_cal_count_nonzeros',\n",
    "             'exp_step_pred_1_nln']\n",
    "percentiles_to_inspect = [.75, .85, .9, .95, .99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba406de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect calibration for the calibration partition:\n",
    "predictions_calib[to_inspect].describe(percentiles=percentiles_to_inspect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d818434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect calibration for the test partition:\n",
    "\n",
    "predictions_test[to_inspect].describe(percentiles=percentiles_to_inspect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7965c7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import gen_even_slices\n",
    "\n",
    "# From https://scikit-learn.org/stable/auto_examples/linear_model/plot_poisson_regression_non_normal_loss.html?highlight=calibration\n",
    "# Simplified: removed weights\n",
    "def _mean_frequency_by_risk_group(y_true, y_pred, n_bins=100):\n",
    "    \"\"\"Compare predictions and observations for bins ordered by y_pred.\n",
    "\n",
    "    We order the samples by ``y_pred`` and split it in bins.\n",
    "    In each bin the observed mean is compared with the predicted mean.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true: array-like of shape (n_samples,)\n",
    "        Ground truth (correct) target values.\n",
    "    y_pred: array-like of shape (n_samples,)\n",
    "        Estimated target values.\n",
    "    sample_weight : array-like of shape (n_samples,)\n",
    "        Sample weights.\n",
    "    n_bins: int\n",
    "        Number of bins to use.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bin_centers: ndarray of shape (n_bins,)\n",
    "        bin centers\n",
    "    y_true_bin: ndarray of shape (n_bins,)\n",
    "        average y_pred for each bin\n",
    "    y_pred_bin: ndarray of shape (n_bins,)\n",
    "        average y_pred for each bin\n",
    "    \"\"\"\n",
    "    idx_sort = np.argsort(y_pred)\n",
    "    bin_centers = np.arange(0, 1, 1 / n_bins) + 0.5 / n_bins\n",
    "    y_pred_bin = np.zeros(n_bins)\n",
    "    y_true_bin = np.zeros(n_bins)\n",
    "\n",
    "    for n, sl in enumerate(gen_even_slices(len(y_true), n_bins)):\n",
    "#        weights = sample_weight[idx_sort][sl]\n",
    "        y_pred_bin[n] = np.average(y_pred[idx_sort][sl])\n",
    "        y_true_bin[n] = np.average(y_true[idx_sort][sl])\n",
    "    return bin_centers, y_true_bin, y_pred_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a095c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Actual number of fatalities: {predictions_test['ged_sb'].sum()}\")\n",
    "fig, ax = plt.subplots(nrows=3, ncols=3, figsize=(12, 8))\n",
    "plt.subplots_adjust(wspace=0.3)\n",
    "\n",
    "for axi, prediction_column in zip(ax.ravel(), ['exp_step_pred_1',\n",
    "                                               'exp_step_pred_1_cal_log_nonshifted',\n",
    "                                               'exp_step_pred_1_cal_log_shifted',\n",
    "                                               'exp_step_pred_1_cal_log_nonzero_shifted',\n",
    "                                               'exp_step_pred_1_cal_log_nonzero_nonshifted',\n",
    "                                               'exp_step_pred_1_cal_count',\n",
    "                                              'exp_step_pred_1_cal_count_nonzeros',\n",
    "                                              'exp_step_pred_1_nln']):\n",
    "    y_pred = predictions_test[prediction_column].values\n",
    "    y_true = predictions_test['ged_sb'].values\n",
    "    q, y_true_seg, y_pred_seg = _mean_frequency_by_risk_group(\n",
    "        y_true, y_pred, n_bins=40\n",
    "    )\n",
    "\n",
    "    # Name of the model after the estimator used in the last step of the\n",
    "    # pipeline.\n",
    "    print(f\"Predicted number of fatalities, {prediction_column}: {np.sum(y_pred):.1f}\")\n",
    "\n",
    "    axi.plot(q, y_pred_seg, marker=\"x\", linestyle=\"--\", label=\"predictions\")\n",
    "    axi.plot(q, y_true_seg, marker=\"o\", linestyle=\"--\", label=\"observations\")\n",
    "    axi.set_xlim(0, 1.0)\n",
    "    axi.set_ylim(0, 2000)\n",
    "    axi.set(\n",
    "        title=prediction_column,\n",
    "        xlabel=\"Fraction of samples sorted by y_pred\",\n",
    "        ylabel=\"Mean number of fatalities (y_pred)\",\n",
    "    )\n",
    "    axi.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a642d5",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22019fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Fixing random state for reproducibility\n",
    "np.random.seed(19680801)\n",
    "\n",
    "x = predictions_test['step_pred_1']\n",
    "y = predictions_test['ln_ged_sb_dep']\n",
    "N = len(x)\n",
    "colors = np.random.rand(N)\n",
    "area = (30 * np.random.rand(N))**2  # 0 to 15 point radii\n",
    "colors = x\n",
    "area = 4\n",
    "\n",
    "ax=0 # per-column\n",
    "mse = ((x - y)**2).mean(axis=ax)\n",
    "print(mse)\n",
    "\n",
    "plt.scatter(x, y, s=area, c=colors, alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57650df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_poisson_deviance\n",
    "\n",
    "\n",
    "def score_estimator(y_pred, df_test):\n",
    "    \"\"\"Score an estimator on the test set.\"\"\"\n",
    "#    y_pred = estimator.predict(df_test)\n",
    "\n",
    "    print(\n",
    "        \"MSE: %.3f\"\n",
    "        % mean_squared_error(df_test, y_pred)\n",
    "    )\n",
    "    print(\n",
    "        \"MAE: %.3f\"\n",
    "        % mean_absolute_error(df_test, y_pred)\n",
    "    )\n",
    "\n",
    "    # Ignore non-positive predictions, as they are invalid for\n",
    "    # the Poisson deviance.\n",
    "    mask = y_pred > 0\n",
    "    if (~mask).any():\n",
    "        n_masked, n_samples = (~mask).sum(), mask.shape[0]\n",
    "        print(\n",
    "            \"WARNING: Estimator yields invalid, non-positive predictions \"\n",
    "            f\" for {n_masked} samples out of {n_samples}. These predictions \"\n",
    "            \"are ignored when computing the Poisson deviance.\"\n",
    "        )\n",
    "\n",
    "    print(\n",
    "        \"mean Poisson deviance: %.3f\"\n",
    "        % mean_poisson_deviance(\n",
    "            df_test[mask],\n",
    "            y_pred[mask],\n",
    "            sample_weight=df_test[mask],\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"Evaluation:\")\n",
    "score_estimator(predictions_test['step_pred_1'], predictions_test['ln_ged_sb_dep'])\n",
    "score_estimator(predictions_test['exp_step_pred_1'], predictions_test['ged_sb'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c810f484",
   "metadata": {},
   "source": [
    "## Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c093b1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from views_dataviz.map import mapper, utils\n",
    "from views_dataviz import color\n",
    "\n",
    "import sqlalchemy as sa\n",
    "from ingester3.config import source_db_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3789e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = sa.create_engine(source_db_path)\n",
    "gdf = gpd.GeoDataFrame.from_postgis(\n",
    "    \"SELECT id, geom FROM prod.country\", engine, geom_col='geom'\n",
    "#    \"SELECT id, geom FROM prod.country WHERE in_africa=1\", engine, geom_col='geom'\n",
    ")\n",
    "gdf[\"preds\"] = predictions.loc[397,:]\n",
    "gdf[\"month_id\"] = 397"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a44564",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c181ad04",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.loc[397,:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
